#!/usr/bin/env python3
"""
Lab5 Part B: CNNä½¿ç”¨PyTorchå®ç°
å·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Network) å®Œæ•´å®ç°

åŠŸèƒ½:
- ä½¿ç”¨torchvisionåŠ è½½MNISTæ•°æ®é›†
- å®ç°LeNeté£æ ¼çš„CNNæ¶æ„
- å®Œæ•´çš„PyTorchè®­ç»ƒä¸è¯„ä¼°æµç¨‹
- ä¸MLPçš„æ€§èƒ½å¯¹æ¯”åˆ†æ

ç›®æ ‡: æµ‹è¯•é›†å‡†ç¡®ç‡ > 98%
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import time
import os

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾å‚æ•°
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)

class LeNetCNN(nn.Module):
    """
    LeNeté£æ ¼çš„å·ç§¯ç¥ç»ç½‘ç»œ
    ç»å…¸çš„CNNæ¶æ„ï¼Œé€‚åˆæ‰‹å†™æ•°å­—è¯†åˆ«
    """

    def __init__(self, num_classes=10):
        super(LeNetCNN, self).__init__()

        # ç¬¬ä¸€ä¸ªå·ç§¯å—: å·ç§¯å±‚ + æ¿€æ´» + æ± åŒ–
        # è¾“å…¥: 1x28x28
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)
        # è¾“å‡º: 6x24x24
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        # è¾“å‡º: 6x12x12

        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)
        # è¾“å‡º: 16x8x8
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        # è¾“å‡º: 16x4x4

        # å…¨è¿æ¥å±‚
        # è¾“å…¥: 16x4x4 = 256
        self.fc1 = nn.Linear(in_features=16*4*4, out_features=120)
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(in_features=120, out_features=84)
        self.relu4 = nn.ReLU()
        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, 1, 28, 28)

        Returns:
            out: è¾“å‡ºå¼ é‡ (batch_size, num_classes)
        """
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.conv1(x)      # -> (batch_size, 6, 24, 24)
        x = self.relu1(x)      # -> (batch_size, 6, 24, 24)
        x = self.pool1(x)      # -> (batch_size, 6, 12, 12)

        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.conv2(x)      # -> (batch_size, 16, 8, 8)
        x = self.relu2(x)      # -> (batch_size, 16, 8, 8)
        x = self.pool2(x)      # -> (batch_size, 16, 4, 4)

        # å±•å¹³
        x = x.view(-1, 16*4*4)  # -> (batch_size, 256)

        # å…¨è¿æ¥å±‚
        x = self.fc1(x)        # -> (batch_size, 120)
        x = self.relu3(x)      # -> (batch_size, 120)
        x = self.fc2(x)        # -> (batch_size, 84)
        x = self.relu4(x)      # -> (batch_size, 84)
        x = self.fc3(x)        # -> (batch_size, 10)

        return x

    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class CNNTrainer:
    """
    CNNè®­ç»ƒå™¨ç±»
    å°è£…å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
    """

    def __init__(self, model, device='cpu', learning_rate=0.001):
        self.model = model.to(device)
        self.device = device
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': [],
            'epoch_times': []
        }

    def train_epoch(self, train_loader):
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨

        Returns:
            avg_loss: å¹³å‡æŸå¤±
            accuracy: è®­ç»ƒå‡†ç¡®ç‡
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)

            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def validate(self, val_loader):
        """
        éªŒè¯æ¨¡å‹

        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨

        Returns:
            avg_loss: å¹³å‡æŸå¤±
            accuracy: éªŒè¯å‡†ç¡®ç‡
        """
        self.model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                val_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        avg_loss = val_loss / len(val_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def train(self, train_loader, val_loader, epochs=10, verbose=1):
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            verbose: æ¯éš”å¤šå°‘è½®æ‰“å°ä¿¡æ¯
        """
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒCNNæ¨¡å‹...")
        print(f"ğŸ“Š è®­ç»ƒé›†æ‰¹æ•°: {len(train_loader)}")
        print(f"ğŸ“Š éªŒè¯é›†æ‰¹æ•°: {len(val_loader)}")
        print(f"ğŸ¯ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
        print(f"â±ï¸  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
        print(f"ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {self.model.count_parameters():,}")

        start_time = time.time()

        for epoch in range(epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss, train_acc = self.train_epoch(train_loader)

            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)

            epoch_time = time.time() - epoch_start_time

            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_accuracy'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_accuracy'].append(val_acc)
            self.train_history['epoch_times'].append(epoch_time)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % verbose == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs:3d} | "
                      f"Train Loss: {train_loss:.4f} | "
                      f"Train Acc: {train_acc:.4f} | "
                      f"Val Loss: {val_loss:.4f} | "
                      f"Val Acc: {val_acc:.4f} | "
                      f"Time: {epoch_time:.2f}s")

            # æ—©åœæœºåˆ¶ (å¦‚æœéªŒè¯å‡†ç¡®ç‡è¾¾åˆ°99%)
            if val_acc >= 0.99:
                print(f"\nğŸ‰ æå‰åœæ­¢! éªŒè¯å‡†ç¡®ç‡è¾¾åˆ° {val_acc:.4f}")
                break

        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

        return self.train_history

    def evaluate(self, test_loader):
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹

        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨

        Returns:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\nğŸ” æ¨¡å‹è¯„ä¼°...")

        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        class_correct = list(0. for i in range(10))
        class_total = list(0. for i in range(10))

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

                # æ¯ç±»åˆ«å‡†ç¡®ç‡
                c = (pred == target).squeeze()
                for i in range(target.size(0)):
                    label = target[i]
                    class_correct[label] += c[i].item()
                    class_total[label] += 1

        test_loss /= len(test_loader)
        accuracy = correct / total

        print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"ğŸ“Š æ¯ç±»åˆ«å‡†ç¡®ç‡:")
        class_accuracies = {}
        for i in range(10):
            if class_total[i] > 0:
                acc = class_correct[i] / class_total[i]
                class_accuracies[i] = acc
                print(f"   æ•°å­— {i}: {acc:.4f} ({acc*100:.2f}%)")

        results = {
            'test_accuracy': accuracy,
            'test_loss': test_loss,
            'class_accuracies': class_accuracies,
            'total_parameters': self.model.count_parameters()
        }

        return results

    def plot_training_curves(self, save_path=None):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_history['train_loss'], label='Training Loss', color='blue', linewidth=2)
        ax1.plot(self.train_history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.train_history['train_accuracy'], label='Training Accuracy', color='blue', linewidth=2)
        ax2.plot(self.train_history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)
        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Accuracy', fontsize=12)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š CNNè®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")

        plt.show()

def load_mnist_data(batch_size=64):
    """
    åŠ è½½MNISTæ•°æ®é›†

    Args:
        batch_size: æ‰¹å¤§å°

    Returns:
        train_loader, test_loader: æ•°æ®åŠ è½½å™¨
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...")

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),  # è½¬æ¢ä¸ºTensorå¹¶å½’ä¸€åŒ–åˆ°[0,1]
        transforms.Normalize((0.1307,), (0.3081,))  # æ ‡å‡†åŒ– (MNISTå‡å€¼å’Œæ ‡å‡†å·®)
    ])

    # ä¸‹è½½å¹¶åŠ è½½è®­ç»ƒé›†
    try:
        train_dataset = torchvision.datasets.MNIST(
            root='./data', train=True, download=True, transform=transform
        )
        test_dataset = torchvision.datasets.MNIST(
            root='./data', train=False, download=True, transform=transform
        )
        print("âœ… MNISTæ•°æ®é›†ä¸‹è½½æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  MNISTæ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨æœ¬åœ°æ•°æ®...")
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        from sklearn.datasets import fetch_openml
        mnist = fetch_openml('mnist_784', version=1, as_frame=False)
        X, y = mnist.data, mnist.target.astype(int)

        # è½¬æ¢ä¸ºtorch tensoræ ¼å¼
        X = X.reshape(-1, 28, 28).astype(np.float32) / 255.0
        X = (X - 0.1307) / 0.3081
        y = y.astype(np.int64)

        # åˆ›å»ºTensorDataset
        train_size = 60000
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        train_dataset = TensorDataset(
            torch.from_numpy(X_train).unsqueeze(1),  # æ·»åŠ é€šé“ç»´åº¦
            torch.from_numpy(y_train)
        )
        test_dataset = TensorDataset(
            torch.from_numpy(X_test).unsqueeze(1),
            torch.from_numpy(y_test)
        )
        print("âœ… ä½¿ç”¨OpenMLæ•°æ®åˆ›å»ºæ•°æ®é›†")

    # åˆ’åˆ†éªŒè¯é›†
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_dataset, [train_size, val_size]
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    print(f"ğŸ“Š æ‰¹å¤§å°: {batch_size}")

    return train_loader, val_loader, test_loader

def visualize_mnist_samples(dataloader, save_path=None):
    """
    å¯è§†åŒ–MNISTæ ·æœ¬

    Args:
        dataloader: æ•°æ®åŠ è½½å™¨
        save_path: ä¿å­˜è·¯å¾„
    """
    # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
    data_iter = iter(dataloader)
    images, labels = next(data_iter)

    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    for i, ax in enumerate(axes.flat):
        if i < 10:
            ax.imshow(images[i][0], cmap='gray')
            ax.set_title(f'Label: {labels[i].item()}')
            ax.axis('off')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š MNISTæ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Lab5 Part B: CNNä½¿ç”¨PyTorchå®ç°")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    train_loader, val_loader, test_loader = load_mnist_data(batch_size=64)

    # å¯è§†åŒ–æ ·æœ¬
    visualize_mnist_samples(train_loader, 'lab5/outputs/cnn_results/mnist_samples.png')

    # 2. åˆ›å»ºCNNæ¨¡å‹
    print(f"\nğŸ§  åˆ›å»ºCNNæ¨¡å‹...")
    model = LeNetCNN(num_classes=10)
    print(f"ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

    # æ‰“å°æ¨¡å‹ç»“æ„
    print("\nğŸ“‹ æ¨¡å‹ç»“æ„:")
    print(model)

    # 3. åˆ›å»ºè®­ç»ƒå™¨
    trainer = CNNTrainer(model, device=device, learning_rate=0.001)

    # 4. è®­ç»ƒæ¨¡å‹
    training_history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=15,
        verbose=2
    )

    # 5. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    trainer.plot_training_curves('lab5/outputs/cnn_results/training_curves.png')

    # 6. æµ‹è¯•é›†è¯„ä¼°
    evaluation_results = trainer.evaluate(test_loader)

    # 7. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")

    # ä¿å­˜è®­ç»ƒå†å²
    torch.save(training_history, 'lab5/outputs/cnn_results/training_history.pth')
    torch.save(evaluation_results, 'lab5/outputs/cnn_results/evaluation_results.pth')
    torch.save(model.state_dict(), 'lab5/outputs/cnn_results/cnn_model.pth')

    print(f"âœ… CNNå®éªŒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {evaluation_results['test_accuracy']:.4f} ({evaluation_results['test_accuracy']*100:.2f}%)")
    print(f"ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {evaluation_results['total_parameters']:,}")

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    target_accuracy = 0.98
    if evaluation_results['test_accuracy'] >= target_accuracy:
        print(f"ğŸ‰ æˆåŠŸè¾¾åˆ°ç›®æ ‡! å‡†ç¡®ç‡ {evaluation_results['test_accuracy']*100:.2f}% > {target_accuracy*100:.1f}%")
    else:
        print(f"âš ï¸  æœªè¾¾åˆ°ç›®æ ‡! å‡†ç¡®ç‡ {evaluation_results['test_accuracy']*100:.2f}% < {target_accuracy*100:.1f}%")

    return evaluation_results

if __name__ == "__main__":
    results = main()