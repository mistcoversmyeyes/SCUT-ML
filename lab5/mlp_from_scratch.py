#!/usr/bin/env python3
"""
Lab5 Part A: MLPä»é›¶å®ç°
å¤šå±‚æ„ŸçŸ¥æœº (Multi-Layer Perceptron) å®Œå…¨ä»é›¶å®ç°

åŠŸèƒ½:
- MNISTæ•°æ®åŠ è½½ä¸é¢„å¤„ç†
- ä»é›¶å®ç°ç¥ç»ç½‘ç»œæ‰€æœ‰ç»„ä»¶
- å®Œæ•´çš„è®­ç»ƒä¸è¯„ä¼°æµç¨‹
- ç»“æœå¯è§†åŒ–ä¸æ€§èƒ½åˆ†æ

ç›®æ ‡: æµ‹è¯•é›†å‡†ç¡®ç‡ > 90%
"""

import numpy as np
import matplotlib.pyplot as plt
import time
import os
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾å‚æ•°
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)

class MLPFromScratch:
    """
    ä»é›¶å®ç°çš„å¤šå±‚æ„ŸçŸ¥æœºç±»
    å®ç°äº†å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œè®­ç»ƒæµç¨‹
    """

    def __init__(self, layer_sizes, learning_rate=0.01, random_seed=42):
        """
        åˆå§‹åŒ–MLPç½‘ç»œ

        Args:
            layer_sizes: ç½‘ç»œå±‚ç»“æ„åˆ—è¡¨ [è¾“å…¥å±‚, éšè—å±‚1, ..., è¾“å‡ºå±‚]
            learning_rate: å­¦ä¹ ç‡
            random_seed: éšæœºç§å­
        """
        np.random.seed(random_seed)
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.num_layers = len(layer_sizes)

        # åˆå§‹åŒ–å‚æ•°
        self.parameters = self.initialize_parameters()

        # è®­ç»ƒå†å²è®°å½•
        self.train_history = {
            'loss': [],
            'accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }

    def initialize_parameters(self):
        """
        åˆå§‹åŒ–ç½‘ç»œå‚æ•° (æƒé‡å’Œåç½®)
        ä½¿ç”¨Heåˆå§‹åŒ–ç­–ç•¥
        """
        parameters = {}

        for l in range(1, self.num_layers):
            # Heåˆå§‹åŒ–: W ~ N(0, sqrt(2/n[l-1]))
            parameters[f'W{l}'] = np.random.randn(
                self.layer_sizes[l], self.layer_sizes[l-1]
            ) * np.sqrt(2.0 / self.layer_sizes[l-1])

            # åç½®åˆå§‹åŒ–ä¸ºå°éšæœºæ•°
            parameters[f'b{l}'] = np.random.randn(
                self.layer_sizes[l], 1
            ) * 0.01

        return parameters

    def relu(self, Z):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, Z)

    def relu_derivative(self, Z):
        """ReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°"""
        return (Z > 0).astype(float)

    def softmax(self, Z):
        """
        Softmaxæ¿€æ´»å‡½æ•°
        å®ç°æ•°å€¼ç¨³å®šçš„ç‰ˆæœ¬
        """
        # æ•°å€¼ç¨³å®šæ€§: å‡å»æœ€å¤§å€¼
        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)
        exp_Z = np.exp(Z_shifted)
        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)

    def one_hot_encode(self, y, num_classes):
        """
        å°†æ ‡ç­¾å‘é‡è¿›è¡Œç‹¬çƒ­ç¼–ç 

        Args:
            y: æ ‡ç­¾å‘é‡ (shape: (m,))
            num_classes: ç±»åˆ«æ•°é‡

        Returns:
            Y_one_hot: ç‹¬çƒ­ç¼–ç çŸ©é˜µ (shape: (num_classes, m))
        """
        m = y.shape[0]
        Y_one_hot = np.zeros((num_classes, m))
        Y_one_hot[y, np.arange(m)] = 1
        return Y_one_hot

    def forward_propagation(self, X):
        """
        å‰å‘ä¼ æ’­

        Args:
            X: è¾“å…¥æ•°æ® (shape: (n_features, m))

        Returns:
            cache: ç¼“å­˜å„å±‚ç»“æœç”¨äºåå‘ä¼ æ’­
        """
        cache = {'A0': X}
        A = X

        # å‰å‘ä¼ æ’­å„å±‚
        for l in range(1, self.num_layers - 1):
            Z = np.dot(self.parameters[f'W{l}'], A) + self.parameters[f'b{l}']
            A = self.relu(Z)
            cache[f'Z{l}'] = Z
            cache[f'A{l}'] = A

        # è¾“å‡ºå±‚ä½¿ç”¨softmax
        Z_L = np.dot(self.parameters[f'W{self.num_layers-1}'], A) + \
              self.parameters[f'b{self.num_layers-1}']
        A_L = self.softmax(Z_L)
        cache[f'Z{self.num_layers-1}'] = Z_L
        cache[f'A{self.num_layers-1}'] = A_L

        return cache

    def compute_loss(self, Y_true, Y_pred):
        """
        è®¡ç®—äº¤å‰ç†µæŸå¤±

        Args:
            Y_true: çœŸå®æ ‡ç­¾ (shape: (num_classes, m))
            Y_pred: é¢„æµ‹æ¦‚ç‡ (shape: (num_classes, m))

        Returns:
            loss: å¹³å‡äº¤å‰ç†µæŸå¤±
        """
        m = Y_true.shape[1]

        # æ·»åŠ å°å¸¸æ•°é¿å…log(0)
        epsilon = 1e-15
        Y_pred_clipped = np.clip(Y_pred, epsilon, 1 - epsilon)

        # äº¤å‰ç†µæŸå¤±
        loss = -np.sum(Y_true * np.log(Y_pred_clipped)) / m
        return loss

    def backward_propagation(self, X, Y, cache):
        """
        åå‘ä¼ æ’­ç®—æ³•

        Args:
            X: è¾“å…¥æ•°æ® (shape: (n_features, m))
            Y: çœŸå®æ ‡ç­¾ (shape: (num_classes, m))
            cache: å‰å‘ä¼ æ’­çš„ç¼“å­˜

        Returns:
            grads: å„å±‚å‚æ•°çš„æ¢¯åº¦
        """
        grads = {}
        m = X.shape[1]
        L = self.num_layers - 1

        # è¾“å‡ºå±‚æ¢¯åº¦
        A_L = cache[f'A{L}']
        dZ_L = A_L - Y

        grads[f'dW{L}'] = np.dot(dZ_L, cache[f'A{L-1}'].T) / m
        grads[f'db{L}'] = np.sum(dZ_L, axis=1, keepdims=True) / m

        # åå‘ä¼ æ’­éšè—å±‚
        for l in reversed(range(1, L)):
            dZ = np.dot(self.parameters[f'W{l+1}'].T, grads[f'dW{l+1}'] * m)
            dZ = dZ * self.relu_derivative(cache[f'Z{l}'])

            grads[f'dW{l}'] = np.dot(dZ, cache[f'A{l-1}'].T) / m
            grads[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True) / m

        return grads

    def update_parameters(self, grads):
        """
        ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°

        Args:
            grads: å‚æ•°æ¢¯åº¦å­—å…¸
        """
        for l in range(1, self.num_layers):
            self.parameters[f'W{l}'] -= self.learning_rate * grads[f'dW{l}']
            self.parameters[f'b{l}'] -= self.learning_rate * grads[f'db{l}']

    def predict(self, X):
        """
        é¢„æµ‹å‡½æ•°

        Args:
            X: è¾“å…¥æ•°æ® (shape: (n_features, m))

        Returns:
            predictions: é¢„æµ‹ç±»åˆ« (shape: (m,))
            probabilities: é¢„æµ‹æ¦‚ç‡ (shape: (num_classes, m))
        """
        cache = self.forward_propagation(X)
        probabilities = cache[f'A{self.num_layers-1}']
        predictions = np.argmax(probabilities, axis=0)
        return predictions, probabilities

    def compute_accuracy(self, y_true, y_pred):
        """
        è®¡ç®—å‡†ç¡®ç‡

        Args:
            y_true: çœŸå®æ ‡ç­¾ (shape: (m,))
            y_pred: é¢„æµ‹æ ‡ç­¾ (shape: (m,))

        Returns:
            accuracy: å‡†ç¡®ç‡
        """
        return np.mean(y_true == y_pred)

    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=128, verbose=10):
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾ (shape: (n_features, m_train))
            y_train: è®­ç»ƒé›†æ ‡ç­¾ (shape: (m_train,))
            X_val: éªŒè¯é›†ç‰¹å¾ (shape: (n_features, m_val))
            y_val: éªŒè¯é›†æ ‡ç­¾ (shape: (m_val,))
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            verbose: æ¯éš”å¤šå°‘è½®æ‰“å°ä¿¡æ¯
        """
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒMLPæ¨¡å‹...")
        print(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape[1]} æ ·æœ¬")
        print(f"ğŸ“Š éªŒè¯é›†: {X_val.shape[1]} æ ·æœ¬")
        print(f"ğŸ“ˆ ç½‘ç»œç»“æ„: {self.layer_sizes}")
        print(f"ğŸ¯ å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"â±ï¸  è®­ç»ƒè½®æ•°: {epochs}")

        # ç‹¬çƒ­ç¼–ç æ ‡ç­¾
        Y_train = self.one_hot_encode(y_train, self.layer_sizes[-1])
        Y_val = self.one_hot_encode(y_val, self.layer_sizes[-1])

        m_train = X_train.shape[1]
        num_batches = m_train // batch_size

        start_time = time.time()

        for epoch in range(epochs):
            epoch_start_time = time.time()

            # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
            permutation = np.random.permutation(m_train)
            X_train_shuffled = X_train[:, permutation]
            Y_train_shuffled = Y_train[:, permutation]

            epoch_loss = 0
            epoch_accuracy = 0

            # å°æ‰¹é‡è®­ç»ƒ
            for i in range(num_batches):
                start = i * batch_size
                end = start + batch_size

                X_batch = X_train_shuffled[:, start:end]
                Y_batch = Y_train_shuffled[:, start:end]

                # å‰å‘ä¼ æ’­
                cache = self.forward_propagation(X_batch)

                # è®¡ç®—æŸå¤±
                batch_loss = self.compute_loss(Y_batch, cache[f'A{self.num_layers-1}'])
                epoch_loss += batch_loss

                # è®¡ç®—å‡†ç¡®ç‡
                y_pred_batch, _ = self.predict(X_batch)
                y_true_batch = np.argmax(Y_batch, axis=0)
                batch_accuracy = self.compute_accuracy(y_true_batch, y_pred_batch)
                epoch_accuracy += batch_accuracy

                # åå‘ä¼ æ’­
                grads = self.backward_propagation(X_batch, Y_batch, cache)

                # æ›´æ–°å‚æ•°
                self.update_parameters(grads)

            # å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_loss = epoch_loss / num_batches
            avg_accuracy = epoch_accuracy / num_batches

            # éªŒè¯é›†è¯„ä¼°
            y_pred_val, _ = self.predict(X_val)
            val_accuracy = self.compute_accuracy(y_val, y_pred_val)
            val_loss = self.compute_loss(Y_val, self.forward_propagation(X_val)[f'A{self.num_layers-1}'])

            # è®°å½•è®­ç»ƒå†å²
            self.train_history['loss'].append(avg_loss)
            self.train_history['accuracy'].append(avg_accuracy)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_accuracy'].append(val_accuracy)

            epoch_time = time.time() - epoch_start_time

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            if (epoch + 1) % verbose == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs:3d} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Acc: {avg_accuracy:.4f} | "
                      f"Val Loss: {val_loss:.4f} | "
                      f"Val Acc: {val_accuracy:.4f} | "
                      f"Time: {epoch_time:.2f}s")

            # æ—©åœæœºåˆ¶ (å¦‚æœéªŒè¯å‡†ç¡®ç‡è¾¾åˆ°98%)
            if val_accuracy >= 0.98:
                print(f"\nğŸ‰ æå‰åœæ­¢! éªŒè¯å‡†ç¡®ç‡è¾¾åˆ° {val_accuracy:.4f}")
                break

        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
        print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {avg_accuracy:.4f}")
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

        return self.train_history

    def plot_training_curves(self, save_path=None):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_history['loss'], label='Training Loss', color='blue', linewidth=2)
        ax1.plot(self.train_history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.train_history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)
        ax2.plot(self.train_history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)
        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Accuracy', fontsize=12)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")

        plt.show()

    def evaluate_model(self, X_test, y_test):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            X_test: æµ‹è¯•é›†ç‰¹å¾
            y_test: æµ‹è¯•é›†æ ‡ç­¾

        Returns:
            evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\nğŸ” æ¨¡å‹è¯„ä¼°...")

        y_pred, probabilities = self.predict(X_test)
        accuracy = self.compute_accuracy(y_test, y_pred)

        # è®¡ç®—æ¯ç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for digit in range(10):
            mask = y_test == digit
            if np.sum(mask) > 0:
                class_acc = self.compute_accuracy(y_test[mask], y_pred[mask])
                class_accuracies[digit] = class_acc

        print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"ğŸ“Š æ¯ç±»åˆ«å‡†ç¡®ç‡:")
        for digit, acc in sorted(class_accuracies.items()):
            print(f"   æ•°å­— {digit}: {acc:.4f} ({acc*100:.2f}%)")

        evaluation_results = {
            'test_accuracy': accuracy,
            'class_accuracies': class_accuracies,
            'total_parameters': self.count_parameters()
        }

        return evaluation_results

    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        total_params = 0
        for l in range(1, self.num_layers):
            total_params += self.parameters[f'W{l}'].size + self.parameters[f'b{l}'].size
        return total_params

def load_and_preprocess_mnist():
    """
    åŠ è½½å’Œé¢„å¤„ç†MNISTæ•°æ®é›†

    Returns:
        X_train, y_train, X_test, y_test: é¢„å¤„ç†åçš„æ•°æ®
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...")

    try:
        # å°è¯•ä½¿ç”¨tensorflowåŠ è½½
        from tensorflow.keras.datasets import mnist
        (X_train, y_train), (X_test, y_test) = mnist.load_data()
        print("âœ… ä½¿ç”¨TensorFlowåŠ è½½MNISTæ•°æ®é›†")
    except ImportError:
        try:
            # å°è¯•ä½¿ç”¨scikit-learnçš„digitsæ•°æ®é›†ä½œä¸ºæ›¿ä»£
            from sklearn.datasets import fetch_openml
            print("ğŸ”„ ä½¿ç”¨OpenMLåŠ è½½MNISTæ•°æ®é›†...")
            mnist = fetch_openml('mnist_784', version=1, as_frame=False)
            X, y = mnist.data, mnist.target.astype(int)

            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=10000, random_state=42
            )

            # é‡æ–°å¡‘å½¢ä¸º28x28å›¾åƒ
            X_train = X_train.reshape(-1, 28, 28)
            X_test = X_test.reshape(-1, 28, 28)

            print("âœ… ä½¿ç”¨OpenMLåŠ è½½MNISTæ•°æ®é›†")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½MNISTæ•°æ®: {e}")
            print("ğŸ”„ ç”Ÿæˆæ¨¡æ‹ŸMNISTæ•°æ®ç”¨äºæ¼”ç¤º...")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            X_train = np.random.randint(0, 256, (60000, 28, 28), dtype=np.uint8)
            y_train = np.random.randint(0, 10, 60000)
            X_test = np.random.randint(0, 256, (10000, 28, 28), dtype=np.uint8)
            y_test = np.random.randint(0, 10, 10000)
            print("âœ… ç”Ÿæˆæ¨¡æ‹ŸMNISTæ•°æ®é›†")

    print(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape[0]} å¼ å›¾ç‰‡")
    print(f"ğŸ“Š æµ‹è¯•é›†: {X_test.shape[0]} å¼ å›¾ç‰‡")

    # æ•°æ®é¢„å¤„ç†
    print("\nğŸ”„ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")

    # 1. å½’ä¸€åŒ–: [0,255] -> [0,1]
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    # 2. æ‰å¹³åŒ–: (m, 28, 28) -> (784, m)
    X_train_flat = X_train.reshape(X_train.shape[0], -1).T
    X_test_flat = X_test.reshape(X_test.shape[0], -1).T

    print(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"ğŸ“Š è¾“å…¥ç»´åº¦: {X_train_flat.shape[0]} (æ‰å¹³åŒ–å)")
    print(f"ğŸ“Š åƒç´ å€¼èŒƒå›´: [{X_train_flat.min():.3f}, {X_train_flat.max():.3f}]")

    return X_train_flat, y_train, X_test_flat, y_test

def visualize_mnist_samples(X, y, save_path=None):
    """
    å¯è§†åŒ–MNISTæ ·æœ¬

    Args:
        X: å›¾åƒæ•°æ® (shape: (784, m))
        y: æ ‡ç­¾ (shape: (m,))
        save_path: ä¿å­˜è·¯å¾„
    """
    # æ¢å¤å›¾åƒå½¢çŠ¶
    X_images = X.T.reshape(-1, 28, 28)

    # éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    indices = np.random.choice(len(X_images), 16, replace=False)

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for i, ax in enumerate(axes.flat):
        idx = indices[i]
        ax.imshow(X_images[idx], cmap='gray')
        ax.set_title(f'Label: {y[idx]}')
        ax.axis('off')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š MNISTæ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Lab5 Part A: MLPä»é›¶å®ç°")
    print("=" * 60)

    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)

    # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    X_train, y_train, X_test, y_test = load_and_preprocess_mnist()

    # å¯è§†åŒ–æ•°æ®æ ·æœ¬
    visualize_mnist_samples(X_train, y_train, 'lab5/outputs/mlp_results/mnist_samples.png')

    # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    from sklearn.model_selection import train_test_split
    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
        X_train.T, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    X_train_split = X_train_split.T
    X_val_split = X_val_split.T

    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"ğŸ“Š è®­ç»ƒé›†: {X_train_split.shape[1]} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {X_val_split.shape[1]} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†: {X_test.shape[1]} æ ·æœ¬")

    # 3. åˆ›å»ºå’Œè®­ç»ƒMLPæ¨¡å‹
    print(f"\nğŸ§  åˆ›å»ºMLPæ¨¡å‹...")

    # ç½‘ç»œç»“æ„: [è¾“å…¥å±‚, éšè—å±‚, è¾“å‡ºå±‚]
    # 784 -> 128 -> 10 (MNIST: 784ç»´è¾“å…¥, 10ç±»è¾“å‡º)
    layer_sizes = [784, 128, 10]

    mlp = MLPFromScratch(
        layer_sizes=layer_sizes,
        learning_rate=0.01,
        random_seed=42
    )

    # è®­ç»ƒæ¨¡å‹
    training_history = mlp.train(
        X_train_split, y_train_split,
        X_val_split, y_val_split,
        epochs=100,
        batch_size=128,
        verbose=10
    )

    # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    mlp.plot_training_curves('lab5/outputs/mlp_results/training_curves.png')

    # 5. æµ‹è¯•é›†è¯„ä¼°
    evaluation_results = mlp.evaluate_model(X_test, y_test)

    # 6. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")

    # ä¿å­˜è®­ç»ƒå†å²
    np.save('lab5/outputs/mlp_results/training_history.npy', training_history)
    np.save('lab5/outputs/mlp_results/evaluation_results.npy', evaluation_results)

    print(f"âœ… MLPå®éªŒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {evaluation_results['test_accuracy']:.4f} ({evaluation_results['test_accuracy']*100:.2f}%)")
    print(f"ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {evaluation_results['total_parameters']:,}")

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    target_accuracy = 0.90
    if evaluation_results['test_accuracy'] >= target_accuracy:
        print(f"ğŸ‰ æˆåŠŸè¾¾åˆ°ç›®æ ‡! å‡†ç¡®ç‡ {evaluation_results['test_accuracy']*100:.2f}% > {target_accuracy*100:.1f}%")
    else:
        print(f"âš ï¸  æœªè¾¾åˆ°ç›®æ ‡! å‡†ç¡®ç‡ {evaluation_results['test_accuracy']*100:.2f}% < {target_accuracy*100:.1f}%")

if __name__ == "__main__":
    main()