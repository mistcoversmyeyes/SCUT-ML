#!/usr/bin/env python3
"""
Lab5 Part A: MLPä»é›¶å®ç° (ç®€åŒ–æµ‹è¯•ç‰ˆæœ¬)
å¿«é€ŸéªŒè¯ç®—æ³•é€»è¾‘ï¼Œä½¿ç”¨è¾ƒå°æ•°æ®é›†
"""

import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾å‚æ•°
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MLPFromScratchSimple:
    """ç®€åŒ–ç‰ˆMLPå®ç°ï¼Œç”¨äºå¿«é€ŸéªŒè¯"""

    def __init__(self, layer_sizes, learning_rate=0.01, random_seed=42):
        np.random.seed(random_seed)
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.num_layers = len(layer_sizes)
        self.parameters = self.initialize_parameters()
        self.train_history = {'loss': [], 'accuracy': []}

    def initialize_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        parameters = {}
        for l in range(1, self.num_layers):
            parameters[f'W{l}'] = np.random.randn(
                self.layer_sizes[l], self.layer_sizes[l-1]
            ) * 0.01
            parameters[f'b{l}'] = np.zeros((self.layer_sizes[l], 1))
        return parameters

    def relu(self, Z):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, Z)

    def relu_derivative(self, Z):
        """ReLUå¯¼æ•°"""
        return (Z > 0).astype(float)

    def softmax(self, Z):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)
        exp_Z = np.exp(Z_shifted)
        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)

    def one_hot_encode(self, y, num_classes):
        """ç‹¬çƒ­ç¼–ç """
        m = y.shape[0]
        Y_one_hot = np.zeros((num_classes, m))
        Y_one_hot[y, np.arange(m)] = 1
        return Y_one_hot

    def forward_propagation(self, X):
        """å‰å‘ä¼ æ’­"""
        cache = {'A0': X}
        A = X

        for l in range(1, self.num_layers - 1):
            Z = np.dot(self.parameters[f'W{l}'], A) + self.parameters[f'b{l}']
            A = self.relu(Z)
            cache[f'Z{l}'] = Z
            cache[f'A{l}'] = A

        Z_L = np.dot(self.parameters[f'W{self.num_layers-1}'], A) + \
              self.parameters[f'b{self.num_layers-1}']
        A_L = self.softmax(Z_L)
        cache[f'Z{self.num_layers-1}'] = Z_L
        cache[f'A{self.num_layers-1}'] = A_L

        return cache

    def compute_loss(self, Y_true, Y_pred):
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        epsilon = 1e-15
        Y_pred_clipped = np.clip(Y_pred, epsilon, 1 - epsilon)
        loss = -np.sum(Y_true * np.log(Y_pred_clipped)) / Y_true.shape[1]
        return loss

    def backward_propagation(self, X, Y, cache):
        """åå‘ä¼ æ’­"""
        grads = {}
        m = X.shape[1]
        L = self.num_layers - 1

        # è¾“å‡ºå±‚æ¢¯åº¦
        A_L = cache[f'A{L}']
        dZ_L = A_L - Y

        grads[f'dW{L}'] = np.dot(dZ_L, cache[f'A{L-1}'].T) / m
        grads[f'db{L}'] = np.sum(dZ_L, axis=1, keepdims=True) / m

        # åå‘ä¼ æ’­éšè—å±‚
        for l in reversed(range(1, L)):
            dZ = np.dot(self.parameters[f'W{l+1}'].T, dZ_L)
            dZ = dZ * self.relu_derivative(cache[f'Z{l}'])
            dZ_L = dZ

            grads[f'dW{l}'] = np.dot(dZ, cache[f'A{l-1}'].T) / m
            grads[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True) / m

        return grads

    def update_parameters(self, grads):
        """æ›´æ–°å‚æ•°"""
        for l in range(1, self.num_layers):
            self.parameters[f'W{l}'] -= self.learning_rate * grads[f'dW{l}']
            self.parameters[f'b{l}'] -= self.learning_rate * grads[f'db{l}']

    def predict(self, X):
        """é¢„æµ‹"""
        cache = self.forward_propagation(X)
        probabilities = cache[f'A{self.num_layers-1}']
        predictions = np.argmax(probabilities, axis=0)
        return predictions

    def compute_accuracy(self, y_true, y_pred):
        """è®¡ç®—å‡†ç¡®ç‡"""
        return np.mean(y_true == y_pred)

    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=64, verbose=5):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆMLP...")
        print(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape[1]} æ ·æœ¬")
        print(f"ğŸ“Š éªŒè¯é›†: {X_val.shape[1]} æ ·æœ¬")

        Y_train = self.one_hot_encode(y_train, self.layer_sizes[-1])
        Y_val = self.one_hot_encode(y_val, self.layer_sizes[-1])

        m_train = X_train.shape[1]

        for epoch in range(epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            permutation = np.random.permutation(m_train)
            X_train_shuffled = X_train[:, permutation]
            Y_train_shuffled = Y_train[:, permutation]

            epoch_loss = 0
            epoch_accuracy = 0
            num_batches = 0

            # å°æ‰¹é‡è®­ç»ƒ
            for i in range(0, m_train, batch_size):
                end = min(i + batch_size, m_train)
                X_batch = X_train_shuffled[:, i:end]
                Y_batch = Y_train_shuffled[:, i:end]

                # å‰å‘ä¼ æ’­
                cache = self.forward_propagation(X_batch)

                # è®¡ç®—æŸå¤±
                batch_loss = self.compute_loss(Y_batch, cache[f'A{self.num_layers-1}'])
                epoch_loss += batch_loss

                # è®¡ç®—å‡†ç¡®ç‡
                y_pred_batch = self.predict(X_batch)
                y_true_batch = np.argmax(Y_batch, axis=0)
                batch_accuracy = self.compute_accuracy(y_true_batch, y_pred_batch)
                epoch_accuracy += batch_accuracy

                # åå‘ä¼ æ’­
                grads = self.backward_propagation(X_batch, Y_batch, cache)

                # æ›´æ–°å‚æ•°
                self.update_parameters(grads)

                num_batches += 1

            # å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_loss = epoch_loss / num_batches
            avg_accuracy = epoch_accuracy / num_batches

            # éªŒè¯é›†è¯„ä¼°
            y_pred_val = self.predict(X_val)
            val_accuracy = self.compute_accuracy(y_val, y_pred_val)

            # è®°å½•å†å²
            self.train_history['loss'].append(avg_loss)
            self.train_history['accuracy'].append(avg_accuracy)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % verbose == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs:3d} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Acc: {avg_accuracy:.4f} | "
                      f"Val Acc: {val_accuracy:.4f}")

        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

        return self.train_history

def create_simple_mnist_data():
    """åˆ›å»ºç®€åŒ–çš„MNISTæ¨¡æ‹Ÿæ•°æ®"""
    print("ğŸ”„ åˆ›å»ºç®€åŒ–çš„MNISTæ¨¡æ‹Ÿæ•°æ®...")

    np.random.seed(42)

    # åˆ›å»ºä¸€äº›åŸºæœ¬çš„æ‰‹å†™æ•°å­—æ¨¡å¼
    num_samples_per_digit = 100
    num_digits = 10
    image_size = 8  # ä½¿ç”¨8x8å°å›¾åƒ

    X = []
    y = []

    for digit in range(num_digits):
        for sample in range(num_samples_per_digit):
            # åˆ›å»ºåŸºæœ¬æ¨¡å¼
            pattern = np.zeros((image_size, image_size))

            # æ ¹æ®æ•°å­—æ·»åŠ ç®€å•æ¨¡å¼
            if digit == 0:
                # åœ†å½¢
                for i in range(image_size):
                    for j in range(image_size):
                        if (i - 3.5)**2 + (j - 3.5)**2 < 9:
                            pattern[i, j] = 1
            elif digit == 1:
                # å‚ç›´çº¿
                pattern[:, 4] = 1
            elif digit == 2:
                # æ·»åŠ ç®€å•æ¨¡å¼
                pattern[2:6, 2:6] = 1
            elif digit == 3:
                # æ·»åŠ ç®€å•æ¨¡å¼
                pattern[1:7, 1:7] = 1
                pattern[2:6, 2:6] = 0
            elif digit == 4:
                # Lå½¢çŠ¶
                pattern[2:6, 2] = 1
                pattern[6, 2:6] = 1
            else:
                # éšæœºæ¨¡å¼
                pattern = np.random.rand(image_size, image_size) * 0.3

            # æ·»åŠ å™ªå£°
            noise = np.random.rand(image_size, image_size) * 0.2
            pattern = pattern + noise

            # æ·»åŠ ä¸€äº›å˜æ¢
            if np.random.rand() > 0.5:
                pattern = np.fliplr(pattern)
            if np.random.rand() > 0.5:
                pattern = np.flipud(pattern)

            # éšæœºå¹³ç§»
            shift_x = np.random.randint(-1, 2)
            shift_y = np.random.randint(-1, 2)
            pattern = np.roll(np.roll(pattern, shift_x, axis=1), shift_y, axis=0)

            X.append(pattern.flatten())
            y.append(digit)

    X = np.array(X).T  # shape: (64, 1000)
    y = np.array(y)    # shape: (1000,)

    # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
    X = X + np.random.normal(0, 0.1, X.shape)
    X = np.clip(X, 0, 1)

    print(f"âœ… ç®€åŒ–MNISTæ•°æ®åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ•°æ®ç»´åº¦: {X.shape}")
    print(f"ğŸ“Š æ ‡ç­¾ç»´åº¦: {y.shape}")
    print(f"ğŸ“Š æ¯ä¸ªæ•°å­—æ ·æœ¬æ•°: {num_samples_per_digit}")

    return X, y

def visualize_samples(X, y, save_path=None):
    """å¯è§†åŒ–æ ·æœ¬"""
    X_images = X.T.reshape(-1, 8, 8)

    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    for digit in range(10):
        mask = y == digit
        if np.sum(mask) > 0:
            idx = np.where(mask)[0][0]
            row, col = digit // 5, digit % 5
            axes[row, col].imshow(X_images[idx], cmap='gray')
            axes[row, col].set_title(f'Digit: {digit}')
            axes[row, col].axis('off')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š æ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Lab5 Part A: MLPä»é›¶å®ç° (ç®€åŒ–æµ‹è¯•ç‰ˆ)")
    print("=" * 60)

    # 1. åˆ›å»ºç®€åŒ–æ•°æ®
    X, y = create_simple_mnist_data()

    # å¯è§†åŒ–æ ·æœ¬
    visualize_samples(X, y, 'lab5/outputs/mlp_results/simple_samples.png')

    # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X.T, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train = X_train.T
    X_test = X_test.T

    # 3. åˆ›å»ºå’Œè®­ç»ƒMLP
    layer_sizes = [64, 32, 10]  # [è¾“å…¥å±‚, éšè—å±‚, è¾“å‡ºå±‚]

    mlp = MLPFromScratchSimple(
        layer_sizes=layer_sizes,
        learning_rate=0.1,
        random_seed=42
    )

    # 4. è®­ç»ƒæ¨¡å‹
    history = mlp.train(
        X_train, y_train,
        X_test, y_test,
        epochs=30,
        batch_size=32,
        verbose=5
    )

    # 5. æœ€ç»ˆè¯„ä¼°
    y_pred = mlp.predict(X_test)
    final_accuracy = mlp.compute_accuracy(y_test, y_pred)

    print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")

    # 6. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history['loss'])
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(history['accuracy'])
    plt.title('Training Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('lab5/outputs/mlp_results/simple_training_curves.png', dpi=150, bbox_inches='tight')
    plt.show()

    # 7. ä¿å­˜ç»“æœ
    np.save('lab5/outputs/mlp_results/simple_history.npy', history)

    print(f"âœ… ç®€åŒ–ç‰ˆMLPå®éªŒå®Œæˆ!")
    print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {sum(mlp.parameters[f'W{l}'].size + mlp.parameters[f'b{l}'].size for l in range(1, mlp.num_layers))}")

if __name__ == "__main__":
    main()