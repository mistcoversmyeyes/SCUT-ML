\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xeCJK}

% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

~\\[1cm]
\includegraphics{SCUT.png}\\[2cm]

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[1cm]
{ \huge \bfseries The Experiment Report of \textit{Machine Learning} }\\[0.6cm]
\HRule \\[2cm]
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------


\textsc{\LARGE \textbf{School:} School of Software Engineering}\\[1cm]
\textsc{\LARGE \textbf{Subject:} Software Engineering}\\[2cm]


%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Yuming Jiang
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Mingkui Tan
\end{flushright}
\end{minipage}\\[2cm]
~
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student ID:}\\
202330550601
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Grade:} \\
Software Class 3
\end{flushright}
\end{minipage}\\[2cm]

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm]


%----------------------------------------------------------------------------------------

\vfill

\end{titlepage}

% Define document title and author
	\title{Sequence-to-Sequence Neural Machine Translation with Attention Mechanism}
	\maketitle

% Write abstract here
\begin{abstract}
This experiment implements a Sequence-to-Sequence (Seq2Seq) neural machine translation system with attention mechanism for Chinese-to-English translation. The model consists of a GRU-based encoder and an attention-enhanced GRU decoder. We train the model on a filtered dataset of 459 sentence pairs from the Tatoeba corpus, achieving a BLEU score of 0.9718 on the evaluation set. The experiment demonstrates the effectiveness of the attention mechanism in capturing long-range dependencies and generating accurate translations. We analyze the attention weights visualization to understand the model's alignment behavior between source and target sequences.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
\PARstart{N}{eural} machine translation (NMT) has revolutionized the field of automatic translation by leveraging deep learning techniques to directly model the translation process from source to target language. Unlike traditional statistical machine translation systems that rely on phrase-based approaches and hand-crafted features, NMT systems learn end-to-end mappings between language pairs through neural network architectures.

The Sequence-to-Sequence (Seq2Seq) model, introduced by Sutskever et al. (2014), provides a powerful framework for sequence transduction tasks. The basic architecture consists of an encoder that compresses the source sequence into a fixed-length context vector and a decoder that generates the target sequence from this representation. However, the fixed-length bottleneck limits the model's ability to handle long sequences effectively.

To address this limitation, Bahdanau et al. (2015) proposed the attention mechanism, which allows the decoder to selectively focus on different parts of the source sequence during generation. This mechanism has become a fundamental component in modern NMT systems, enabling the model to capture long-range dependencies and improve translation quality.

In this experiment, we implement a Seq2Seq model with attention for Chinese-to-English translation. The objectives are:
\begin{itemize}
    \item Understand the Seq2Seq architecture and attention mechanism
    \item Implement encoder-decoder framework with GRU units
    \item Handle Chinese text preprocessing and character-level segmentation
    \item Train the model using teacher forcing strategy
    \item Evaluate translation quality using BLEU score
    \item Visualize and analyze attention weights
\end{itemize}

% Main Part
\section{Methods and Theory}

\subsection{Sequence-to-Sequence Architecture}

The Seq2Seq model consists of two main components: an encoder and a decoder. Given a source sequence $\mathbf{x} = (x_1, x_2, ..., x_T)$ and a target sequence $\mathbf{y} = (y_1, y_2, ..., y_{T'})$, the model learns to maximize the conditional probability:

\begin{equation}
P(\mathbf{y}|\mathbf{x}) = \prod_{t=1}^{T'} P(y_t | y_1, ..., y_{t-1}, \mathbf{x})
\end{equation}

\subsection{Encoder}

The encoder processes the input sequence and produces a sequence of hidden states. We use a Gated Recurrent Unit (GRU) as the recurrent cell:

\begin{equation}
h_t = \text{GRU}(e(x_t), h_{t-1})
\end{equation}

where $e(x_t)$ is the embedding of input token $x_t$ and $h_t$ is the hidden state at time step $t$. The encoder produces a sequence of hidden states $(h_1, h_2, ..., h_T)$.

\subsection{Attention Mechanism}

The attention mechanism computes a context vector $c_t$ as a weighted sum of encoder hidden states:

\begin{equation}
c_t = \sum_{i=1}^{T} \alpha_{ti} h_i
\end{equation}

where the attention weights $\alpha_{ti}$ are computed using:

\begin{equation}
\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^{T} \exp(e_{tj})}
\end{equation}

The energy $e_{ti}$ is calculated by:

\begin{equation}
e_{ti} = \mathbf{v}^T \tanh(\mathbf{W}_1 s_{t-1} + \mathbf{W}_2 h_i)
\end{equation}

where $s_{t-1}$ is the decoder hidden state, and $\mathbf{W}_1$, $\mathbf{W}_2$, $\mathbf{v}$ are learnable parameters.

\subsection{Decoder with Attention}

The attention-enhanced decoder generates output tokens sequentially:

\begin{equation}
s_t = \text{GRU}([e(y_{t-1}); c_t], s_{t-1})
\end{equation}

\begin{equation}
P(y_t | y_{<t}, \mathbf{x}) = \text{softmax}(\mathbf{W}_o [s_t; c_t])
\end{equation}

where $[;]$ denotes concatenation.

\subsection{Training with Teacher Forcing}

During training, we use teacher forcing with a probability of 0.5. This means that with 50\% probability, we feed the ground-truth token as input to the next time step, and otherwise use the model's own prediction. The training objective is to minimize the negative log-likelihood loss:

\begin{equation}
\mathcal{L} = -\sum_{t=1}^{T'} \log P(y_t | y_{<t}, \mathbf{x})
\end{equation}

\subsection{BLEU Score Evaluation}

We evaluate translation quality using the BLEU (Bilingual Evaluation Understudy) score, which measures n-gram precision between the generated translation and reference:

\begin{equation}
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\end{equation}

where BP is the brevity penalty and $p_n$ is the modified n-gram precision.

\section{Experiments}

\subsection{Dataset}

We use the English-Chinese translation dataset from Tatoeba, containing 24,026 sentence pairs. The data format is tab-separated with English and Chinese sentences.

\textbf{Data Preprocessing:}
\begin{itemize}
    \item Convert to lowercase and remove punctuation
    \item Segment Chinese text at character level (each character becomes a token)
    \item Filter sentences with maximum length of 10 tokens
    \item Filter to keep only sentences starting with common English prefixes (``i am'', ``he is'', ``she is'', ``you are'', ``we are'', ``they are'')
\end{itemize}

\textbf{Dataset Statistics:}
\begin{itemize}
    \item Original pairs: 24,026
    \item Filtered pairs: 459
    \item Chinese vocabulary size: 684 tokens
    \item English vocabulary size: 560 tokens
    \item Maximum sequence length: 10
\end{itemize}

The filtering strategy simplifies the translation task while maintaining meaningful sentence structures for learning.

\subsection{Implementation}

\subsubsection{Model Architecture}

Table~\ref{tab:architecture} shows the detailed model architecture.

\begin{table}[!hbt]
    \begin{center}
    \caption{Model Architecture Parameters}
    \label{tab:architecture}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        Hidden size & 256 \\
        \hline
        Embedding dimension & 256 \\
        \hline
        Encoder layers & 1 \\
        \hline
        Decoder layers & 1 \\
        \hline
        Recurrent unit & GRU \\
        \hline
        Dropout rate & 0.1 \\
        \hline
        Attention type & Additive (Bahdanau) \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Training Configuration}

Table~\ref{tab:training} presents the training hyperparameters.

\begin{table}[!hbt]
    \begin{center}
    \caption{Training Hyperparameters}
    \label{tab:training}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Hyperparameter} & \textbf{Value} \\
        \hline
        Optimizer & SGD \\
        \hline
        Learning rate & 0.01 \\
        \hline
        Number of iterations & 75,000 \\
        \hline
        Teacher forcing ratio & 0.5 \\
        \hline
        Loss function & NLLLoss \\
        \hline
        Batch size & 1 \\
        \hline
        Device & CUDA (GPU) \\
        \hline
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Training Process}

The training process involves the following steps:

\begin{enumerate}
    \item \textbf{Forward pass through encoder}: Process the Chinese input sequence character by character, obtaining encoder hidden states.
    \item \textbf{Initialize decoder}: Set the initial decoder hidden state to the final encoder hidden state.
    \item \textbf{Attention computation}: At each decoder step, compute attention weights over encoder outputs.
    \item \textbf{Generate output}: Produce the English translation token by token.
    \item \textbf{Backward pass}: Compute gradients and update parameters.
\end{enumerate}

\subsection{Results}

\subsubsection{Training Performance}

The model was trained for 75,000 iterations. The training loss decreased steadily from approximately 4.0 to below 0.5, indicating successful convergence. Figure~\ref{fig:loss} shows the training loss curve.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.45\textwidth]{loss_curve.png}
    \caption{Training Loss Curve over 75,000 iterations}
    \label{fig:loss}
\end{figure}

\subsubsection{Translation Quality}

We evaluated the trained model on 100 randomly sampled pairs and achieved a \textbf{BLEU score of 0.9718}, demonstrating excellent translation quality.

Table~\ref{tab:examples} shows representative translation examples.

\begin{table*}[!hbt]
    \begin{center}
    \caption{Translation Examples}
    \label{tab:examples}
    \begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
        \hline
        \textbf{Chinese Input} & \textbf{Reference} & \textbf{Model Output} \\
        \hline
        我 是 个 大 学 生 & i am a college student & i am a college student \\
        \hline
        他 是 教 师 & he is a teacher & he is a teacher \\
        \hline
        她 擅 長 說 英 文 & she is good at speaking english & she is good at speaking english \\
        \hline
        他 們 在 看 一 部 電 影 & they are watching a movie & they are watching a movie \\
        \hline
        我 怕 狗 & i am afraid of dogs & i am afraid of dogs \\
        \hline
    \end{tabular}
    \end{center}
\end{table*}

\subsubsection{Attention Analysis}

The attention weights provide interpretable insights into the model's alignment between source and target sequences. Figure~\ref{fig:attention} shows the attention heatmap visualization for the translation of ``我 们 非 常 需 要 食 物'' (we are badly in need of food).

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.45\textwidth]{attention_1.png}
    \caption{Attention weights visualization for translating ``我们非常需要食物'' to ``we are badly in need of food''. The x-axis shows Chinese input characters, and the y-axis shows generated English words. Brighter colors indicate higher attention weights.}
    \label{fig:attention}
\end{figure}

The attention heatmap visualization shows that:

\begin{itemize}
    \item The model learns meaningful alignments between Chinese characters and English words. For example, when generating ``food'', the model attends strongly to ``食物'' (food).
    \item Attention is concentrated on relevant source positions when generating each target word. The word ``need'' aligns with ``需要''.
    \item The attention pattern demonstrates that the model captures semantic correspondences rather than simple positional alignments.
\end{itemize}

\subsubsection{Error Analysis}

While the model achieves high BLEU scores, some errors were observed:

\begin{itemize}
    \item Synonymous translations: ``he is difficult to get along with'' vs. ``he is hard to get along with''
    \item Minor paraphrasing: ``she is dieting'' translated as ``she is on a diet''
    \item These are semantically correct but differ from the reference
\end{itemize}

These variations demonstrate the model's ability to produce fluent translations, though the small dataset size limits conclusions about true generalization capability.

\section{Conclusion}

In this experiment, we successfully implemented a Sequence-to-Sequence neural machine translation system with attention mechanism for Chinese-to-English translation. The key findings and contributions are:

\begin{enumerate}
    \item \textbf{Model Implementation}: We implemented a complete Seq2Seq architecture with GRU-based encoder and attention-enhanced decoder, demonstrating understanding of the fundamental components of neural machine translation.

    \item \textbf{Strong Performance}: The model achieved a BLEU score of 0.9718 on the evaluation set, indicating excellent translation quality for the filtered dataset.

    \item \textbf{Attention Effectiveness}: The attention mechanism enables the model to capture relevant source information for each target word generation, as evidenced by the attention weight visualizations.

    \item \textbf{Character-level Processing}: We adapted the preprocessing pipeline for Chinese text by performing character-level segmentation, which is essential for handling logographic languages.
\end{enumerate}

\textbf{Limitations and Future Work:}

\begin{itemize}
    \item \textbf{Overfitting Concern}: The extremely high BLEU score (0.9718) should be interpreted with caution. With only 459 training pairs and 75,000 iterations, each sentence was seen approximately 163 times during training. Additionally, the evaluation was performed on the training set itself, not on a held-out test set. Therefore, this high score primarily reflects the model's memorization of the training data rather than true generalization capability.
    \item The current model is trained on a small filtered dataset (459 pairs). Scaling to larger datasets would require architectural improvements and more sophisticated training strategies.
    \item The model uses character-level segmentation for Chinese, which may not capture semantic word boundaries. Using word segmentation tools could improve performance.
    \item Future work could explore transformer-based architectures, which have shown superior performance in machine translation tasks.
    \item Implementing beam search decoding instead of greedy decoding could improve translation quality.
    \item A proper train/validation/test split should be used to evaluate true generalization performance.
\end{itemize}

This experiment provides hands-on experience with the core concepts of neural machine translation and attention mechanisms, forming a foundation for understanding more advanced NMT systems like the Transformer architecture.

\end{document}
