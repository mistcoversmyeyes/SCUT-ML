# 1. 随堂实验：

## 实验一：**线性回归与梯度下降**

**实验助教：**&#x9648;穗章

**对应章节**：第二章（线性回归、梯度下降）

**实验目的**

1. 掌握线性回归模型的构建、训练及评估流程；

2. 手动实现梯度下降算法，理解参数优化的迭代过程，进一步理解线性回归，闭式解和梯度下降的原理；

3. 分析学习率、迭代次数对模型收敛的影响，在小规模数据集上实践，体会优化和调参的过程。

**实验内容**

* **数据集**

线性回归使用的是LIBSVM Data中的Housing数据，包含506个样本，每个样本有13个属性，并将其切分为训练集，测试集。

链接：

https://www.csie.ntu.edu.tw/\~cjlin/libsvmtools/datasets/regression.html#housing

* **核心任务**

通过线性回归模型预测房价，对比手动实现与库函数的效果，分析梯度下降参数对结果的影响。

**实验要求**

1. 独立完成数据预处理（缺失值处理、特征标准化）；

2. 利用matlab、c语言或python3求取线性回归的闭式解、Loss函数值

3. 手动编写单变量线性回归的梯度下降代码（需推导更新公式）；

4. 使用 sklearn 实现多变量线性回归，对比两种实现的 MSE（均方误差）；

提交包含代码、损失曲线、参数分析的实验报告。

**实验环境**

可使用matlab、c语言或python3完成，鼓励使用c语言

**提交信息**

**提交内容**：实验报告（**pdf格式，使用英文模板撰写**）与程序代码（**zip格式**），两者均命名为：**实验2-姓名-学号** &#x20;

**提交方式**：**待定**

**提交截止时间**：**待定**

**实验形式**

个人独立完成

**实验步骤**

本次实验代码及画图均在jupyter上完成

1. **数据准备**

   * 下载并读取housing数据集，使用sklearn库的load\_svmlight\_file函数读取数据。

   * 对数据集进行归一化

   * 将数据集切分为训练集和测试集。使用train\_test\_split函数切分数据集（训练集80%和测试集20%）

2. **线性回归的闭式解**

   * 定义MSE（均方误差）损失函数。

   * 获取闭式解的公式，过程详见课件ppt。

   * 通过闭式解得到参数W的值。

   * 在训练集上测试并获得Loss函数值loss\_train，在验证集上获得Loss函数值loss\_val。

   * 输出loss\_train和loss\_val的值。

3. **单变量线性回归实现**

   * 从数据集中提取出房间数特征X\_train\_rm、X\_val\_rm

   * 对损失函数进行求导，推导出w、b的梯度

   * 设置学习率alpha（如 0.1、0.01）和迭代次数epochs（如 1000 次）

   * 根据梯度下降更新公式（w = w - α·∂L/∂w，b = b - α·∂L/∂b）更新参数

   * 在训练集上测试并得到Loss函数值loss\_train，在验证集上测试并得到Loss函数值loss\_val

   * 输出loss\_train和loss\_val的值，画出loss\_train和loss\_val变化曲线图

4. **多变量线性回归对比**

   * 导入sklearn 的LinearRegression库函数。

   * 用训练集的全部特征 X\_train 和 y\_train 训练 LinearRegression()（默认使用最小二乘闭式解或 SVD）。

   * 在验证集上计算 MSE，和单变量模型比较性能差异。

5. **参数分析报告**

   * 修改学习率alpha，观察不同学习率下损失函数下降曲线变化

   * 总结梯度下降的核心原理及参数选择技巧

&#x20;

整理实验结果并完成实验报告 (实验报告模板将包含在示例仓库中)

示例仓库及实验报告模板：https://github.com/tan1n0/MachineLearningExperiment2025-lab1

## 实验二：分类算法对比 —— 鸢尾花 / 乳腺癌数据集分类

**实验助教**：张华铨

**对应章节**：第三章（线性分类、SVM）、第四章（逻辑回归）

**实验目的**

1. 掌握逻辑回归、SVM 的分类原理与实现方法；

2. 学会用准确率、混淆矩阵、ROC 曲线评估分类模型性能；

3) 对比不同分类器在二分类与多分类任务中的表现。

**实验内容**

**数据集**：在LIBSVM上下载两个数据集：

二分类用 “乳腺癌数据集”（特征为肿瘤细胞属性，标签为良性 / 恶性）

链接：https://www.csie.ntu.edu.tw/\~cjlin/libsvmtools/datasets/binary/breast-cancer\_scale

多分类用 “鸢尾花数据集”（特征为花萼 / 花瓣尺寸，标签为 3 类花卉）

链接：https://www.csie.ntu.edu.tw/\~cjlin/libsvmtools/datasets/multiclass/iris.scale

**核心任务**：实现逻辑回归、线性 SVM、核 SVM 模型，对比其在两类数据集上的分类效果。

**实验要求**

1. 完成数据可视化（特征散点图、类别分布直方图）；

2. 独立实现模型训练（含参数调优）与性能评估，提交可运行代码；

3. 提交对比报告，包含不同模型的准确率、混淆矩阵及结果分析。

**实验环境**

可使用matlab、c语言或python3完成，鼓励使用c语言

**提交信息**

提交内容：实验报告（pdf格式，使用英文模板撰写）与程序代码（zip格式），两者均命名为：实验2-姓名-学号 &#x20;

提交方式：待定

提交截止时间：待定

**实验形式**

个人独立完成

**实验步骤**

1. 数据可视化与预处理：

&#x20;       ◦ 加载数据集，绘制鸢尾花 “花瓣长度 - 花瓣宽度” 散点图（按类别着色）、乳腺癌数据集特征分布直方图；

&#x20;       ◦ 划分训练集（70%）和测试集（30%），对特征进行标准化。

* 模型训练与调优：

&#x20;       ◦ 实现逻辑回归（调参C值，控制正则化强度）、线性 SVM（调参C值）、RBF 核 SVM（调参C和gamma）；

&#x20;       ◦ 用 5 折交叉验证选择最优参数，记录各模型在验证集上的平均准确率。

* 性能评估：

&#x20;       ◦ 在测试集上计算各模型准确率，绘制乳腺癌数据集的 ROC 曲线（逻辑回归、SVM）；

&#x20;       ◦ 生成鸢尾花数据集的混淆矩阵，分析 “易混淆类别”（如 Setosa 与 Versicolor）。

* 结果分析：

&#x20;       ◦ 对比线性模型（逻辑回归、线性 SVM）与核 SVM 在非线性数据上的性能差异；

&#x20;       ◦ 总结 “何时选择线性分类器，何时需要核函数”，完成实验报告。



分类算法实现示例仓库及实验报告模板：https://github.com/kanghailong99/lab2

可以参考示例仓库代码，根据实验步骤修改代码完成实验二内容。

# 2. 基础课程实验：

## 实验三：**PCA 降维与可视化 —— 高维数据降维实践（2学时）**

**实验助教：**&#x718A;振东

**对应章节**：第七章（维度约简 PCA）

#### **一、 实验目的**

1. 掌握 PCA 的原理与实现，理解 “最大方差” 准则；

2. 学会用 PCA 对高维数据降维并可视化；

3. 分析降维对分类任务的精度与效率的影响。

#### **二、 实验内容**

* **数据集**：MNIST 手写数字数据集（部分样本，28×28 像素，共 784 维特征）。

* **核心任务**：用 PCA 对高维图像数据降维，可视化降维结果，对比降维前后的分类性能。

#### **三、 实验要求**

1. 加载 MNIST 数据集（取 0-9 类各 100 个样本），完成数据标准化；

2. 实现 PCA（可调用 sklearn），计算主成分的方差贡献率；

3. 提交降维可视化结果（2 维散点图）、分类性能对比表及分析报告。

#### **四、 实验环境（参考)**

* python3

* numpy&#x20;

* matplotlib&#x20;

* scikit-learn&#x20;

* tensorflow

#### **五、 提交信息**

* **提交内容**：实验报告（**pdf格式，使用英文模板撰写**）与程序代码（**zip格式**），两者均命名为：**实验2-姓名-学号** &#x20;

* **提交方式**：**待定**

* **提交截止时间**：**待定**

#### **六、 任务步骤**

1. **数据预处理**：

   * 加载 MNIST 数据集，选取 1000 个样本（10 类，每类 100 个）；

   * 将图像展平为 784 维向量，对特征进行标准化（均值为 0，方差为 1）。

2. **PCA 降维与方差分析**：

   * 用 sklearn 的PCA进行降维，保留前 20 个主成分；

   * 计算各主成分的方差贡献率，绘制 “累计方差贡献率曲线”，确定 “保留 95% 方差所需的主成分数”。

3. **降维可视化与分类对比**：

   * 将数据降维至 2 维，绘制散点图（按数字类别着色），观察同类样本的聚类效果；

   * 分别在原始数据（784 维）和降维数据（保留 95% 方差）上训练 SVM 分类器，记录：

     * 测试集准确率；

     * 模型训练时间（用time模块计时）。

4. **结果分析（30 分钟）**：

   * 分析 “降维如何影响分类效率与精度”，解释 “为何降维后准确率可能略有下降但训练更快”；

   * 总结 PCA 在 “数据可视化、噪声去除、模型加速” 中的应用场景，完成实验报告。

实验参考代码：https://github.com/xiongzhendong/Mechine-Learing-Lessons/blob/main/lab3

## **实验四：基于 AdaBoost 算法的人脸检测**

**实验助教**：毛锐

**对应章节**：第四章（集成学习算法：Adaboost）

### **实验目的**

1. 深入理解 Adaboost 算法的核心原理：包括弱分类器的迭代训练过程、样本权重更新机制及加权投票决策逻辑；

2. 掌握人脸检测的基本流程：Haar 特征提取、特征筛选与分类器训练；

3. 学会使用 AdaBoost 构建人脸分类器，并对比开源工具（如 OpenCV）的实现效果，理解理论到工程应用的转化；

4. 完整体验机器学习项目流程：数据预处理→模型训练→性能评估→结果分析。

### **实验内容**

1. **数据集**：使用扩展 Yale B 人脸数据集（含 15 个人的 165 张人脸图像，附非人脸图像如风景、动物图作为负样本）；

2. **核心任务**：

   * 手动实现基于 Haar 特征的 AdaBoost 人脸分类器，训练并评估其在验证集上的精确率、召回率；

   * 调用 OpenCV 中基于 Haar 特征 + AdaBoost 的预训练模型，测试其在复杂场景（如多人、侧脸、光照变化）下的检测效果；

   * 对比自研模型与 OpenCV 模型的性能差异，分析原因。

### **实验要求**

1. 独立完成人脸 / 非人脸数据的预处理（尺寸统一为 24×24 像素，灰度化）；

2. 手动实现 AdaBoost 核心逻辑（弱分类器选择、样本权重更新、强分类器集成），提交可运行代码；

3. 记录自研模型在验证集上的精确率（Precision）、召回率（Recall），并与 OpenCV 模型的检测结果对比；

4. 提交实验报告，包含 Haar 特征提取过程、AdaBoost 迭代曲线、两种模型的性能分析。

5. 提交内容：实验报告（pdf格式，使用英文模板撰写）与程序代码（zip格式），两者均命名为：实验4-姓名-学号 &#x20;

6. 提交方式：待定

7. 提交截止时间：待定

8. 实验形式：个人独立完成

### **实验环境**

**（参考）**

Anaconda3

Python 3.9.23

Numpy 1.23.5

Opencv-python 4.7.0.72

Matplotlib 3.7.3

### **任务步骤**

1. **数据准备与预处理**：

   * 下载扩展 Yale B 人脸数据集（正样本）和 100 张非人脸图像（负样本），划分训练集（70%）、验证集（30%）；

   * 对所有图像进行预处理：转为灰度图，resize 至 24×24 像素，存储为 numpy 数组。

2. **Haar 特征提取**：

   * 实现 Haar 特征计算：包括边缘特征（如水平 / 垂直边缘）、线性特征（如明暗块），每个样本提取不少于 1000 个 Haar 特征；

   * 对特征进行归一化（如缩放至 \[0,1]），减少光照差异影响。

3. **AdaBoost 模型训练**：

   * 初始化样本权重（正 / 负样本权重平均分配）；

   * 迭代训练弱分类器（选择决策树桩作为弱分类器）：

     * 每次迭代中，根据样本权重选择最优弱分类器（最小加权错误率）；

     * 计算弱分类器权重（基于错误率），更新样本权重（错分样本权重提高，正确样本降低）；

     * 集成弱分类器为强分类器，记录每轮迭代在验证集上的精确率。

4. **OpenCV 对比实验**：

   * 调用 OpenCV 的CascadeClassifier加载预训练的 Haar+AdaBoost 模型（haarcascade\_frontalface\_default.xml）；

   * 对测试图像（含正 / 负样本）进行检测，记录检测结果（正确检测数、误检数、漏检数）。

5. **结果分析**：

   * 对比自研模型与 OpenCV 模型的精确率、召回率，分析差异原因（如特征数量、弱分类器数量）；

   * 绘制 AdaBoost 迭代过程中验证集精确率曲线，总结 “弱分类器数量对强分类器性能的影响”；

   * 完成实验报告，附关键代码与检测结果截图。

### **实验参考步骤**

**一、获取代码**

实验参考代码：https://github.com/AXIBA082/lab4

**二、实验环境安装**

1. 确保已安装 Conda 环境：实验推荐使用 Conda 进行环境管理。

2. 创建并激活 Conda 虚拟环境：

```python
#创建名为 lab4 的虚拟环境，并指定 Python 版本为 3.9.23
conda create --name lab4 python=3.9.23
#激活虚拟环境
conda activate lab4
```

* 安装依赖库：项目所需的依赖库已在 requirements.txt 文件中列出，使用 pip 命令一键安装。

```python
#安装依赖
pip install -r requirements.txt

#requirements.txt 文件内容如下：
#numpy==1.23.5
#opencv-python==4.7.0.72
#matplotlib==3.7.3
```

**三、数据准备与预处理**

1. 下载数据集：

* 下载 Extended Yale B 人脸数据集，并将其解压到项目的数据目录 data/ExtendedYaleB/ 下。

  * Extended Yale B 人脸数据集（官方）：http://vision.ucsd.edu/datasets/extended-yale-face-database-b-b

  * 网盘： https://pan.baidu.com/s/1lj7zfbqqCgcUZlxoITxpDg?pwd=x4e5

* 准备非人脸图像（如风景、动物等），并将它们放入 data/nonfaces/ 目录下。

  * 参考非人脸图像数据集：https://pan.baidu.com/s/16J-Kl8c0nUHNEGFpyRAj6Q?pwd=m87w&#x20;

- 采样人脸数据：

运行 sample\_extended\_yaleb.py 脚本，从 Extended Yale B 数据集中随机采样15位不同人物，每人11张图像，共165张人脸图像作为正样本。

```python
python sample_extended_yaleb.py --src "data/ExtendedYaleB" --out "data/faces" --folders 15 --per-folder 11 --seed 42
```

执行完毕后，data/faces/ 目录下会生成采样好的人脸图像。

* 确认数据结构：

此时，data 目录结构应如下所示。

```python
data/
├── ExtendedYaleB/   # 原始数据集
├── faces/           # 采样后的人脸图像（正样本）
└── nonfaces/        # 非人脸图像（负样本）
```

**四、训练和评估自研 AdaBoost 模型**

1. 执行训练脚本：

运行 adaboost\_face.py ，开始训练AdaBoost 人脸检测模型。该脚本会自动完成数据预处理、Haar 特征提取、模型训练和评估的全过程。

```python
python adaboost_face.py --data-root "./data" --split-ratio 0.7 --seed 42 --rounds 50 --features 2000 --save-dir outputs --opencv-eval --export-npz
```

* 参数说明：

  * data-root: 数据集的根目录路径。

  * rounds: AdaBoost 训练的迭代轮数（弱分类器数量），默认为50。

  * features: 每个样本提取的 Haar 特征数量，默认为2000。

  * save-dir: 保存输出结果的目录，例如 outputs。

  * opencv-eval: 关键参数，启用此项会在训练结束后，自动调用 OpenCV 的预训练模型在相同的验证集上进行评估，以便对比。

  * export-npz: 将预处理后的数据（24x24灰度图）保存为 .npz 文件，方便下次快速加载。

  * split-ratio: 训练集与验证集的划分比例，默认为0.7。

  * seed: 随机种子，确保每次数据划分的结果一致。

* 查看训练结果：

训练完成后，在 outputs 目录下会生成一系列结果文件。

**五、OpenCV 模型对比与结果分析**

1. 获取 OpenCV 评估结果：

   * 如果在上一步中添加了 --opencv-eval 参数，程序会自动在验证集上评估 OpenCV 的 haarcascade\_frontalface\_default.xml 预训练模型。

   * 评估结果保存在 outputs/opencv\_eval.json 文件中，记录了其精确率、召回率和混淆矩阵。

2. 结果分析与报告撰写：

   * 对比性能：比较 adaboost\_eval.json 和 opencv\_eval.json 中的精确率（Precision）和召回率（Recall），分析自研的模型与 OpenCV 预训练模型之间的性能差异。可以从特征数量、训练轮数（弱分类器数量）、数据集差异等方面思考原因。

   * 分析迭代曲线：打开 training\_curves.png 图像，观察验证集的精确率和召回率曲线。总结弱分类器的数量（即迭代轮数）对最终强分类器性能的影响规律。

   * 撰写实验报告：整理以上所有步骤、结果和分析，附上关键代码截图、training\_curves.png 曲线图以及两种模型的性能对比表格，完成实验报告。



## 实验五：简单神经网络实现-----手写数字识别

实验助教：边浪宇

对应章节：第十章（图像处理）



### **一、 实验目的**

1. **掌握核心原理**：通过从零开始构建一个多层感知机（MLP），深入理解神经网络的前向传播、反向传播、梯度下降和参数更新等核心机制。

2. **学习现代框架**：学习并熟练使用主流深度学习框架（以 PyTorch 为例），掌握构建、训练和评估卷积神经网络（CNN）的标准流程。

3) **对比模型架构**：通过实践，深刻理解 MLP 与 CNN 在处理图像数据时的根本差异，特别是 CNN 在权重共享、局部感受野等方面的优势。

4) **培养分析能力**：能够从性能、效率（参数量）、收敛速度等多个维度，科学地对比和分析不同神经网络模型的优劣，并能解释其背后的原因。

5. **提升工程能力**：熟练运用 Python 和 NumPy, PyTorch 等库完成一个完整的机器学习项目，包括数据处理、模型搭建、训练、评估和结果可视化。



### **二、 实验内容**

本实验包含两大部分，要求学生针对经典的 MNIST 手写数字识别任务，分别实现 MLP 和 CNN 两种模型，并进行详细的对比分析。

1. **Part A: 从零实现多层感知机 (MLP)**

   * **方法**：不使用任何深度学习框架（如 PyTorch, TensorFlow），仅依赖 `NumPy` 库。

   * **任务**：

     * 对 MNIST 图像数据进行预处理（扁平化、归一化）。

     * 对标签数据进行独热编码。

     * 构建一个包含输入层、单个隐藏层和输出层的 MLP 网络。

     * 手动实现前向传播、交叉熵损失函数、反向传播和梯度下降优化器。

     * 编写训练循环，训练模型并在测试集上评估其准确率。

2. **Part B: 使用框架实现卷积神经网络 (CNN)**

   * **方法**：使用 `PyTorch` 深度学习框架。

   * **任务**：

     * 使用 `torchvision` 和 `DataLoader` 对 MNIST 数据进行加载和预处理（保持二维结构、归一化）。

     * 设计并构建一个经典的卷积神经网络结构（如 LeNet-style），包含卷积层、池化层和全连接层。

     * 使用 PyTorch 内置的损失函数（`nn.CrossEntropyLoss`）和优化器（`optim.Adam`）。

     * 编写标准的 PyTorch 训练循环，训练模型并在测试集上评估其准确率。



### **三、 实验要求**

1. **代码实现**：

   * 提交两个独立的、可完整运行的 Python 脚本或 Jupyter Notebook 文件 (`mlp_from_scratch.py`, `cnn_with_pytorch.py`)。

   * 代码逻辑清晰，结构合理，并附有必要的注释来解释关键部分。

   * **性能基准**：实现的 MLP 模型在测试集上的准确率应\*\*高于 90%\*\*；CNN 模型准确率应\*\*高于 98%\*\*。

2. **实验报告**：

   * 撰写一份详细的实验报告（.md 或 .pdf 格式），内容应至少包含：

     * **实验简介**：简述实验目的和内容。

     * **模型设计**：分别描述 MLP 和 CNN 的网络结构、激活函数选择等。

     * **核心实现**：(针对 MLP) 详细阐述反向传播算法的推导和代码实现思路。

     * **结果展示**：

       * 清晰列出两个模型在测试集上的最终准确率。

       * 绘制并展示两个模型在训练过程中的损失（Loss）变化曲线图。

     * **对比分析**：(报告的核心) 深入对比分析 MLP 和 CNN，详见下面的“思考与扩展”部分。

     * **结论与心得**：总结实验收获以及遇到的问题和解决方法。



### **四、 实验环境**

* **硬件**：普通个人计算机即可（CNN 部分若有 GPU 可加速，但非必需）。

* **操作系统**：Windows / macOS / Linux。

* **编程语言**：Python 3.7+。

* **核心库**：

  * `numpy`：用于 MLP 的数值计算。

  * `matplotlib`：用于数据可视化（如绘制损失曲线）。

  * `tensorflow` 或 `keras`：仅用于方便地加载 MNIST 数据集。

  * `torch` 和 `torchvision`：用于 CNN 的构建和训练。

* **建议开发环境**：Jupyter Notebook 或 VS Code。



### **五、 实验步骤**

**Part A: MLP 从零实现**

1. **环境配置**：导入 `numpy`, `matplotlib` 和 `tensorflow.keras.datasets`。

2. **数据加载与预处理**：

   * 加载 MNIST 数据集。

   * 将 28x28 的图像\*\*扁平化\*\*为 784x1 的向量。

   * 将像素值从 \[0, 255] \*\*归一化\*\*到 \[0, 1]。

   * 将标签 \[0-9] \*\*独热编码\*\*为 10 维向量。

   * **检查维度**：确保处理后的数据维度正确。

3) **实现核心函数**：

   * `initialize_parameters()`: 初始化权重和偏置。

   * `relu()`, `softmax()`: 实现激活函数。

   * `forward_propagation()`: 实现前向传播逻辑。

   * `compute_loss()`: 实现交叉熵损失。

   * `backward_propagation()`: **（关键步骤）** 根据链式法则计算各层参数的梯度。

   * `update_parameters()`: 根据梯度和学习率更新参数。

4) **模型训练**：

   * 编写主训练循环，迭代执行“前向传播 -> 计算损失 -> 反向传播 -> 更新参数”。

   * 在训练过程中定期打印损失和准确率，监控训练状态。

5. **模型评估**：

   * 训练结束后，在测试集上运行前向传播，计算最终准确率并记录。



**Part B: CNN 使用 PyTorch 实现**

1. **环境配置**：导入 `torch`, `torch.nn`, `torch.optim`, `torchvision` 等。

2. **数据加载与预处理**：

   * 使用 `torchvision.transforms` 定义数据转换（转为 Tensor、归一化）。

   * 使用 `torchvision.datasets.MNIST` 加载数据集。

   * 使用 `torch.utils.data.DataLoader` 创建数据加载器，实现数据分批。

   * **注意**：此时数据应保持 `(N, 1, 28, 28)` 的四维格式。

3) **构建模型**：

   * 定义一个继承自 `nn.Module` 的 `CNN` 类。

   * 在 `__init__` 方法中定义网络层，如 `nn.Conv2d`, `nn.MaxPool2d`, `nn.Linear`。

   * 在 `forward` 方法中定义数据的前向传播路径。

4) **模型训练**：

   * 实例化模型、损失函数 (`nn.CrossEntropyLoss`) 和优化器 (`optim.Adam`)。

   * 编写主训练循环，遍历 `DataLoader`，在每个批次上执行：

     1. 梯度清零 `optimizer.zero_grad()`。

     2. 前向传播 `model(inputs)`。

     3) 计算损失 `criterion(...)`。

     4) 反向传播 `loss.backward()`。

     5. 参数更新 `optimizer.step()`。

5. **模型评估**：

   * 编写评估函数，将模型设为评估模式 (`model.eval()`)，在测试集上计算最终准确率并记录。



### **六、 思考与扩展**

请在实验报告的“对比分析”部分，深入回答以下问题：

1. **性能与效率**：

   * MLP 和 CNN 哪个模型在 MNIST 任务上表现更好？为什么？

   * 计算并比较两个模型的总参数数量。为什么 CNN 在参数更少（或相当）的情况下，通常能达到更高的精度？（提示：权重共享）

2. **数据处理的哲学**：

   * MLP 的“扁平化”操作丢失了图像的什么重要信息？

   * CNN 是如何通过其结构（卷积核）来利用这些信息的？（提示：空间局部性）

3) **模型的泛化能力**：

   * 想象一下，如果测试集中的数字被轻微平移或旋转，哪个模型（MLP 还是 CNN）的性能下降会更小？为什么？（提示：平移不变性）

4) **应用场景**：

   * 如果任务是根据用户的年龄、收入、职业等表格数据来预测其信用等级，你会选择 MLP 还是 CNN？请说明理由。

5. **（选做）进阶探索**：

   * 尝试为你的 CNN 模型添加 **Dropout** 层或 **Batch Normalization** 层，观察其对训练过程和最终性能的影响。

   * 尝试将你的 NumPy 版 MLP 优化器从普通梯度下降改为带动量的梯度下降（Momentum）。

七、参考代码

代码参考：https://github.com/BianLangyu/Lab5.git

# 3. 综合实验

## **实验六：基于神经网络的人脸检测**

**实验助教**：吴昊

**对应章节**：第八章（神经网络与深度学习）

### **实验目的**

1. 掌握 MTCNN（多任务级联卷积神经网络）的核心原理：理解 PNet、RNet、ONet 三级网络的分工与串联逻辑；

2. 理解人脸检测中的多任务学习：边框回归（Bounding Box Regression）与人脸关键点检测（如眼睛、鼻子位置）；

3. 学会使用深度学习框架（如 PyTorch）搭建级联神经网络，训练并评估人脸检测模型；

4. 分析网络结构、训练数据对检测精度的影响。

### **实验内容**

* **数据集**：训练PNet和RNet使用WIDER Face 数据集（含 32203 张图像，159424 个人脸标注，包含不同尺度、姿态、光照的人脸）

**&#x20;    链接：**[WIDER FACE: A Face Detection Benchmark](http://shuoyang1213.me/WIDERFACE/)

&#x20;    训练Onet使用FaceDection的Training Dataset数据集（包含5,590张LFW图像以及7876张网络图像）&#x20;

**&#x20;    链接：**[CNN for Facial Point Detection](http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm)

* **核心任务**：基于 MTCNN 架构，分别训练 PNet（候选框生成）、RNet（候选框筛选）、ONet（精细检测与关键点定位），测试模型在复杂场景下的检测性能。

### **实验要求**

1. 完成数据集预处理：人脸区域裁剪、边框坐标归一化、关键点坐标标注；

2. 用 PyTorch 搭建 MTCNN 的三级网络结构，提交网络定义代码；

3. 记录训练过程（损失曲线、准确率），测试模型的平均精度（AP）；

4. 提交实验报告，包含网络结构设计、训练参数对结果的影响分析。

### **实验环境**

anaconda3

pytorch&#x20;

torchvision

opencv-python

tensorboard

tensorboardX

### **任务步骤**

1. **数据预处理**：

   * 下载 WIDER Face 数据集，划分训练集（80%）、验证集（20%）；

   * 提取标注信息：人脸边框坐标（x1,y1,x2,y2）、5 个关键点坐标（左眼、右眼、鼻子、左嘴角、右嘴角）；

   * 生成训练样本：

     * PNet 样本：随机裁剪图像 patches（含人脸 / 非人脸，比例 1:3），尺寸 12×12；

     * RNet 样本：从 PNet 输出中裁剪候选框，尺寸 24×24；

     * ONet 样本：从 RNet 输出中裁剪候选框，尺寸 48×48。

2. **网络搭建**：

   * 搭建 PNet（全卷积网络）：输入 12×12×3，输出候选框得分（人脸概率）、边框偏移量；

   * 搭建 RNet（卷积 + 全连接）：输入 24×24×3，输出筛选后的得分、边框偏移量；

   * 搭建 ONet（更深卷积 + 全连接）：输入 48×48×3，输出最终得分、边框偏移量、5 个关键点坐标；

   * 定义多任务损失函数：分类损失（交叉熵）+ 边框回归损失（L2）+ 关键点损失（L2）。

3. **模型训练**：

   * 按 PNet→RNet→ONet 顺序依次训练（前级网络输出作为后级输入）；

   * 设置训练参数：batch size=32，学习率 = 0.001，Adam 优化器，训练 20 个 epoch；

   * **每 5 个 epoch 在验证集上测试，记录平均精度（AP）和关键点误差。**

4. **模型测试**：

   * 用训练好的 MTCNN 对复杂场景图像（如多人、侧脸、模糊图像）进行检测；

   * **可视化检测结果（绘制边框和关键点），统计漏检率、误检率。**

5. **结果分析**：

   * 分析三级网络的作用（如 PNet 快速过滤非人脸区域，ONet 提升精度）；

   * 总结 “训练数据多样性（如姿态、光照）对模型泛化能力的影响”；

   * **完成实验报告，附网络结构示意图、训练损失曲线及检测结果截图**。

### **实验参考步骤**

**一、获取代码**

实验完整代码已给出mtcnn\_pytorch，网站为：https://github.com/kanghailong99/lab4

**二、实验环境安装**

1.确保本机或是服务器已安装好conda环境;

2.pip或conda安装pytorch和 torchvision环境&#x20;

3.pip或conda安装opencv-python环境

4.pip或conda安装tensorboardX

```python
# Windows
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116# Windows
# Linux
pip install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl
# All
pip install opencv-python   
pip install tensorboardx
```

**三、简单测试给定模型**

直接使用我们训练好的网络模型在给定的测试数据集(位于mtcnn\_pytorch/data/test\_images/目录下,共64张测试图片)，运行以下命令，即可在mtcnn\_pytorch/data/you\_result/目录下查看检测结果

```python
cd mtcnn pytorch/
python test_image.py
```



**四、训练自己的模型**

**注意:在训练过程中，检查训练数据集路径是否与你本机或服务器存放路径一致，若不一致，则需修改运行命令中的prefix\_path参数**

**(训练数据集路径不要包含中文)**

**训练数据集**

为了训练PNet和RNet，将[WiderFace数据集](http://shuoyang1213.me/WIDERFACE/)用于人脸分类和人脸边界框回归;为了训练ONet，将[WiderFace](http://shuoyang1213.me/WIDERFACE/)用于人脸分类和人脸边界框回归，再使用[Training Dataset](http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm)用于人脸特征点回归

> **注意！！在训练过程中请大家将中间结果（例如loss值等）保存为文件，例如使用csv包将loss数组保存为csv文件，方便后续重绘loss值曲线。否则重新训练时间成本较大。**



下面训练集路径以 WIDER/WIDER\_train/images 为例

1.训练PNet网络

```python
cd mtcnn_pytorch
python preprocessing/gen_pnet_data.py --prefix_path WIDER/WIDER_train/images
python preprocessing/assemble_pnet_imglist.py
python training/pnet/train.py
```

2.训练RNet网络

```python
cd mtcnn_pytorch
python preprocessing/gen_rnet_data.py --prefix_path WIDER/WIDER_train/images
python preprocessing/assemble_rnet_imglist.py
python training/rnet/train.py
```

> **注意！！在训练过程中，可以适当减小batch\_size，在运行gen\_rnet\_data.py时，可以修改代码，不需要完整跑完所有batch\_idx，可以将以下代码注释，跑部分图片即可**
>
> **assert len(det\_boxes) == num\_of\_images, "incorrect detections or ground truths"**
>
> ***完整运行所有图片可能导致训练时爆内存***

3.训练ONet网络

(首先使用WiderFace训练，再使用Training Dataset训练)

```python
cd mtcnn_pytorch
python preprocessing/gen_landmark_48.py --prefix_path train
python preprocessing/gen_onet_data.py --prefix_path WIDER/WIDER_train/images
python preprocessing/assemble_onet_imglist.py
python training/onet/train.py
```

> **注意！！在训练过程中，可以适当减小batch\_size，在运行gen\_onet\_data.py时，可以修改代码，不需要完整跑完所有batch\_idx，可以将以下代码注释，跑部分图片即可**
>
> **assert len(det\_boxes) == num\_of\_images, "incorrect detections or ground truths"**
>
> ***完整运行所有图片可能导致训练时爆内存***

训练完成，即可在mtcnn\_pytorch/results/目录下得到三个训练好的网络模型

4.测试自己训练好的模型

在给定的测试数据集(位于mtcnn\_pytorch/data/test images/目录下，共64张测试图片)简单测试自己训练好的网络模型，运行以下命令，即可在mtcnn\_pytorch/data/you result/目录下查看检测结果

```python
cd mtcnn_pytorch/
python test_youModel_images.py
```

分类算法实现示例仓库及实验报告模板：[GitHub - kanghailong99/lab4](https://github.com/kanghailong99/lab4)

MTCNN论文原文链接：[\[1604.02878\] Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](https://arxiv.org/abs/1604.02878)&#x20;

**可以参考示例仓库代码和论文原文理解MTCNN网络架构。**

**请根据具体任务步骤修改代码完成实验六的实际实验内容。**

> **注意:**
>
> * 检查test\_youModel\_images.py中的模型路径是否与你训练好的模型路径一致，若不一致，则修改路径，整理实验结果并完成实验报告。
>
> * 模型训练算力消耗较大，同学们可调小batch size，并且没必要按照示例程序训练全部epoch。训练部分的目的在于理解程序运行过程而非得到最终模型。
>
> * 本次实验将使用JetAutoML平台，使用过程中需要将**任务步骤中要求的结果(loss曲线等)上传到平台上**，平台可直接可视化结果。因此需要在结果生成处添加少量代码以完成结果上传。具体使用方法待定。



## 实验七：基于序列模型的中英文翻译机

**实验助教**：杨东金

### **实验目的**

1. 初步了解自然语言处理（NLP）的基本概念与任务。

2. 理解并掌握经典的Sequence-to-Sequence (Seq2Seq) 模型结构及其在机器翻译中的应用。

3. 学习并实践注意力（Attention）机制，理解其如何解决长序列依赖问题。

4. 能够利用PyTorch等深度学习框架搭建、训练和评估一个简单的神经机器翻译模型。

5. 熟悉BLEU等机器翻译任务中常用的评估指标。

### **数据集描述**

1. 本实验采用中英文翻译数据集，数据集已在实验仓库的 `./data/eng-cmn.txt` 路径下。

2. 数据集包含超过20,000个翻译数据对。每行数据由“英文\t中文\t属性信息”构成，字段间由制表符（\t）分隔。

### **实验内容**

本次实验将基于一个 Encoder-Decoder 框架，构建一个从中文到英文的神经机器翻译模型。实验将重点引入注意力（Attention）机制，以解决传统 Seq2Seq 模型在处理长句子时的信息瓶颈问题。整个过程包括数据预处理、词典构建、模型搭建、模型训练，并最终使用 BLEU 分数对翻译质量进行定量评估。

### **实验要求**

1. 以团队形式协作的形式完成实验，并在实验报告中明确自己的贡献。

2. 成功运行完整的代码流程，包括数据处理、模型训练和评估，并理解各部分代码模块的功能。

3. 生成并保存模型训练过程中的损失变化曲线，并作相关记录。

4. 使用 BLEU 指标评估训练后模型的翻译性能。

5. 复现展示可视化的注意力权重矩阵，并结合具体翻译案例，简要分析其工作原理。

6. 按照要求撰写并提交实验报告和完整的项目代码。

**实验环境**

* Python 3，推荐使用 Anaconda3 进行环境管理，熟悉创建虚拟环境，激活，配置以及删除虚拟环境。

* Python 包：`pytorch`, `numpy`, `matplotlib`, `nltk` 等。

### **任务步骤**

1. **实验准备**

   * **代码与数据**： `https://github.com/kanghailong99/lab7`&#x20;

   * **环境配置**：按照实验的虚拟环境。

2. **数据预处理**

   * **读取与清洗**：按行读取 `./data/eng-cmn.txt` 文件。注意原始数据格式，需正确提取中英文句子对，并移除多余的属性信息。构建训练数据对时注意移除属性信息 (每行只取前两个数据)

   ```python
   pairs = [(normalizeString(s) for s in l.split('\t')[:2]] for l in lines]

   # 其中normalizestring函数中的正则表达式需对应更改，否则会将中文单词替换成空格。

   def normalizestring(s):
           s=s.lower().strip()
           if ' ' not in s:
                   s = list(s)
                   s = ' '.join(s)
                   s = unicodeToAscii(s)
                   s = re.sub(r"([.!?])", r" \1"，s)
                   return s
   ```

   * **文本规范化**：对中英文文本进行必要的处理，例如转为小写、分离标-点符号等。**思考：** `normalizeString` 函数中的正则表达式是为英文设计的，直接用于处理中文是否会产生问题？应如何调整以正确处理中文句子？

   * **词典构建**：遍历所有处理后的句子，为中文和英文分别构建词典（即单词到索引的映射）。

3. **模型构建**

   * **编码器 (Encoder)**：构建一个基于 GRU 的编码器。其主要任务是读取输入序列（中文分词序列），并将其信息编码到一个或多个上下文向量中。

   * **注意力解码器 (Attention Decoder)**：构建一个同样基于 GRU 并结合了注意力机制的解码器。**重点理解：** 在生成每一个目标词时，注意力机制是如何计算权重，并利用这些权重从编码器的输出中提取最相关信息的。

4. **模型训练**

   * **定义训练流程**：设置损失函数和优化器。

   > **注意！！在训练过程中请大家将中间结果（例如loss值、翻译评估指标等）保存为文件，例如使用csv包将loss数组保存为csv文件，方便后续重绘loss值曲线。同时保存模型权重以便测试翻译效果。否则重新训练时间成本较大。**

5. **评估与可视化**

   * **定量评估**：计算模型翻译结果的 BLEU 分数，并与其他同学的结果进行对比。

   ```plain&#x20;text
   # pip install nltk

   from nltk.translate.bleu_score import sentence_bleu
   bleu_score = sentence_bleu([reference1, reference2, reference3], hypothesis1)
   ```

   * **定性分析与可视化**：选择几个测试句子，输出模型的翻译结果，并与原始标签进行对比。同时，调用 `showAttention` 函数，将注意力权重矩阵可视化成热力图，直观分析模型的翻译过程。

6. **撰写实验报告：**&#x6574;理实验结果并完成实验报告

**实验参考步骤**

1. **数据加载**：分析 `readLangs` 函数，理解如何读取 `eng-cmn.txt` 并生成句子对。

2. **文本规范化**：重点修改 `normalizeString` 函数中的正则表达式，确保其能正确处理中文句子（提示：中文句子应按字进行分割，而不是移除）。

3. **词典创建**：通过 `Lang` 类为输入（中）和输出（英）语言构建词典。

4. **模型定义**：分别阅读并理解 `EncoderRNN` 和 `AttnDecoderRNN` 两个类的代码实现。

5. **训练**：运行 `trainIters` 函数启动模型训练。可适当调小 `n_iters` 以缩短训练时间。

6. **评估**：运行 `evaluateRandomly` 函数，查看随机翻译样本的效果并获取平均 BLEU 分数。

7. **可视化**：使用 `showPlot` 函数绘制损失曲线，使用 `evaluateAndShowAttention` 函数展示具体翻译案例的注意力分布。

### 思考与扩展

\[可选 1] 自行调整参数，如调整句子最大长度 MAX\_LENGTH，总训练次数 n\_iters，特征维度 hidden\_size，分析相关内容

\[可选 2] 可**自行划分训练集/测试集**,推荐的划分比例是 7:3，根据定性及定量的实验结果进一步分析模型性能

\[可选 3] 可自行探索使用 Transformer 完成任务，示例代码可参考[ The Annotated Transformer blog](https://nlp.seas.harvard.edu/2018/04/03/attention.html)  (注意: 同样需要自行处理中英文翻译数据集)

**\[可选 4] 调试与分析 `IndexError`**：参考 PyTorch 官方教程 (`seq2seq_translation_tutorial.ipynb`)，尝试在不严格过滤长句子的数据集上训练模型。分析为何会出现 `IndexError`，并提出解决方案。

***

\[扩展 1] 可以自己搜索高考试卷或者四六级试卷的资料对数据集进行扩充&#x20;

\[扩展 2] 可以自己设计个软件 (简单的手机 APP 或者网页)，例如页面有一个输入框和一个输出框，输入中文或英文，输出对应的翻译的文本

\[扩展 3] 也可以展示一些失败的例子，比如翻译的文本质很差等

> **注意:**
>
> * 模型训练算力消耗较大，同学们可调小batch size，并且没必要按照示例程序训练全部epoch。训练部分的目的在于理解程序运行过程而非得到最终模型。
>
> * 本次实验将使用JetAutoML平台，使用过程中需要将结果(loss曲线等)上传到平台上，平台可直接可视化结果。因此需要在结果生成处添加少量代码以完成结果上传。具体使用方法待定。

