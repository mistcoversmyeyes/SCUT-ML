#!/usr/bin/env python3
"""
Lab3: PCAé™ç»´ä¸å¯è§†åŒ–å®éªŒ
MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†é™ç»´å®è·µ

å®éªŒå†…å®¹ï¼š
1. åŠ è½½MNISTæ•°æ®é›†ï¼Œé€‰å–æ¯ç±»100ä¸ªæ ·æœ¬
2. æ•°æ®é¢„å¤„ç†ï¼ˆå±•å¹³ã€æ ‡å‡†åŒ–ï¼‰
3. PCAé™ç»´ä¸æ–¹å·®åˆ†æ
4. é™ç»´å¯è§†åŒ–ä¸åˆ†ç±»æ€§èƒ½å¯¹æ¯”
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.datasets import fetch_openml
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_mnist_subset(samples_per_class=100):
    """
    åŠ è½½MNISTæ•°æ®é›†å¹¶é€‰å–æ¯ç±»æŒ‡å®šæ•°é‡çš„æ ·æœ¬

    Args:
        samples_per_class: æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡

    Returns:
        X: ç‰¹å¾çŸ©é˜µ (n_samples, 784)
        y: æ ‡ç­¾å‘é‡ (n_samples,)
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...")

    # ä»OpenMLåŠ è½½MNISTæ•°æ®é›†
    mnist = fetch_openml('mnist_784', version=1, as_frame=False)
    X, y = mnist.data, mnist.target

    # è½¬æ¢æ ‡ç­¾ä¸ºæ•´æ•°
    y = y.astype(int)

    print(f"ğŸ“Š åŸå§‹æ•°æ®é›†å¤§å°: {X.shape[0]} ä¸ªæ ·æœ¬, {X.shape[1]} ä¸ªç‰¹å¾")

    # é€‰å–æ¯ç±»æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    X_subset = []
    y_subset = []

    for digit in range(10):
        # æ‰¾åˆ°å½“å‰æ•°å­—çš„æ‰€æœ‰ç´¢å¼•
        indices = np.where(y == digit)[0]
        # éšæœºé€‰å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        selected_indices = np.random.choice(indices, samples_per_class, replace=False)
        X_subset.append(X[selected_indices])
        y_subset.append(y[selected_indices])

    # åˆå¹¶æ‰€æœ‰ç±»åˆ«çš„æ ·æœ¬
    X_subset = np.vstack(X_subset)
    y_subset = np.hstack(y_subset)

    print(f"âœ… é€‰å–åçš„æ•°æ®é›†å¤§å°: {X_subset.shape[0]} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ˆ æ¯ä¸ªç±»åˆ«æ ·æœ¬æ•°: {samples_per_class}")

    return X_subset, y_subset

def preprocess_data(X, y):
    """
    æ•°æ®é¢„å¤„ç†ï¼šå±•å¹³å’Œæ ‡å‡†åŒ–

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        y: æ ‡ç­¾å‘é‡

    Returns:
        X_scaled: æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
        y: æ ‡ç­¾å‘é‡
    """
    print("\nğŸ”„ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")

    # æ•°æ®å·²ç»æ˜¯å±•å¹³çš„784ç»´å‘é‡ï¼Œåªéœ€è¦æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X_scaled.shape[1]}")
    print(f"ğŸ“ˆ æ•°æ®å‡å€¼: {np.mean(X_scaled):.6f} (åº”æ¥è¿‘0)")
    print(f"ğŸ“ˆ æ•°æ®æ ‡å‡†å·®: {np.std(X_scaled):.6f} (åº”æ¥è¿‘1)")

    return X_scaled, y

def perform_pca_analysis(X, n_components=200):
    """
    æ‰§è¡ŒPCAé™ç»´åˆ†æ

    Args:
        X: æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
        n_components: è¦ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡

    Returns:
        pca: PCAæ¨¡å‹å¯¹è±¡
        X_pca: é™ç»´åçš„æ•°æ®
        explained_variance_ratio: æ–¹å·®è´¡çŒ®ç‡
        cumulative_variance_ratio: ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡
    """
    print(f"\nğŸ”„ æ­£åœ¨è¿›è¡ŒPCAé™ç»´åˆ†æï¼ˆä¿ç•™{n_components}ä¸ªä¸»æˆåˆ†ï¼‰...")

    # åˆ›å»ºPCAæ¨¡å‹ï¼Œä¿ç•™æ›´å¤šä¸»æˆåˆ†ä»¥ä¾¿æ‰¾åˆ°95%æ–¹å·®é˜ˆå€¼
    pca = PCA(n_components=n_components)

    # æ‹Ÿåˆæ¨¡å‹å¹¶è½¬æ¢æ•°æ®
    X_pca = pca.fit_transform(X)

    # è·å–æ–¹å·®è´¡çŒ®ç‡
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

    # æ‰¾åˆ°è¾¾åˆ°95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡
    n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1

    print(f"âœ… PCAé™ç»´å®Œæˆ")
    print(f"ğŸ“Š åŸå§‹ç»´åº¦: {X.shape[1]}")
    print(f"ğŸ“Š é™ç»´åç»´åº¦: {X_pca.shape[1]}")
    print(f"ğŸ“ˆ å‰{n_components}ä¸ªä¸»æˆåˆ†æ€»æ–¹å·®ä¿ç•™ç‡: {cumulative_variance_ratio[-1]:.4f} ({cumulative_variance_ratio[-1]*100:.2f}%)")
    print(f"ğŸ“ˆ è¾¾åˆ°95%æ–¹å·®éœ€è¦çš„ä¸»æˆåˆ†æ•°: {n_components_95}")

    return pca, X_pca, explained_variance_ratio, cumulative_variance_ratio, n_components_95

def plot_variance_analysis(cumulative_variance_ratio, save_path=None):
    """
    ç»˜åˆ¶ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡æ›²çº¿

    Args:
        cumulative_variance_ratio: ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡
        save_path: ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(10, 6))

    # ç»˜åˆ¶ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡æ›²çº¿
    plt.plot(range(1, len(cumulative_variance_ratio) + 1),
             cumulative_variance_ratio * 100,
             'b-', linewidth=2, marker='o', markersize=4)

    # æ·»åŠ 95%æ–¹å·®çº¿
    plt.axhline(y=95, color='r', linestyle='--', alpha=0.7, label='95% æ–¹å·®é˜ˆå€¼')

    # æ‰¾åˆ°è¾¾åˆ°95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡
    n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
    plt.axvline(x=n_components_95, color='r', linestyle='--', alpha=0.7)

    plt.xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=12)
    plt.ylabel('ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡ (%)', fontsize=12)
    plt.title('PCAç´¯è®¡æ–¹å·®è´¡çŒ®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # æ·»åŠ æ³¨é‡Š
    plt.annotate(f'å‰{n_components_95}ä¸ªä¸»æˆåˆ†\nè¾¾åˆ°95%æ–¹å·®',
                 xy=(n_components_95, 95),
                 xytext=(n_components_95 + 2, 85),
                 arrowprops=dict(arrowstyle='->', color='red'),
                 fontsize=10,
                 bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ–¹å·®åˆ†æå›¾å·²ä¿å­˜: {save_path}")

    plt.show()

    return n_components_95

def plot_2d_pca_visualization(X_pca, y, save_path=None):
    """
    ç»˜åˆ¶2D PCAé™ç»´å¯è§†åŒ–æ•£ç‚¹å›¾

    Args:
        X_pca: é™ç»´åçš„æ•°æ®ï¼ˆè‡³å°‘2ç»´ï¼‰
        y: æ ‡ç­¾å‘é‡
        save_path: ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(12, 8))

    # å®šä¹‰é¢œè‰²
    colors = plt.cm.tab10(np.linspace(0, 1, 10))

    # ä¸ºæ¯ä¸ªæ•°å­—ç±»åˆ«ç»˜åˆ¶æ•£ç‚¹å›¾
    for digit in range(10):
        mask = y == digit
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                   c=[colors[digit]], label=f'æ•°å­— {digit}',
                   alpha=0.7, s=50, edgecolors='black', linewidth=0.5)

    plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    plt.title('MNISTæ•°æ®é›†PCAé™ç»´2Då¯è§†åŒ–', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š 2Då¯è§†åŒ–å›¾å·²ä¿å­˜: {save_path}")

    plt.show()

def train_svm_classifier(X_train, X_test, y_train, y_test, kernel='rbf'):
    """
    è®­ç»ƒSVMåˆ†ç±»å™¨

    Args:
        X_train, X_test: è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
        y_train, y_test: è®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾
        kernel: SVMæ ¸å‡½æ•°ç±»å‹

    Returns:
        accuracy: æµ‹è¯•é›†å‡†ç¡®ç‡
        training_time: è®­ç»ƒæ—¶é—´
    """
    print(f"ğŸ”„ æ­£åœ¨è®­ç»ƒSVMåˆ†ç±»å™¨ (kernel={kernel})...")

    # åˆ›å»ºSVMåˆ†ç±»å™¨
    svm = SVC(kernel=kernel, random_state=42)

    # è®°å½•è®­ç»ƒæ—¶é—´
    start_time = time.time()
    svm.fit(X_train, y_train)
    training_time = time.time() - start_time

    # é¢„æµ‹å¹¶è®¡ç®—å‡†ç¡®ç‡
    y_pred = svm.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"âœ… SVMè®­ç»ƒå®Œæˆ")
    print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.4f} ç§’")

    return accuracy, training_time

def compare_classification_performance(X_original, X_pca, y, n_components_95):
    """
    å¯¹æ¯”åŸå§‹æ•°æ®å’Œé™ç»´æ•°æ®çš„åˆ†ç±»æ€§èƒ½

    Args:
        X_original: åŸå§‹ç‰¹å¾çŸ©é˜µ
        X_pca: PCAé™ç»´åçš„ç‰¹å¾çŸ©é˜µ
        y: æ ‡ç­¾å‘é‡
        n_components_95: è¾¾åˆ°95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡
    """
    print("\n" + "="*60)
    print("ğŸ” åˆ†ç±»æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("="*60)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_orig_train, X_orig_test, y_train, y_test = train_test_split(
        X_original, y, test_size=0.3, random_state=42, stratify=y)

    X_pca_train, X_pca_test, _, _ = train_test_split(
        X_pca, y, test_size=0.3, random_state=42, stratify=y)

    # ä½¿ç”¨ä¿ç•™95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡
    X_pca_95_train = X_pca_train[:, :n_components_95]
    X_pca_95_test = X_pca_test[:, :n_components_95]

    print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(y_train)} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(y_test)} æ ·æœ¬")
    print(f"ğŸ“Š åŸå§‹ç‰¹å¾ç»´åº¦: {X_orig_train.shape[1]}")
    print(f"ğŸ“Š é™ç»´ç‰¹å¾ç»´åº¦: {X_pca_95_train.shape[1]}")

    # åœ¨åŸå§‹æ•°æ®ä¸Šè®­ç»ƒSVM
    print(f"\nğŸ”¹ åŸå§‹æ•°æ® (784ç»´) SVMè®­ç»ƒ:")
    orig_accuracy, orig_time = train_svm_classifier(
        X_orig_train, X_orig_test, y_train, y_test)

    # åœ¨é™ç»´æ•°æ®ä¸Šè®­ç»ƒSVM
    print(f"\nğŸ”¹ é™ç»´æ•°æ® ({n_components_95}ç»´) SVMè®­ç»ƒ:")
    pca_accuracy, pca_time = train_svm_classifier(
        X_pca_95_train, X_pca_95_test, y_train, y_test)

    # æ€§èƒ½å¯¹æ¯”æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*60)
    print(f"åŸå§‹æ•°æ® (784ç»´):")
    print(f"  å‡†ç¡®ç‡: {orig_accuracy:.4f} ({orig_accuracy*100:.2f}%)")
    print(f"  è®­ç»ƒæ—¶é—´: {orig_time:.4f} ç§’")

    print(f"\né™ç»´æ•°æ® ({n_components_95}ç»´):")
    print(f"  å‡†ç¡®ç‡: {pca_accuracy:.4f} ({pca_accuracy*100:.2f}%)")
    print(f"  è®­ç»ƒæ—¶é—´: {pca_time:.4f} ç§’")

    print(f"\nğŸ” æ€§èƒ½å˜åŒ–:")
    print(f"  å‡†ç¡®ç‡å˜åŒ–: {pca_accuracy - orig_accuracy:+.4f} ({(pca_accuracy - orig_accuracy)*100:+.2f}%)")
    print(f"  è®­ç»ƒæ—¶é—´å˜åŒ–: {pca_time - orig_time:+.4f} ç§’")
    print(f"  è®­ç»ƒé€Ÿåº¦æå‡: {orig_time/pca_time:.2f}x")
    print(f"  ç»´åº¦å‹ç¼©ç‡: {(1 - n_components_95/784)*100:.1f}%")

    return {
        'original_accuracy': orig_accuracy,
        'original_time': orig_time,
        'pca_accuracy': pca_accuracy,
        'pca_time': pca_time,
        'n_components_95': n_components_95
    }

def main():
    """ä¸»å®éªŒæµç¨‹"""
    print("ğŸš€ å¼€å§‹Lab3: PCAé™ç»´ä¸å¯è§†åŒ–å®éªŒ")
    print("="*60)

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    np.random.seed(42)

    # 1. åŠ è½½MNISTæ•°æ®é›†
    X, y = load_mnist_subset(samples_per_class=100)

    # 2. æ•°æ®é¢„å¤„ç†
    X_scaled, y = preprocess_data(X, y)

    # 3. PCAé™ç»´åˆ†æï¼ˆä¿ç•™200ä¸ªä¸»æˆåˆ†ä»¥æ‰¾åˆ°95%æ–¹å·®é˜ˆå€¼ï¼‰
    pca, X_pca, explained_variance_ratio, cumulative_variance_ratio, n_components_95 = perform_pca_analysis(X_scaled, n_components=200)

    # 4. ç»˜åˆ¶æ–¹å·®åˆ†æå›¾
    plot_variance_analysis(cumulative_variance_ratio, 'lab3/variance_analysis.png')

    # 5. ç»˜åˆ¶2Då¯è§†åŒ–å›¾
    plot_2d_pca_visualization(X_pca, y, 'lab3/pca_2d_visualization.png')

    # 6. åˆ†ç±»æ€§èƒ½å¯¹æ¯”
    performance_results = compare_classification_performance(X_scaled, X_pca, y, n_components_95)

    print(f"\nğŸ‰ Lab3å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° lab3/ ç›®å½•")

if __name__ == "__main__":
    main()