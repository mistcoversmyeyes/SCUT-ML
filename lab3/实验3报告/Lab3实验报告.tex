\documentclass[journal, a4paper]{IEEEtran}

% some very useful LaTeX packages include:
\usepackage{graphicx}   % Required for graphics, photos, etc.
\usepackage{url}        % Better support for handling and breaking URLs
\usepackage{amsmath}    % Popular package that provides many helpful commands for dealing with mathematics
\usepackage{amssymb}    % Provides \mathbb and additional math symbols
\usepackage{booktabs}   % For professional quality tables
\usepackage{algorithm}  % For algorithms
\usepackage{algorithmic} % For algorithmic environment

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***

% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

~\\[1cm]
\includegraphics{SCUT.png}\\[2cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[1cm]
{ \huge \bfseries The Experiment Report of \textit{Machine Learning} }\\[0.6cm] % Title of your document
\HRule \\[2cm]

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE \textbf{School:} School of Software Engineering}\\[1cm]
\textsc{\LARGE \textbf{Subject:} Software Engineering}\\[2cm]

%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Yuming Jiang% Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Mingkui Tan % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student ID:}\\
202330550601
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Grade:} \\
Undergraduate
\end{flushright}
\end{minipage}\\[2cm]

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

% Define document title and author
\title{PCA Dimensionality Reduction and Visualization -- High-Dimensional Data Dimensionality Reduction Practice}
\maketitle

% Write abstract here
\begin{abstract}
This experiment focuses on Principal Component Analysis (PCA) for dimensionality reduction and visualization of high-dimensional data. Using the MNIST handwritten digit dataset, we implement PCA to reduce the dimensionality from 784 to lower dimensions while preserving most of the variance. We analyze the variance contribution rate of principal components, visualize the 2D projection of the data, and compare the classification performance between original and reduced dimensions using Support Vector Machine (SVM) classifiers. The results demonstrate that PCA can effectively reduce dimensionality while maintaining classification accuracy, significantly improving training efficiency.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
\IEEEPARstart{H}{igh-dimensional} data presents significant challenges in machine learning, including increased computational complexity, storage requirements, and the curse of dimensionality. Principal Component Analysis (PCA) is a widely used unsupervised dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving most of the variance in the data.

The MNIST handwritten digit dataset serves as an excellent testbed for demonstrating PCA's effectiveness. Each digit image contains 784 pixels (28×28), making it a high-dimensional dataset suitable for dimensionality reduction analysis. This experiment aims to:

\begin{itemize}
\item Understand the mathematical principles behind PCA
\item Implement PCA on the MNIST dataset to achieve dimensionality reduction
\item Analyze the variance contribution rate of principal components
\item Visualize the data in 2D space using PCA
\item Compare classification performance between original and reduced dimensions
\item Evaluate the trade-offs between dimensionality reduction and model performance
\end{itemize}

\section{Methods and Theory}

\subsection{Principal Component Analysis (PCA)}
PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component has the highest variance possible under the constraint that it is orthogonal to the preceding components.

Mathematically, given a data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n$ is the number of samples and $d$ is the number of features, PCA finds the eigenvectors and eigenvalues of the covariance matrix:

\begin{equation}
\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}
\end{equation}

The eigenvalue decomposition of $\mathbf{C}$ is:

\begin{equation}
\mathbf{C} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T
\end{equation}

where $\mathbf{V}$ contains the eigenvectors (principal components) and $\mathbf{\Lambda}$ is a diagonal matrix containing the eigenvalues.

\subsection{Variance Contribution Rate}
The variance contribution rate of the $i$-th principal component is given by:

\begin{equation}
\text{Variance Ratio}_i = \frac{\lambda_i}{\sum_{j=1}^{d} \lambda_j}
\end{equation}

where $\lambda_i$ is the $i$-th eigenvalue. The cumulative variance contribution rate for the first $k$ principal components is:

\begin{equation}
\text{Cumulative Variance Ratio}_k = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{j=1}^{d} \lambda_j}
\end{equation}

\subsection{Support Vector Machine (SVM)}
SVM is a supervised learning algorithm used for classification and regression tasks. For classification, SVM finds the optimal hyperplane that separates data points of different classes with the maximum margin. The kernel trick allows SVM to handle non-linearly separable data by mapping the input space into a higher-dimensional feature space.

\section{Experiments}

\subsection{Dataset}
The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), each of size 28×28 pixels. For this experiment, we selected a balanced subset with 100 samples per class, totaling 1,000 samples. The data was split into training (70\%) and testing (30\%) sets with stratified sampling to maintain class distribution.

\begin{table}[!hbt]
\begin{center}
\caption{Dataset Statistics}
\label{tab:dataset_stats}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Train} & \textbf{Test} & \textbf{Total} \\
\hline
0 & 70 & 30 & 100 \\
1 & 70 & 30 & 100 \\
2 & 70 & 30 & 100 \\
3 & 70 & 30 & 100 \\
4 & 70 & 30 & 100 \\
5 & 70 & 30 & 100 \\
6 & 70 & 30 & 100 \\
7 & 70 & 30 & 100 \\
8 & 70 & 30 & 100 \\
9 & 70 & 30 & 100 \\
\hline
\textbf{Total} & \textbf{700} & \textbf{300} & \textbf{1000} \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Implementation}
\subsubsection{Data Preprocessing}
\begin{enumerate}
\item \textbf{Data Loading}: Loaded MNIST dataset from OpenML repository
\item \textbf{Sampling}: Randomly selected 100 samples per class (total 1,000 samples)
\item \textbf{Standardization}: Applied StandardScaler to normalize features (mean=0, std=1)
\end{enumerate}

\subsubsection{PCA Dimensionality Reduction}
\begin{enumerate}
\item \textbf{Covariance Matrix}: Computed covariance matrix of standardized data
\item \textbf{Eigenvalue Decomposition}: Calculated eigenvalues and eigenvectors
\item \textbf{Component Selection}: Analyzed cumulative variance to determine optimal number of components
\item \textbf{Transformation}: Projected data onto selected principal components
\end{enumerate}

\subsubsection{Classification Performance Comparison}
\begin{enumerate}
\item \textbf{SVM Training}: Trained RBF-kernel SVM on both original and reduced data
\item \textbf{Performance Metrics}: Measured accuracy and training time
\item \textbf{Comparative Analysis}: Evaluated trade-offs between dimensionality and performance
\end{enumerate}

\subsection{Experimental Results}

\subsubsection{Variance Analysis}
The PCA analysis revealed that the first 194 principal components capture 95\% of the total variance in the data, representing a 75.3\% reduction in dimensionality from 784 to 194 dimensions.

\begin{table}[!hbt]
\begin{center}
\caption{PCA Variance Analysis Results}
\label{tab:pca_variance}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Components} & \textbf{Variance} & \textbf{Reduction} \\
\hline
50 & 82.15\% & 93.6\% \\
100 & 89.34\% & 87.2\% \\
150 & 92.67\% & 80.9\% \\
\textbf{194} & \textbf{95.00\%} & \textbf{75.3\%} \\
200 & 95.35\% & 74.5\% \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[!hbt]
\begin{center}
\includegraphics[width=\columnwidth]{../variance_analysis}
\caption{Cumulative variance contribution rate of principal components. The red dashed line indicates the 95\% variance threshold, which requires 194 components.}
\label{fig:variance_analysis}
\end{center}
\end{figure}

\subsubsection{2D Visualization}
The 2D PCA projection reveals distinct clusters for different digits, particularly for digits 0, 1, and 6, which are well-separated. Some overlapping is observed between similar digits (e.g., 4 and 9, 3 and 8), indicating the challenges in classification.

\begin{figure}[!hbt]
\begin{center}
\includegraphics[width=\columnwidth]{../pca_2d_visualization}
\caption{2D PCA visualization of MNIST digits. Each color represents a different digit class (0-9). The first two principal components capture approximately 17.2\% of the total variance.}
\label{fig:pca_2d}
\end{center}
\end{figure}

\subsubsection{Classification Performance Comparison}
The classification results demonstrate that PCA can maintain classification accuracy while significantly improving computational efficiency.

\begin{table}[!hbt]
\begin{center}
\caption{SVM Classification Performance Comparison}
\label{tab:classification_performance}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Data} & \textbf{Dim} & \textbf{Acc} & \textbf{Time (s)} & \textbf{Speed} \\
\hline
Original & 784 & 85.67\% & 0.1350 & 1.00× \\
PCA & 194 & 85.67\% & 0.0206 & 6.55× \\
\hline
\end{tabular}
\end{center}
\end{table}

The key findings include:
\begin{itemize}
\item \textbf{Accuracy Preservation}: No loss in classification accuracy (85.67\% for both cases)
\item \textbf{Computational Efficiency}: 6.55× faster training with reduced dimensions
\item \textbf{Dimensionality Reduction}: 75.3\% reduction in feature space
\item \textbf{Memory Efficiency}: Significant reduction in storage requirements
\end{itemize}

\section{Discussion}

\subsection{Effectiveness of PCA}
The experiment demonstrates PCA's effectiveness in dimensionality reduction for high-dimensional image data. The ability to maintain 100\% classification accuracy while reducing dimensions by 75.3\% highlights PCA's utility in practical machine learning applications.

\subsection{Variance Threshold Selection}
The choice of 95\% variance threshold is a common practice that balances information preservation and dimensionality reduction. Our results show that 194 components are sufficient to capture most discriminative information for digit classification.

\subsection{Computational Benefits}
The 6.55× improvement in training time demonstrates the practical benefits of dimensionality reduction, especially important for real-time applications or resource-constrained environments.

\subsection{Visualization Insights}
The 2D PCA visualization, while capturing only 17.2\% of total variance, still reveals meaningful clustering patterns. Some digit classes (0, 1, 6) are clearly separable, while others (3, 5, 8) show overlap, explaining classification challenges.

\section{Conclusion}

This experiment successfully demonstrated the application of PCA for dimensionality reduction on the MNIST handwritten digit dataset. The key achievements include:

\begin{itemize}
\item \textbf{Dimensionality Reduction}: Achieved 75.3\% reduction in dimensions while preserving 95\% of data variance
\item \textbf{Performance Preservation}: Maintained 100\% classification accuracy with significant computational efficiency gains
\item \textbf{Visualization}: Successfully visualized high-dimensional data in 2D space, revealing meaningful patterns
\item \textbf{Practical Application}: Demonstrated PCA's utility for real-world machine learning applications
\end{itemize}

The experiment highlights the importance of dimensionality reduction techniques in modern machine learning, particularly for handling high-dimensional data efficiently. PCA's ability to preserve discriminative information while reducing computational complexity makes it an essential tool in the machine learning practitioner's toolkit.

Future work could explore:
\begin{itemize}
\item Comparison with other dimensionality reduction techniques (t-SNE, LDA)
\item Impact of different variance thresholds on classification performance
\item Application to larger datasets and more complex classification tasks
\item Integration with deep learning architectures
\end{itemize}

% Your document ends here!
\end{document}