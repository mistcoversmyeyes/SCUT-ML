




# 📚 机器学习与深度学习：全序复习清单 (Lec 1-14)

### Lec 1: 机器学习基础 (Basics of Machine Learning)
- [x] **三大要素**：数据 (Data)、模型 (Model)、损失函数 (Loss Function)。
- [ ] **参数估计 (隐藏高频考点)**：
    - **MLE (极大似然)**：$\theta_{MLE} = \arg\max P(Data|\theta)$。假设参数固定。
    - **MAP (最大后验)**：$\theta_{MAP} = \arg\max P(Data|\theta)P(\theta)$。引入了先验概率。
    - *关系*：当先验 $P(\theta)$ 为均匀分布 (Uniform) 时，MAP 等价于 MLE。
- [ ] **信息论**：熵 (Entropy) $H(X) = -\sum p(x)\log p(x)$ 与信息增益。

### Lec 2: 线性回归与梯度下降 (Linear Regression)
- 记号约定（默认列向量）：$x,w$ 为列向量；若使用设计矩阵 $X\in\mathbb{R}^{N\times d}$（行是 $x_i^T$），则 $y\in\mathbb{R}^{N}$ 为列向量。
- [x] **模型与损失**：
    - 模型：$f(x) = w^T x + b$。
    - 损失：均方误差 $\mathcal{L}(w) = \frac{1}{2}||y - Xw||^2$。
- [x] **解析解 (Closed-form)**：
    - 公式：$w^* = (X^T X)^{-1} X^T y$。
    - *条件*：$X^T X$ 必须可逆 (Invertible)。
- [x] **梯度下降 (Gradient Descent)**：
    - 迭代公式：$w_{k+1} = w_k - \eta \nabla \mathcal{L}(w)$。
    - *手算准备*：给你简单数据，能算一步迭代。

### Lec 3: 线性分类与 SVM (Linear Classification & SVM)
- [x] **SVM 核心**：最大化间隔 (Max Margin)。$$ y_i (w^T x_i + b) \geq 1, \quad \forall i $$
- [x] **Hinge Loss (必背公式)**：$\mathcal{L} = \sum \max(0, 1 - y_i(w^T x_i + b)) + \frac{\lambda}{2} ||w||^2$。
- [ ] **梯度下降变体 (必考)**：
    - **SGD**：每次用 1 个样本（快但震荡）。
    - **Mini-batch**：每次用一小批（平衡）。
    - **Batch**：每次用全部样本（稳但慢）。
- [x] **[❌不考]**：SVM 对偶问题 (Dual Problem)、KKT 条件、核技巧 (Kernel Trick)。

### Lec 4: 逻辑回归与 Softmax (Logistic Regression)
- **逻辑回归 (二分类)**：
    * [ ] **Sigmoid 函数**：$g(z) = \frac{1}{1+e^{-z}}$，性质：$g'(z) = g(z)(1-g(z))$。
    * [ ] **交叉熵损失 (Cross-Entropy/Log Loss)**：
        * 公式：$L = -\frac{1}{n}\sum [y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)]$。
        * 物理意义：源自最大似然估计 (MLE)。
- **Softmax 回归 (多分类)**：
    * [ ] **Softmax 函数**：$P(y=j|x) = \frac{e^{w_j^T x}}{\sum_{k} e^{w_k^T x}}$。
    * [ ] **多分类损失**：$L = -\sum_{j} \mathbb{I}(y=j) \log(p_j)$。

### Lec 5: 过拟合与欠拟合 (Underfitting & Overfitting)
- [ ] **核心概念**：
    - 欠拟合 = High Bias (训练差，测试差)。
    - 过拟合 = High Variance (训练好，测试差)。
- [ ] **解决方案**：
    - 解决过拟合：增加数据、正则化 (Regularization)、早停 (Early Stopping)。
- [ ] **交叉验证**：K-Fold Cross-Validation 的流程。

### Lec 6: 集成学习 (Ensemble Methods)
- [ ] **Bagging**：并行训练，投票机制。代表：**随机森林 (Random Forest)**。
    - 作用：降低方差 (Variance)。
- [ ] **Boosting**：串行训练，加权错误样本。代表：**AdaBoost**。
    - 作用：降低偏差 (Bias)。
- [ ] **[❌不考]**：GBDT (Gradient Boosting Decision Tree)。

### Lec 7: 聚类 (Clustering)
- [ ] **K-Means**：
    - 步骤：初始化 $\to$ 分配簇 $\to$ 更新中心 $\to$ 重复。
    - 缺点：需预设 K，对初始值敏感。
- [ ] **[了解]**：层次聚类 (HAC)。

### Lec 8: 主成分分析 (PCA)
- [ ] **核心思想**：投影到方差最大 (Max Variance) 的方向。
- [ ] **步骤**：去均值 $\to$ 计算协方差矩阵 $S$ $\to$ 特征值分解 $\to$ 取前 k 个特征向量。
- [ ] **Autoencoder 联系**：PCA 是一种线性的自动编码器。

### Lec 9: 推荐系统 (Recommender System)
- [ ] **[❌不考]**：协同过滤、矩阵分解等本章内容均不在考试范围。

### Lec 10: 图像处理 (Image Processing)
- [ ] **卷积原理**：理解卷积核 (Kernel) 如何在图像上滑动并计算加权和。
- [ ] **[❌不考]**：传统算子细节 (Sobel, Canny, Prewitt 等具体计算不考，重点在 CNN 中的卷积)。

### Lec 11: 神经网络与深度学习 (NN & DL)
- [ ] **反向传播 (BP)**：基于链式法则 (Chain Rule)。
- [ ] **激活函数**：ReLU (解决梯度消失，计算快), Sigmoid。
- [ ] **CNN 结构**：
    - 卷积层 (提取特征)、池化层 (降维/平移不变性)。
- [ ] **[必考计算] CNN 输出尺寸**：
    - $H_{out} = \lfloor \frac{H_{in} - F + 2P}{S} \rfloor + 1$。
    - 参数量计算：$(F \times F \times C_{in} + 1) \times C_{out}$。

### Lec 12: 序列模型 (Sequence Modeling)
- [ ] **RNN 问题**：梯度消失 (Vanishing Gradient) $\to$ 无法捕捉长距离依赖。
- [ ] **LSTM**：通过 **门 (Gates)** (遗忘门、输入门、输出门) 解决梯度消失。
- [ ] **Transformer (重点)**：
    - 核心：**Self-Attention** (自注意力机制)。
    - 公式：$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$。
    - 相比 RNN 优势：可以并行计算 (Parallelizable)。

### Lec 13: 强化学习 (Reinforcement Learning)
- [ ] **基本元组**：Agent, Environment, State, Action, Reward。
- [ ] **目标**：最大化长期累积回报 (Return)。
- [ ] **[❌不考]**：贝尔曼方程的具体推导、AlphaGo 的复杂算法细节。

### Lec 14: 正则化与优化回顾 (Regularization & Optimization)
- [ ] **[必考证明] 梯度下降原理**：
    - 使用 **泰勒展开 (Taylor Expansion)** 一阶近似，证明负梯度方向是函数值下降最快的方向。
- [ ] **正则化几何解释 (L1 vs L2)**：
    - **L1 (Lasso)**：解空间是菱形 $\to$ 易得角点解 $\to$ **稀疏解 (Sparsity)** (特征选择)。 
    - **L2 (Ridge)**：解空间是圆形 $\to$ 权重变小但非零。
- [ ] **学习率**：过大导致发散 (Diverge)，过小收敛慢。