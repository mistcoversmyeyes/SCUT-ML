**Lec 4: 逻辑回归与 Softmax (Logistic Regression 考点：**
- [ ] **二分类**：
    - Sigmoid 函数：$g(z) = \frac{1}{1+e^{-z}}$。
    - 损失：二元交叉熵 (Log Loss)。
- [ ] **多分类 (Softmax)**：
    - Softmax 公式：$P(y=j) = \frac{e^{z_j}}{\sum e^{z_k}}$。
    - 损失：交叉熵 $-\sum y \log \hat{y}$。


## 数据


## 模型

## 损失函数

## 梯度计算