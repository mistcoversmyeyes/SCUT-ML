# 实验7 RNN 中英文翻译（Seq2Seq + Attention）

- 做了啥：实现了一个基于 Seq2Seq 架构的中英文翻译模型。Encoder 和 Decoder 都用了 GRU，关键是加上了 Attention 机制。用 Tatoeba 数据集里的一小部分中英句对进行训练，最后看 BLEU 分数和 Attention 权重的可视化热力图。
- 遇到的问题：长句子翻译效果差，模型记不住前面的内容；训练初期 Loss 下降慢；有些词翻译出来不通顺或者重复。
- 解决方案：引入 Attention 机制，让 Decoder 在生成每个词的时候都能“回头看”源句子的相关部分，解决了长距离依赖问题；使用 GRU 替代普通 RNN，缓解梯度消失；可视化 Attention 矩阵，确认模型确实学到了词与词之间的对齐关系（比如“我”对应“I”）。
- 启发：Attention 是 NLP 的核心，它打破了定长向量的瓶颈；Seq2Seq 结构非常通用，不仅能做翻译，还能做对话生成；可视化是理解模型内部工作原理的最好手段，看到对齐的 heatmap 就能确信模型学到了东西。
