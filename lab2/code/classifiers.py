#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Lab2 åˆ†ç±»ç®—æ³•å®ç°æ¨¡å—
å®ç°é€»è¾‘å›å½’ã€çº¿æ€§SVMå’Œæ ¸SVMåˆ†ç±»å™¨
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve
)
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

class Lab2Classifier:
    """Lab2åˆ†ç±»ç®—æ³•å®ç°ç±»"""

    def __init__(self):
        self.models = {}
        self.best_params = {}
        self.training_history = {}

    def train_logistic_regression(self, X_train, y_train, param_grid=None, cv=5):
        """
        è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨

        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            param_grid: å‚æ•°ç½‘æ ¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæœ€ä½³å‚æ•°
        """
        print("ğŸ”§ è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨...")

        if param_grid is None:
            param_grid = {
                'C': [0.001, 0.01, 0.1, 1, 10, 100],
                'penalty': ['l2'],
                'solver': ['liblinear', 'lbfgs'],
                'max_iter': [1000]
            }

        # åŸºç¡€æ¨¡å‹
        base_model = LogisticRegression(random_state=42)

        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv,
            scoring='accuracy', n_jobs=-1, verbose=1
        )

        # è®­ç»ƒ
        grid_search.fit(X_train, y_train)

        # ä¿å­˜ç»“æœ
        self.models['logistic_regression'] = grid_search.best_estimator_
        self.best_params['logistic_regression'] = grid_search.best_params_
        self.training_history['logistic_regression'] = {
            'cv_scores': grid_search.cv_results_,
            'best_score': grid_search.best_score_
        }

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  âœ… äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")

        return grid_search.best_estimator_, grid_search.best_params_

    def train_linear_svm(self, X_train, y_train, param_grid=None, cv=5):
        """
        è®­ç»ƒçº¿æ€§SVMåˆ†ç±»å™¨

        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            param_grid: å‚æ•°ç½‘æ ¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæœ€ä½³å‚æ•°
        """
        print("ğŸ”§ è®­ç»ƒçº¿æ€§SVMåˆ†ç±»å™¨...")

        if param_grid is None:
            param_grid = {
                'C': [0.001, 0.01, 0.1, 1, 10, 100],
                'kernel': ['linear'],
                'probability': [True]
            }

        # åŸºç¡€æ¨¡å‹
        base_model = SVC(random_state=42)

        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv,
            scoring='accuracy', n_jobs=-1, verbose=1
        )

        # è®­ç»ƒ
        grid_search.fit(X_train, y_train)

        # ä¿å­˜ç»“æœ
        self.models['linear_svm'] = grid_search.best_estimator_
        self.best_params['linear_svm'] = grid_search.best_params_
        self.training_history['linear_svm'] = {
            'cv_scores': grid_search.cv_results_,
            'best_score': grid_search.best_score_
        }

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  âœ… äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")

        return grid_search.best_estimator_, grid_search.best_params_

    def train_rbf_svm(self, X_train, y_train, param_grid=None, cv=5):
        """
        è®­ç»ƒRBFæ ¸SVMåˆ†ç±»å™¨

        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            param_grid: å‚æ•°ç½‘æ ¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæœ€ä½³å‚æ•°
        """
        print("ğŸ”§ è®­ç»ƒRBFæ ¸SVMåˆ†ç±»å™¨...")

        if param_grid is None:
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
                'kernel': ['rbf'],
                'probability': [True]
            }

        # åŸºç¡€æ¨¡å‹
        base_model = SVC(random_state=42)

        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv,
            scoring='accuracy', n_jobs=-1, verbose=1
        )

        # è®­ç»ƒ
        grid_search.fit(X_train, y_train)

        # ä¿å­˜ç»“æœ
        self.models['rbf_svm'] = grid_search.best_estimator_
        self.best_params['rbf_svm'] = grid_search.best_params_
        self.training_history['rbf_svm'] = {
            'cv_scores': grid_search.cv_results_,
            'best_score': grid_search.best_score_
        }

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  âœ… äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")

        return grid_search.best_estimator_, grid_search.best_params_

    def evaluate_model(self, model, X_test, y_test, model_name, dataset_name):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            model_name: æ¨¡å‹åç§°
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"ğŸ“Š è¯„ä¼° {model_name} åœ¨ {dataset_name} ä¸Šçš„æ€§èƒ½...")

        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_prob = None

        # å¯¹äºäºŒåˆ†ç±»ï¼Œè·å–æ¦‚ç‡é¢„æµ‹
        if len(np.unique(y_test)) == 2:
            y_prob = model.predict_proba(X_test)[:, 1]

        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_test, y_pred, output_dict=True)

        print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.4f}")

        return {
            'accuracy': accuracy,
            'confusion_matrix': cm,
            'classification_report': report,
            'y_pred': y_pred,
            'y_prob': y_prob,
            'y_true': y_test
        }

    def plot_confusion_matrix(self, cm, class_names, model_name, dataset_name):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µ

        Args:
            cm: æ··æ·†çŸ©é˜µ
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            model_name: æ¨¡å‹åç§°
            dataset_name: æ•°æ®é›†åç§°
        """
        plt.figure(figsize=(8, 6))

        # ä½¿ç”¨çƒ­åŠ›å›¾æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)

        plt.title(f'{model_name} - {dataset_name} æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')

        # ä¿å­˜å›¾ç‰‡
        filename = f"{model_name}_{dataset_name}_confusion_matrix.png"
        filepath = f"lab2/outputs/{filename}"
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.show()

        print(f"  ğŸ“ˆ æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {filepath}")

    def plot_roc_curve(self, y_true, y_prob, model_name, dataset_name):
        """
        ç»˜åˆ¶ROCæ›²çº¿ï¼ˆä»…ç”¨äºäºŒåˆ†ç±»ï¼‰

        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_prob: é¢„æµ‹æ¦‚ç‡
            model_name: æ¨¡å‹åç§°
            dataset_name: æ•°æ®é›†åç§°
        """
        if y_prob is None:
            print("  âš ï¸  è·³è¿‡ROCæ›²çº¿ç»˜åˆ¶ï¼ˆå¤šåˆ†ç±»ä»»åŠ¡ï¼‰")
            return

        # è®¡ç®—ROCæ›²çº¿
        fpr, tpr, thresholds = roc_curve(y_true, y_prob)
        roc_auc = auc(fpr, tpr)

        # ç»˜åˆ¶ROCæ›²çº¿
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'{model_name} (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡é˜³æ€§ç‡')
        plt.ylabel('çœŸé˜³æ€§ç‡')
        plt.title(f'{model_name} - {dataset_name} ROCæ›²çº¿')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        filename = f"{model_name}_{dataset_name}_roc_curve.png"
        filepath = f"lab2/outputs/{filename}"
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.show()

        print(f"  ğŸ“ˆ ROCæ›²çº¿å·²ä¿å­˜åˆ°: {filepath}")
        print(f"  âœ… AUCå€¼: {roc_auc:.4f}")

    def compare_models(self, results_dict, dataset_name):
        """
        æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½

        Args:
            results_dict: åŒ…å«æ‰€æœ‰æ¨¡å‹ç»“æœçš„å­—å…¸
            dataset_name: æ•°æ®é›†åç§°
        """
        print(f"ğŸ“Š æ¯”è¾ƒå„æ¨¡å‹åœ¨ {dataset_name} ä¸Šçš„æ€§èƒ½...")

        # æå–å‡†ç¡®ç‡
        models = list(results_dict.keys())
        accuracies = [results_dict[model]['accuracy'] for model in models]

        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        plt.figure(figsize=(10, 6))
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom', fontsize=12)

        plt.title(f'å„æ¨¡å‹åœ¨ {dataset_name} ä¸Šçš„å‡†ç¡®ç‡å¯¹æ¯”')
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.ylim(0, 1)
        plt.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        filename = f"{dataset_name}_model_comparison.png"
        filepath = f"lab2/outputs/{filename}"
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.show()

        print(f"  ğŸ“ˆ æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {filepath}")

        # æ‰“å°è¯¦ç»†æ¯”è¾ƒç»“æœ
        print("\nğŸ“‹ è¯¦ç»†æ€§èƒ½å¯¹æ¯”:")
        print("-" * 60)
        print(f"{'æ¨¡å‹':<15} {'å‡†ç¡®ç‡':<10} {'æœ€ä½³å‚æ•°':<30}")
        print("-" * 60)

        for model in models:
            accuracy = results_dict[model]['accuracy']
            if model in self.best_params:
                params = str(self.best_params[model])
                if len(params) > 25:
                    params = params[:25] + "..."
            else:
                params = "N/A"
            print(f"{model:<15} {accuracy:<10.4f} {params:<30}")

    def run_full_experiment(self, X_train, X_test, y_train, y_test, dataset_name):
        """
        è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹

        Args:
            X_train, X_test: è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
            y_train, y_test: è®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        """
        print(f"\n{'='*20} {dataset_name} åˆ†ç±»å®éªŒ {'='*20}")

        results = {}

        # 1. è®­ç»ƒé€»è¾‘å›å½’
        print("\n1ï¸âƒ£ é€»è¾‘å›å½’")
        lr_model, lr_params = self.train_logistic_regression(X_train, y_train)
        results['logistic_regression'] = self.evaluate_model(
            lr_model, X_test, y_test, 'é€»è¾‘å›å½’', dataset_name
        )

        # 2. è®­ç»ƒçº¿æ€§SVM
        print("\n2ï¸âƒ£ çº¿æ€§SVM")
        svm_linear_model, svm_linear_params = self.train_linear_svm(X_train, y_train)
        results['linear_svm'] = self.evaluate_model(
            svm_linear_model, X_test, y_test, 'çº¿æ€§SVM', dataset_name
        )

        # 3. è®­ç»ƒRBFæ ¸SVM
        print("\n3ï¸âƒ£ RBFæ ¸SVM")
        svm_rbf_model, svm_rbf_params = self.train_rbf_svm(X_train, y_train)
        results['rbf_svm'] = self.evaluate_model(
            svm_rbf_model, X_test, y_test, 'RBFæ ¸SVM', dataset_name
        )

        # 4. ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")

        # ç¡®å®šç±»åˆ«åç§°
        unique_classes = np.unique(y_test)
        if len(unique_classes) == 2:
            class_names = ['è´Ÿç±»', 'æ­£ç±»']
        else:
            class_names = [f'ç±»åˆ« {i}' for i in unique_classes]

        # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿
        for model_name, result in results.items():
            self.plot_confusion_matrix(
                result['confusion_matrix'], class_names, model_name, dataset_name
            )
            self.plot_roc_curve(
                result['y_true'], result['y_prob'], model_name, dataset_name
            )

        # 5. æ¨¡å‹å¯¹æ¯”
        self.compare_models(results, dataset_name)

        return results

    def get_model_summary(self):
        """è·å–æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹è®­ç»ƒæ‘˜è¦")
        print("="*60)

        for model_name, history in self.training_history.items():
            print(f"\n{model_name}:")
            print(f"  æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®ç‡: {history['best_score']:.4f}")
            print(f"  æœ€ä½³å‚æ•°: {self.best_params[model_name]}")

if __name__ == "__main__":
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

    print("Lab2 åˆ†ç±»ç®—æ³•å®ç°æ¨¡å—")
    print("è¯·ä½¿ç”¨ä¸»å®éªŒè„šæœ¬è¿è¡Œå®Œæ•´å®éªŒ")