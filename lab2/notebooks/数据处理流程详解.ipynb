{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š æ•°æ®å¤„ç†æµç¨‹è¯¦è§£ - ä»é›¶åˆ°ç²¾é€š\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "é€šè¿‡è¿™ä¸ªnotebookï¼Œæ‚¨å°†å®Œå…¨æŒæ¡æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ•°æ®åŠ è½½** - å¦‚ä½•è¯»å–ä¸åŒæ ¼å¼çš„æ•°æ®\n",
    "2. **æ•°æ®æ¢ç´¢** - ç†è§£æ•°æ®çš„ç‰¹å¾å’Œé—®é¢˜\n",
    "3. **æ•°æ®æ¸…æ´—** - å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ç­‰\n",
    "4. **ç‰¹å¾å·¥ç¨‹** - åˆ›å»ºå’Œä¼˜åŒ–ç‰¹å¾\n",
    "5. **æ•°æ®é¢„å¤„ç†** - æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ç­‰\n",
    "6. **æ•°æ®åˆ†å‰²** - è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ’åˆ†\n",
    "\n",
    "## ğŸ’¡ å­¦ä¹ æ–¹æ³•\n",
    "- **ç†è§£æ¯ä¸€æ­¥çš„ç›®çš„** - ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªå¤„ç†ï¼Ÿ\n",
    "- **æŒæ¡å®ç°æ–¹æ³•** - ä»£ç æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ\n",
    "- **å­¦ä¼šè¯Šæ–­é—®é¢˜** - å¦‚ä½•åˆ¤æ–­æ•°æ®å¤„ç†æ˜¯å¦æ­£ç¡®ï¼Ÿ\n",
    "- **äº†è§£æœ€ä½³å®è·µ** - ä»€ä¹ˆæ—¶å€™ç”¨ä»€ä¹ˆæ–¹æ³•ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ æ­¥éª¤1: æ•°æ®åŠ è½½\n",
    "\n",
    "### ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "æ•°æ®åŠ è½½æ˜¯ç¬¬ä¸€æ­¥ï¼Œå¦‚æœè¿™ä¸€æ­¥å‡ºé”™ï¼Œåé¢æ‰€æœ‰çš„åˆ†æéƒ½ä¼šé”™è¯¯ã€‚\n",
    "\n",
    "### ğŸ“š å¸¸è§çš„æ•°æ®æ ¼å¼ï¼š\n",
    "- CSVæ–‡ä»¶ (.csv)\n",
    "- Excelæ–‡ä»¶ (.xlsx)\n",
    "- LIBSVMæ ¼å¼ (.scale)\n",
    "- JSONæ ¼å¼ (.json)\n",
    "- æ•°æ®åº“è¿æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š æ­¥éª¤1: æ•°æ®åŠ è½½\n",
      "==================================================\n",
      "\n",
      "ğŸ”¹ æ–¹æ³•1: åŠ è½½LIBSVMæ ¼å¼\n",
      "LIBSVMæ ¼å¼ç‰¹ç‚¹ï¼š\n",
      "- æ¯è¡Œ: <æ ‡ç­¾> <ç‰¹å¾ç´¢å¼•>:<ç‰¹å¾å€¼> <ç‰¹å¾ç´¢å¼•>:<ç‰¹å¾å€¼> ...\n",
      "- é€‚åˆç¨€ç–æ•°æ®ï¼ˆå¤§éƒ¨åˆ†ç‰¹å¾å€¼ä¸º0ï¼‰\n",
      "- ç¤ºä¾‹: 1 0:0.5 2:1.0 5:0.3\n",
      "âœ… ä¹³è…ºç™Œæ•°æ®åŠ è½½æˆåŠŸï¼\n",
      "   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: (683, 10)\n",
      "   æ ‡ç­¾å‘é‡å½¢çŠ¶: (683,)\n",
      "   æ•°æ®ç±»å‹: <class 'scipy.sparse._csr.csr_matrix'> (ç¨€ç–çŸ©é˜µ)\n",
      "\n",
      "ğŸ”¹ æ–¹æ³•2: åŠ è½½CSVæ ¼å¼\n",
      "CSVæ ¼å¼ç‰¹ç‚¹ï¼š\n",
      "- æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬\n",
      "- æ¯åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾æˆ–æ ‡ç­¾\n",
      "- é€‚åˆå¯†é›†æ•°æ®ï¼ˆå¤§éƒ¨åˆ†ç‰¹å¾å€¼é0ï¼‰\n",
      "\n",
      "ç¤ºä¾‹æ•°æ®:\n",
      "   feature1  feature2  feature3  target\n",
      "0       1.2       0.5       2.1       0\n",
      "1       2.3       1.7       1.9       1\n",
      "2       3.1       2.2       3.5       0\n",
      "3       4.5       3.8       2.7       1\n",
      "4       5.2       4.1       4.3       1\n",
      "\n",
      "ğŸ”¹ æ–¹æ³•3: åŠ è½½NumPyæ ¼å¼\n",
      "NumPyæ ¼å¼ç‰¹ç‚¹ï¼š\n",
      "- çº¯æ•°å€¼æ•°æ®ï¼Œå¤„ç†é€Ÿåº¦å¿«\n",
      "- å†…å­˜æ•ˆç‡é«˜\n",
      "- é€‚åˆå¤§è§„æ¨¡æ•°å€¼è®¡ç®—\n",
      "\n",
      "è½¬æ¢åçš„NumPyæ•°ç»„å½¢çŠ¶: (683, 10)\n",
      "æ•°æ®ç±»å‹: <class 'numpy.ndarray'>\n",
      "\n",
      "ğŸ’¡ æ•°æ®åŠ è½½è¦ç‚¹æ€»ç»“ï¼š\n",
      "1. é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹æ³•å–å†³äºæ•°æ®æ ¼å¼\n",
      "2. ç¨€ç–æ•°æ®ä½¿ç”¨LIBSVMæˆ–CSRæ ¼å¼èŠ‚çœå†…å­˜\n",
      "3. å¯†é›†æ•°æ®ä½¿ç”¨DataFrameæˆ–NumPyæ•°ç»„æ›´æ–¹ä¾¿\n",
      "4. æ€»æ˜¯æ£€æŸ¥æ•°æ®åŠ è½½åçš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” æ•°æ®åŠ è½½è¯¦è§£\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“š æ­¥éª¤1: æ•°æ®åŠ è½½\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ–¹æ³•1: åŠ è½½LIBSVMæ ¼å¼æ•°æ®ï¼ˆæˆ‘ä»¬å®éªŒç”¨çš„æ ¼å¼ï¼‰\n",
    "print(\"\\nğŸ”¹ æ–¹æ³•1: åŠ è½½LIBSVMæ ¼å¼\")\n",
    "print(\"LIBSVMæ ¼å¼ç‰¹ç‚¹ï¼š\")\n",
    "print(\"- æ¯è¡Œ: <æ ‡ç­¾> <ç‰¹å¾ç´¢å¼•>:<ç‰¹å¾å€¼> <ç‰¹å¾ç´¢å¼•>:<ç‰¹å¾å€¼> ...\")\n",
    "print(\"- é€‚åˆç¨€ç–æ•°æ®ï¼ˆå¤§éƒ¨åˆ†ç‰¹å¾å€¼ä¸º0ï¼‰\")\n",
    "print(\"- ç¤ºä¾‹: 1 0:0.5 2:1.0 5:0.3\")\n",
    "\n",
    "# å®é™…åŠ è½½ä¹³è…ºç™Œæ•°æ®\n",
    "try:\n",
    "    X_bc, y_bc = load_svmlight_file(\"../data/breast-cancer_scale\")\n",
    "    print(f\"âœ… ä¹³è…ºç™Œæ•°æ®åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_bc.shape}\")\n",
    "    print(f\"   æ ‡ç­¾å‘é‡å½¢çŠ¶: {y_bc.shape}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹: {type(X_bc)} (ç¨€ç–çŸ©é˜µ)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„\")\n",
    "\n",
    "# æ–¹æ³•2: åŠ è½½CSVæ ¼å¼æ•°æ®\n",
    "print(\"\\nğŸ”¹ æ–¹æ³•2: åŠ è½½CSVæ ¼å¼\")\n",
    "print(\"CSVæ ¼å¼ç‰¹ç‚¹ï¼š\")\n",
    "print(\"- æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬\")\n",
    "print(\"- æ¯åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾æˆ–æ ‡ç­¾\")\n",
    "print(\"- é€‚åˆå¯†é›†æ•°æ®ï¼ˆå¤§éƒ¨åˆ†ç‰¹å¾å€¼é0ï¼‰\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç¤ºä¾‹CSVæ•°æ®æ¥æ¼”ç¤º\n",
    "sample_data = {\n",
    "    'feature1': [1.2, 2.3, 3.1, 4.5, 5.2],\n",
    "    'feature2': [0.5, 1.7, 2.2, 3.8, 4.1],\n",
    "    'feature3': [2.1, 1.9, 3.5, 2.7, 4.3],\n",
    "    'target': [0, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "print(\"\\nç¤ºä¾‹æ•°æ®:\")\n",
    "print(df_sample)\n",
    "\n",
    "# æ–¹æ³•3: åŠ è½½NumPyæ ¼å¼æ•°æ®\n",
    "print(\"\\nğŸ”¹ æ–¹æ³•3: åŠ è½½NumPyæ ¼å¼\")\n",
    "print(\"NumPyæ ¼å¼ç‰¹ç‚¹ï¼š\")\n",
    "print(\"- çº¯æ•°å€¼æ•°æ®ï¼Œå¤„ç†é€Ÿåº¦å¿«\")\n",
    "print(\"- å†…å­˜æ•ˆç‡é«˜\")\n",
    "print(\"- é€‚åˆå¤§è§„æ¨¡æ•°å€¼è®¡ç®—\")\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå°†LIBSVMæ•°æ®è½¬æ¢ä¸ºNumPy\n",
    "if 'X_bc' in locals():\n",
    "    X_bc_dense = X_bc.toarray()  # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ\n",
    "    print(f\"\\nè½¬æ¢åçš„NumPyæ•°ç»„å½¢çŠ¶: {X_bc_dense.shape}\")\n",
    "    print(f\"æ•°æ®ç±»å‹: {type(X_bc_dense)}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•°æ®åŠ è½½è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹æ³•å–å†³äºæ•°æ®æ ¼å¼\")\n",
    "print(\"2. ç¨€ç–æ•°æ®ä½¿ç”¨LIBSVMæˆ–CSRæ ¼å¼èŠ‚çœå†…å­˜\")\n",
    "print(\"3. å¯†é›†æ•°æ®ä½¿ç”¨DataFrameæˆ–NumPyæ•°ç»„æ›´æ–¹ä¾¿\")\n",
    "print(\"4. æ€»æ˜¯æ£€æŸ¥æ•°æ®åŠ è½½åçš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” æ­¥éª¤2: æ•°æ®æ¢ç´¢\n",
    "\n",
    "### ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "æ•°æ®æ¢ç´¢å¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®çš„ç‰¹å¾ã€å‘ç°é—®é¢˜ã€ä¸ºåç»­å¤„ç†åšå‡†å¤‡ã€‚\n",
    "\n",
    "### ğŸ“Š éœ€è¦æ¢ç´¢çš„å†…å®¹ï¼š\n",
    "- æ•°æ®è§„æ¨¡ï¼ˆæ ·æœ¬æ•°ã€ç‰¹å¾æ•°ï¼‰\n",
    "- æ•°æ®ç±»å‹ï¼ˆæ•°å€¼å‹ã€åˆ†ç±»å‹ï¼‰\n",
    "- æ ‡ç­¾åˆ†å¸ƒï¼ˆç±»åˆ«æ˜¯å¦å¹³è¡¡ï¼‰\n",
    "- ç¼ºå¤±å€¼æƒ…å†µ\n",
    "- å¼‚å¸¸å€¼æƒ…å†µ\n",
    "- ç‰¹å¾åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” æ•°æ®æ¢ç´¢è¯¦è§£\n",
    "\n",
    "def explore_data(X, y, dataset_name=\"æ•°æ®é›†\"):\n",
    "    \"\"\"å…¨é¢æ¢ç´¢æ•°æ®é›†çš„ç‰¹å¾\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {dataset_name}æ•°æ®æ¢ç´¢æŠ¥å‘Š\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. åŸºæœ¬ä¿¡æ¯\n",
    "    print(f\"\\nğŸ”¹ åŸºæœ¬ä¿¡æ¯:\")\n",
    "    print(f\"   æ ·æœ¬æ•°é‡: {X.shape[0]}\")\n",
    "    print(f\"   ç‰¹å¾æ•°é‡: {X.shape[1]}\")\n",
    "    print(f\"   æ ‡ç­¾èŒƒå›´: {np.min(y)} - {np.max(y)}\")\n",
    "    \n",
    "    # 2. æ ‡ç­¾åˆ†å¸ƒåˆ†æ\n",
    "    print(f\"\\nğŸ”¹ æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = count / len(y) * 100\n",
    "        print(f\"   ç±»åˆ« {int(label)}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # æ£€æŸ¥ç±»åˆ«å¹³è¡¡æ€§\n",
    "    if len(unique_labels) == 2:  # äºŒåˆ†ç±»\n",
    "        balance_ratio = min(counts) / max(counts)\n",
    "        if balance_ratio > 0.8:\n",
    "            print(f\"   âœ… æ•°æ®å¹³è¡¡è‰¯å¥½ (æ¯”ä¾‹: {balance_ratio:.2f})\")\n",
    "        elif balance_ratio > 0.5:\n",
    "            print(f\"   âš ï¸ æ•°æ®è½»åº¦ä¸å¹³è¡¡ (æ¯”ä¾‹: {balance_ratio:.2f})\")\n",
    "        else:\n",
    "            print(f\"   âŒ æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ (æ¯”ä¾‹: {balance_ratio:.2f})\")\n",
    "    \n",
    "    # 3. ç‰¹å¾ç»Ÿè®¡\n",
    "    print(f\"\\nğŸ”¹ ç‰¹å¾ç»Ÿè®¡:\")\n",
    "    if hasattr(X, 'toarray'):  # ç¨€ç–çŸ©é˜µ\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X\n",
    "    \n",
    "    print(f\"   ç‰¹å¾å‡å€¼èŒƒå›´: {np.min(np.mean(X_dense, axis=0)):.4f} - {np.max(np.mean(X_dense, axis=0)):.4f}\")\n",
    "    print(f\"   ç‰¹å¾æ–¹å·®èŒƒå›´: {np.min(np.var(X_dense, axis=0)):.4f} - {np.max(np.var(X_dense, axis=0)):.4f}\")\n",
    "    print(f\"   ç‰¹å¾æœ€å°å€¼: {np.min(X_dense):.4f}\")\n",
    "    print(f\"   ç‰¹å¾æœ€å¤§å€¼: {np.max(X_dense):.4f}\")\n",
    "    \n",
    "    # 4. æ•°æ®å¯†åº¦åˆ†æï¼ˆé’ˆå¯¹ç¨€ç–æ•°æ®ï¼‰\n",
    "    if hasattr(X, 'toarray'):\n",
    "        sparsity = 1 - X.nnz / (X.shape[0] * X.shape[1])\n",
    "        print(f\"\\nğŸ”¹ ç¨€ç–åº¦åˆ†æ:\")\n",
    "        print(f\"   éé›¶å…ƒç´ æ¯”ä¾‹: {(1-sparsity)*100:.2f}%\")\n",
    "        print(f\"   ç¨€ç–åº¦: {sparsity*100:.2f}%\")\n",
    "        \n",
    "        if sparsity > 0.9:\n",
    "            print(f\"   âœ… é«˜åº¦ç¨€ç–æ•°æ®ï¼Œé€‚åˆä½¿ç”¨ç¨€ç–çŸ©é˜µæ ¼å¼\")\n",
    "        elif sparsity > 0.5:\n",
    "            print(f\"   âš ï¸ ä¸­ç­‰ç¨€ç–æ•°æ®\")\n",
    "        else:\n",
    "            print(f\"   ğŸ“Š å¯†é›†æ•°æ®ï¼Œå¯ä»¥è€ƒè™‘è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ\")\n",
    "    \n",
    "    return X_dense\n",
    "\n",
    "# æ¢ç´¢ä¹³è…ºç™Œæ•°æ®\n",
    "if 'X_bc' in locals():\n",
    "    X_bc_dense = explore_data(X_bc, y_bc, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "# æ¢ç´¢é¸¢å°¾èŠ±æ•°æ®\n",
    "try:\n",
    "    X_iris, y_iris = load_svmlight_file(\"../data/iris.scale\")\n",
    "    X_iris_dense = explore_data(X_iris, y_iris, \"é¸¢å°¾èŠ±\")\n",
    "except FileNotFoundError:\n",
    "    print(\"é¸¢å°¾èŠ±æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•°æ®æ¢ç´¢è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. äº†è§£æ•°æ®è§„æ¨¡ï¼Œé€‰æ‹©åˆé€‚çš„ç®—æ³•\")\n",
    "print(\"2. æ£€æŸ¥ç±»åˆ«å¹³è¡¡ï¼Œé¿å…è¯„ä¼°åå·®\")\n",
    "print(\"3. åˆ†æç‰¹å¾åˆ†å¸ƒï¼Œé€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ–¹æ³•\")\n",
    "print(\"4. è¯†åˆ«æ•°æ®ç‰¹æ€§ï¼Œä¼˜åŒ–å­˜å‚¨å’Œå¤„ç†æ–¹å¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ æ­¥éª¤3: æ•°æ®æ¸…æ´—\n",
    "\n",
    "### ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "çœŸå®ä¸–ç•Œçš„æ•°æ®å¾€å¾€ä¸å®Œç¾ï¼Œéœ€è¦æ¸…æ´—æ‰èƒ½ç”¨äºæ¨¡å‹è®­ç»ƒã€‚\n",
    "\n",
    "### ğŸ§¹ å¸¸è§çš„æ¸…æ´—ä»»åŠ¡ï¼š\n",
    "- **å¤„ç†ç¼ºå¤±å€¼** - å¡«å……æˆ–åˆ é™¤\n",
    "- **å¤„ç†å¼‚å¸¸å€¼** - è¯†åˆ«å’Œå¤„ç†\n",
    "- **å¤„ç†é‡å¤å€¼** - å»é™¤é‡å¤æ ·æœ¬\n",
    "- **æ•°æ®ç±»å‹è½¬æ¢** - ç¡®ä¿æ­£ç¡®çš„æ•°æ®ç±»å‹\n",
    "- **æ ‡ç­¾æ ‡å‡†åŒ–** - ç»Ÿä¸€æ ‡ç­¾æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ æ•°æ®æ¸…æ´—è¯¦è§£\n",
    "\n",
    "def clean_data(X, y, dataset_name=\"æ•°æ®é›†\"):\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®æ¸…æ´—çš„å„ä¸ªç¯èŠ‚\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ§¹ {dataset_name}æ•°æ®æ¸…æ´—\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µè¿›è¡Œå¤„ç†\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X.copy()\n",
    "    \n",
    "    original_shape = X_dense.shape\n",
    "    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {original_shape}\")\n",
    "    \n",
    "    # 1. æ£€æŸ¥å’Œå¤„ç†ç¼ºå¤±å€¼\n",
    "    print(f\"\\nğŸ”¹ ç¼ºå¤±å€¼å¤„ç†:\")\n",
    "    missing_count = np.sum(np.isnan(X_dense))\n",
    "    if missing_count > 0:\n",
    "        print(f\"   å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼\")\n",
    "        \n",
    "        # ç­–ç•¥1: åˆ é™¤å«æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "        # X_clean = X_dense[~np.isnan(X_dense).any(axis=1)]\n",
    "        \n",
    "        # ç­–ç•¥2: ç”¨å‡å€¼å¡«å……ï¼ˆæ¨èç”¨äºæ•°å€¼ç‰¹å¾ï¼‰\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_dense = imputer.fit_transform(X_dense)\n",
    "        print(f\"   âœ… ä½¿ç”¨å‡å€¼å¡«å……å®Œæˆç¼ºå¤±å€¼å¤„ç†\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°ç¼ºå¤±å€¼\")\n",
    "    \n",
    "    # 2. æ£€æŸ¥å’Œå¤„ç†å¼‚å¸¸å€¼\n",
    "    print(f\"\\nğŸ”¹ å¼‚å¸¸å€¼å¤„ç†:\")\n",
    "    \n",
    "    # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼\n",
    "    def detect_outliers_iqr(data):\n",
    "        Q1 = np.percentile(data, 25)\n",
    "        Q3 = np.percentile(data, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    outlier_counts = []\n",
    "    for feature_idx in range(X_dense.shape[1]):\n",
    "        feature_data = X_dense[:, feature_idx]\n",
    "        outliers = detect_outliers_iqr(feature_data)\n",
    "        outlier_count = np.sum(outliers)\n",
    "        outlier_counts.append(outlier_count)\n",
    "    \n",
    "    total_outliers = sum(outlier_counts)\n",
    "    if total_outliers > 0:\n",
    "        print(f\"   å‘ç° {total_outliers} ä¸ªå¼‚å¸¸å€¼\")\n",
    "        print(f\"   å„ç‰¹å¾å¼‚å¸¸å€¼æ•°é‡: {outlier_counts[:5]}...\")  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾\n",
    "        \n",
    "        # å¤„ç†ç­–ç•¥ï¼šä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼\n",
    "        for feature_idx in range(X_dense.shape[1]):\n",
    "            feature_data = X_dense[:, feature_idx]\n",
    "            outliers = detect_outliers_iqr(feature_data)\n",
    "            if np.any(outliers):\n",
    "                median_val = np.median(feature_data[~outliers])\n",
    "                X_dense[outliers, feature_idx] = median_val\n",
    "        \n",
    "        print(f\"   âœ… ä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸å€¼\")\n",
    "    \n",
    "    # 3. æ£€æŸ¥é‡å¤å€¼\n",
    "    print(f\"\\nğŸ”¹ é‡å¤å€¼å¤„ç†:\")\n",
    "    \n",
    "    # å°†ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆåœ¨ä¸€èµ·æ£€æŸ¥é‡å¤\n",
    "    data_with_labels = np.column_stack([X_dense, y])\n",
    "    unique_data, unique_indices = np.unique(data_with_labels, axis=0, return_index=True)\n",
    "    \n",
    "    duplicate_count = len(data_with_labels) - len(unique_data)\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   å‘ç° {duplicate_count} ä¸ªé‡å¤æ ·æœ¬\")\n",
    "        X_dense = X_dense[unique_indices]\n",
    "        y = y[unique_indices]\n",
    "        print(f\"   âœ… å·²åˆ é™¤é‡å¤æ ·æœ¬\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°é‡å¤æ ·æœ¬\")\n",
    "    \n",
    "    # 4. æ ‡ç­¾æ ‡å‡†åŒ–\n",
    "    print(f\"\\nğŸ”¹ æ ‡ç­¾æ ‡å‡†åŒ–:\")\n",
    "    \n",
    "    unique_labels = np.unique(y)\n",
    "    print(f\"   åŸå§‹æ ‡ç­¾: {unique_labels}\")\n",
    "    \n",
    "    # ç¡®ä¿æ ‡ç­¾ä»0å¼€å§‹è¿ç»­ç¼–å·\n",
    "    if np.min(y) != 0 or len(unique_labels) != np.max(y) + 1:\n",
    "        print(f\"   æ ‡ç­¾éœ€è¦æ ‡å‡†åŒ–...\")\n",
    "        \n",
    "        # åˆ›å»ºæ ‡ç­¾æ˜ å°„\n",
    "        label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(unique_labels))}\n",
    "        print(f\"   æ ‡ç­¾æ˜ å°„: {label_mapping}\")\n",
    "        \n",
    "        # åº”ç”¨æ˜ å°„\n",
    "        y_standardized = np.array([label_mapping[label] for label in y])\n",
    "        print(f\"   æ ‡å‡†åŒ–åæ ‡ç­¾: {np.unique(y_standardized)}\")\n",
    "        y = y_standardized\n",
    "    else:\n",
    "        print(f\"   âœ… æ ‡ç­¾å·²ç»æ˜¯æ ‡å‡†æ ¼å¼\")\n",
    "    \n",
    "    # 5. æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    print(f\"\\nğŸ”¹ æ•°æ®ç±»å‹ä¼˜åŒ–:\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„ç±»å‹\n",
    "    if X_dense.dtype == np.float64:\n",
    "        print(f\"   å½“å‰æ•°æ®ç±»å‹: {X_dense.dtype}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ•°å€¼èŒƒå›´ï¼Œçœ‹æ˜¯å¦å¯ä»¥ç”¨float32\n",
    "        if np.all(np.abs(X_dense) < 1e6):  # ç®€å•æ£€æŸ¥\n",
    "            X_dense = X_dense.astype(np.float32)\n",
    "            print(f\"   âœ… è½¬æ¢ä¸ºfloat32ï¼ŒèŠ‚çœå†…å­˜\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ æ•°å€¼èŒƒå›´è¾ƒå¤§ï¼Œä¿æŒfloat64\")\n",
    "    \n",
    "    # æ€»ç»“æ¸…æ´—ç»“æœ\n",
    "    print(f\"\\nğŸ“‹ æ¸…æ´—æ€»ç»“:\")\n",
    "    print(f\"   åŸå§‹æ•°æ®: {original_shape}\")\n",
    "    print(f\"   æ¸…æ´—åæ•°æ®: {X_dense.shape}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹: {X_dense.dtype}\")\n",
    "    print(f\"   æ ‡ç­¾èŒƒå›´: {np.min(y)} - {np.max(y)}\")\n",
    "    \n",
    "    return X_dense, y\n",
    "\n",
    "# æ¼”ç¤ºæ•°æ®æ¸…æ´—è¿‡ç¨‹\n",
    "if 'X_bc' in locals():\n",
    "    X_bc_clean, y_bc_clean = clean_data(X_bc, y_bc, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•°æ®æ¸…æ´—è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. ç¼ºå¤±å€¼å¤„ç†ï¼šåˆ é™¤æˆ–å¡«å……ï¼Œå¡«å……é€šå¸¸æ¯”åˆ é™¤å¥½\")\n",
    "print(\"2. å¼‚å¸¸å€¼å¤„ç†ï¼šè¯†åˆ«å¹¶æ ¹æ®æƒ…å†µå¤„ç†ï¼Œé¿å…è¿‡åº¦å¤„ç†\")\n",
    "print(\"3. é‡å¤å€¼å¤„ç†ï¼šé€šå¸¸åº”è¯¥åˆ é™¤\")\n",
    "print(\"4. æ ‡ç­¾æ ‡å‡†åŒ–ï¼šç¡®ä¿æ ‡ç­¾ä»0å¼€å§‹è¿ç»­ç¼–å·\")\n",
    "print(\"5. æ•°æ®ç±»å‹ä¼˜åŒ–ï¼šåœ¨ç²¾åº¦å’Œå†…å­˜ä¹‹é—´æ‰¾åˆ°å¹³è¡¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ æ­¥éª¤4: æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "### ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "ä¸åŒç®—æ³•å¯¹æ•°æ®çš„è¦æ±‚ä¸åŒï¼Œé¢„å¤„ç†å¯ä»¥æé«˜ç®—æ³•æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚\n",
    "\n",
    "### âš™ï¸ å¸¸è§çš„é¢„å¤„ç†æŠ€æœ¯ï¼š\n",
    "- **ç‰¹å¾ç¼©æ”¾** - æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–\n",
    "- **ç‰¹å¾é€‰æ‹©** - é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾\n",
    "- **é™ç»´** - PCAã€LDAç­‰\n",
    "- **ç¼–ç è½¬æ¢** - ç±»åˆ«ç‰¹å¾ç¼–ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ æ•°æ®é¢„å¤„ç†è¯¦è§£\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_data(X, y, dataset_name=\"æ•°æ®é›†\"):\n",
    "    \"\"\"å…¨é¢çš„æ•°æ®é¢„å¤„ç†æ¼”ç¤º\"\"\"\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ {dataset_name}æ•°æ®é¢„å¤„ç†\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. ç‰¹å¾ç¼©æ”¾\n",
    "    print(f\"\\nğŸ”¹ ç‰¹å¾ç¼©æ”¾:\")\n",
    "    print(f\"   åŸå§‹æ•°æ®ç»Ÿè®¡:\")\n",
    "    print(f\"   å‡å€¼èŒƒå›´: {np.min(np.mean(X, axis=0)):.4f} - {np.max(np.mean(X, axis=0)):.4f}\")\n",
    "    print(f\"   æ ‡å‡†å·®èŒƒå›´: {np.min(np.std(X, axis=0)):.4f} - {np.max(np.std(X, axis=0)):.4f}\")\n",
    "    \n",
    "    # æ–¹æ³•1: æ ‡å‡†åŒ– (Z-score normalization)\n",
    "    scaler_standard = StandardScaler()\n",
    "    X_standard = scaler_standard.fit_transform(X)\n",
    "    print(f\"\\n   ğŸ“Š æ ‡å‡†åŒ–å:\")\n",
    "    print(f\"   å‡å€¼èŒƒå›´: {np.min(np.mean(X_standard, axis=0)):.4f} - {np.max(np.mean(X_standard, axis=0)):.4f}\")\n",
    "    print(f\"   æ ‡å‡†å·®èŒƒå›´: {np.min(np.std(X_standard, axis=0)):.4f} - {np.max(np.std(X_standard, axis=0)):.4f}\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯SVMã€é€»è¾‘å›å½’\")\n",
    "    \n",
    "    # æ–¹æ³•2: å½’ä¸€åŒ– (Min-Max normalization)\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    X_minmax = scaler_minmax.fit_transform(X)\n",
    "    print(f\"\\n   ğŸ“Š å½’ä¸€åŒ–å:\")\n",
    "    print(f\"   æ•°å€¼èŒƒå›´: {np.min(X_minmax):.4f} - {np.max(X_minmax):.4f}\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: ç¥ç»ç½‘ç»œã€å›¾åƒå¤„ç†ã€è·ç¦»æ•æ„Ÿçš„ç®—æ³•\")\n",
    "    \n",
    "    # æ–¹æ³•3: é²æ£’ç¼©æ”¾ (Robust scaling)\n",
    "    scaler_robust = RobustScaler()\n",
    "    X_robust = scaler_robust.fit_transform(X)\n",
    "    print(f\"\\n   ğŸ“Š é²æ£’ç¼©æ”¾å:\")\n",
    "    print(f\"   ä¸­ä½æ•°èŒƒå›´: {np.min(np.median(X_robust, axis=0)):.4f} - {np.max(np.median(X_robust, axis=0)):.4f}\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: å­˜åœ¨å¼‚å¸¸å€¼çš„æ•°æ®\")\n",
    "    \n",
    "    # 2. ç‰¹å¾é€‰æ‹©\n",
    "    print(f\"\\nğŸ”¹ ç‰¹å¾é€‰æ‹©:\")\n",
    "    \n",
    "    # ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)  # é€‰æ‹©å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
    "    X_selected = selector.fit_transform(X_standard, y)\n",
    "    \n",
    "    selected_features = selector.get_support()\n",
    "    feature_scores = selector.scores_\n",
    "    \n",
    "    print(f\"   åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}\")\n",
    "    print(f\"   é€‰æ‹©åç‰¹å¾æ•°é‡: {X_selected.shape[1]}\")\n",
    "    print(f\"   ç‰¹å¾åˆ†æ•°èŒƒå›´: {np.min(feature_scores):.2f} - {np.max(feature_scores):.2f}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾\n",
    "    top_indices = np.argsort(feature_scores)[-10:][::-1]  # åˆ†æ•°æœ€é«˜çš„10ä¸ªç‰¹å¾\n",
    "    print(f\"   æœ€é‡è¦ç‰¹å¾ç´¢å¼•: {top_indices[:5]}...\")  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: é«˜ç»´æ•°æ®ã€ç‰¹å¾å†—ä½™ã€éœ€è¦æé«˜è®­ç»ƒé€Ÿåº¦\")\n",
    "    \n",
    "    # 3. é™ç»´ (PCA)\n",
    "    print(f\"\\nğŸ”¹ é™ç»´ (PCA):\")\n",
    "    \n",
    "    # ä¿ç•™95%çš„æ–¹å·®\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_standard)\n",
    "    \n",
    "    print(f\"   åŸå§‹ç»´åº¦: {X_standard.shape[1]}\")\n",
    "    print(f\"   PCAåç»´åº¦: {X_pca.shape[1]}\")\n",
    "    print(f\"   ä¿ç•™æ–¹å·®æ¯”ä¾‹: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: ç‰¹å¾é«˜åº¦ç›¸å…³ã€éœ€è¦å¯è§†åŒ–ã€å™ªå£°è¾ƒå¤š\")\n",
    "    \n",
    "    # å¯è§†åŒ–ä¸»æˆåˆ†çš„è´¡çŒ®\n",
    "    if len(pca.explained_variance_ratio_) > 5:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(1, min(6, len(pca.explained_variance_ratio_) + 1)), \n",
    "                pca.explained_variance_ratio_[:5])\n",
    "        plt.xlabel('ä¸»æˆåˆ†')\n",
    "        plt.ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')\n",
    "        plt.title(f'{dataset_name}: å‰5ä¸ªä¸»æˆåˆ†çš„è´¡çŒ®')\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'standard': X_standard,\n",
    "        'minmax': X_minmax,\n",
    "        'robust': X_robust,\n",
    "        'selected': X_selected,\n",
    "        'pca': X_pca\n",
    "    }, {\n",
    "        'standard_scaler': scaler_standard,\n",
    "        'minmax_scaler': scaler_minmax,\n",
    "        'robust_scaler': scaler_robust,\n",
    "        'selector': selector,\n",
    "        'pca': pca\n",
    "    }\n",
    "\n",
    "# æ¼”ç¤ºæ•°æ®é¢„å¤„ç†\n",
    "if 'X_bc_clean' in locals():\n",
    "    preprocessed_data, preprocessors = preprocess_data(X_bc_clean, y_bc_clean, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•°æ®é¢„å¤„ç†è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. æ ‡å‡†åŒ–: é€‚åˆå¤§å¤šæ•°ç®—æ³•ï¼Œæ¶ˆé™¤é‡çº²å½±å“\")\n",
    "print(\"2. å½’ä¸€åŒ–: é€‚åˆç¥ç»ç½‘ç»œï¼Œä¿æŒæ•°æ®çš„åŸå§‹åˆ†å¸ƒç‰¹å¾\")\n",
    "print(\"3. é²æ£’ç¼©æ”¾: é€‚åˆæœ‰å¼‚å¸¸å€¼çš„æ•°æ®\")\n",
    "print(\"4. ç‰¹å¾é€‰æ‹©: å‡å°‘ç»´åº¦ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦ï¼Œé¿å…è¿‡æ‹Ÿåˆ\")\n",
    "print(\"5. é™ç»´: å¤„ç†ç›¸å…³ç‰¹å¾ï¼Œé™å™ªï¼Œå¯è§†åŒ–\")\n",
    "print(\"\\nâš ï¸ é‡è¦æé†’: æµ‹è¯•é›†çš„é¢„å¤„ç†å¿…é¡»ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ æ­¥éª¤5: æ•°æ®åˆ†å‰²\n",
    "\n",
    "### ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "æ•°æ®åˆ†å‰²æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®æ­¥éª¤ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "### âœ‚ï¸ åˆ†å‰²ç­–ç•¥ï¼š\n",
    "- **è®­ç»ƒé›†-æµ‹è¯•é›†åˆ†å‰²** - åŸºç¡€åˆ†å‰²æ–¹æ³•\n",
    "- **äº¤å‰éªŒè¯** - æ›´å¯é çš„è¯„ä¼°æ–¹æ³•\n",
    "- **åˆ†å±‚é‡‡æ ·** - ä¿æŒç±»åˆ«åˆ†å¸ƒ\n",
    "- **æ—¶é—´åºåˆ—åˆ†å‰²** - é’ˆå¯¹æ—¶é—´æ•°æ®\n",
    "- **åˆ†ç»„åˆ†å‰²** - é¿å…æ•°æ®æ³„éœ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ‚ï¸ æ•°æ®åˆ†å‰²è¯¦è§£\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def demonstrate_data_splitting(X, y, dataset_name=\"æ•°æ®é›†\"):\n",
    "    \"\"\"æ¼”ç¤ºå„ç§æ•°æ®åˆ†å‰²æ–¹æ³•\"\"\"\n",
    "    \n",
    "    print(f\"\\nâœ‚ï¸ {dataset_name}æ•°æ®åˆ†å‰²æ¼”ç¤º\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. åŸºç¡€è®­ç»ƒ-æµ‹è¯•åˆ†å‰²\n",
    "    print(f\"\\nğŸ”¹ æ–¹æ³•1: åŸºç¡€è®­ç»ƒ-æµ‹è¯•åˆ†å‰²\")\n",
    "    \n",
    "    X_train_basic, X_test_basic, y_train_basic, y_test_basic = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†: {X_train_basic.shape[0]} æ ·æœ¬\")\n",
    "    print(f\"   æµ‹è¯•é›†: {X_test_basic.shape[0]} æ ·æœ¬\")\n",
    "    print(f\"   åˆ†å‰²æ¯”ä¾‹: {X_train_basic.shape[0]/len(X):.1%} / {X_test_basic.shape[0]/len(X):.1%}\")\n",
    "    print(f\"   ä¼˜ç‚¹: ç®€å•ã€å¿«é€Ÿ\")\n",
    "    print(f\"   ç¼ºç‚¹: å¯èƒ½å¯¼è‡´ç±»åˆ«ä¸å¹³è¡¡ã€è¯„ä¼°ä¸ç¨³å®š\")\n",
    "    \n",
    "    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ\n",
    "    def check_class_distribution(y, name):\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        ratios = counts / len(y)\n",
    "        print(f\"   {name}ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, ratios))}\")\n",
    "    \n",
    "    check_class_distribution(y_train_basic, \"è®­ç»ƒé›†\")\n",
    "    check_class_distribution(y_test_basic, \"æµ‹è¯•é›†\")\n",
    "    \n",
    "    # 2. åˆ†å±‚åˆ†å‰² (Stratified Split)\n",
    "    print(f\"\\nğŸ”¹ æ–¹æ³•2: åˆ†å±‚åˆ†å‰²\")\n",
    "    \n",
    "    X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†: {X_train_strat.shape[0]} æ ·æœ¬\")\n",
    "    print(f\"   æµ‹è¯•é›†: {X_test_strat.shape[0]} æ ·æœ¬\")\n",
    "    print(f\"   ä¼˜ç‚¹: ä¿æŒç±»åˆ«åˆ†å¸ƒä¸€è‡´\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: ç±»åˆ«ä¸å¹³è¡¡æ•°æ®\")\n",
    "    \n",
    "    check_class_distribution(y_train_strat, \"åˆ†å±‚è®­ç»ƒé›†\")\n",
    "    check_class_distribution(y_test_strat, \"åˆ†å±‚æµ‹è¯•é›†\")\n",
    "    \n",
    "    # å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç±»åˆ«åˆ†å¸ƒå·®å¼‚\n",
    "    original_dist = dict(zip(*np.unique(y, return_counts=True)))\n",
    "    basic_test_dist = dict(zip(*np.unique(y_test_basic, return_counts=True)))\n",
    "    strat_test_dist = dict(zip(*np.unique(y_test_strat, return_counts=True)))\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”:\")\n",
    "    print(f\"   åŸå§‹æ•°æ®: {original_dist}\")\n",
    "    print(f\"   åŸºç¡€åˆ†å‰²æµ‹è¯•é›†: {basic_test_dist}\")\n",
    "    print(f\"   åˆ†å±‚åˆ†å‰²æµ‹è¯•é›†: {strat_test_dist}\")\n",
    "    \n",
    "    # 3. äº¤å‰éªŒè¯æ¼”ç¤º\n",
    "    print(f\"\\nğŸ”¹ æ–¹æ³•3: KæŠ˜äº¤å‰éªŒè¯\")\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"   æŠ˜æ•°: 5\")\n",
    "    print(f\"   æ¯æŠ˜å¤§å°: {len(X) // 5}\")\n",
    "    print(f\"   ä¼˜ç‚¹: æ›´å¯é çš„æ€§èƒ½è¯„ä¼°\")\n",
    "    print(f\"   ç¼ºç‚¹: è®¡ç®—æˆæœ¬é«˜\")\n",
    "    \n",
    "    # å¯è§†åŒ–äº¤å‰éªŒè¯åˆ†å‰²\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "        # åˆ›å»ºæ•°ç»„è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„åˆ†é…\n",
    "        sample_allocation = np.zeros(len(X))\n",
    "        sample_allocation[train_idx] = 1  # è®­ç»ƒé›†\n",
    "        sample_allocation[test_idx] = 2   # æµ‹è¯•é›†\n",
    "        \n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(sample_allocation.reshape(-1, 1), cmap='RdYlGn', aspect='auto')\n",
    "        plt.title(f'Fold {i+1}')\n",
    "        plt.yticks([])\n",
    "        plt.xlabel('æ ·æœ¬ç´¢å¼•')\n",
    "    \n",
    "    plt.suptitle(f'{dataset_name}: 5æŠ˜äº¤å‰éªŒè¯åˆ†å‰²ç¤ºæ„å›¾\\nç»¿è‰²=è®­ç»ƒé›†, çº¢è‰²=æµ‹è¯•é›†')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. åˆ†å±‚äº¤å‰éªŒè¯\n",
    "    print(f\"\\nğŸ”¹ æ–¹æ³•4: åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯\")\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"   ä¼˜ç‚¹: æ¯æŠ˜éƒ½ä¿æŒç±»åˆ«åˆ†å¸ƒ\")\n",
    "    print(f\"   é€‚ç”¨åœºæ™¯: ç±»åˆ«ä¸å¹³è¡¡æ•°æ®çš„å¯é è¯„ä¼°\")\n",
    "    \n",
    "    # 5. æ•°æ®æ³„éœ²æ£€æŸ¥\n",
    "    print(f\"\\nğŸ”¹ æ•°æ®æ³„éœ²æ£€æŸ¥\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ ·æœ¬åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    def check_data_leakage(X_train, X_test, y_train, y_test, method_name):\n",
    "        # å°†æ ·æœ¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿æ¯”è¾ƒ\n",
    "        train_samples = [str(x) + str(y) for x, y in zip(X_train, y_train)]\n",
    "        test_samples = [str(x) + str(y) for x, y in zip(X_test, y_test)]\n",
    "        \n",
    "        leakage = len(set(train_samples) & set(test_samples))\n",
    "        \n",
    "        if leakage > 0:\n",
    "            print(f\"   âš ï¸ {method_name}: å‘ç° {leakage} ä¸ªæ³„éœ²æ ·æœ¬\")\n",
    "        else:\n",
    "            print(f\"   âœ… {method_name}: æ— æ•°æ®æ³„éœ²\")\n",
    "    \n",
    "    check_data_leakage(X_train_basic, X_test_basic, y_train_basic, y_test_basic, \"åŸºç¡€åˆ†å‰²\")\n",
    "    check_data_leakage(X_train_strat, X_test_strat, y_train_strat, y_test_strat, \"åˆ†å±‚åˆ†å‰²\")\n",
    "    \n",
    "    return {\n",
    "        'basic_split': (X_train_basic, X_test_basic, y_train_basic, y_test_basic),\n",
    "        'stratified_split': (X_train_strat, X_test_strat, y_train_strat, y_test_strat),\n",
    "        'kfold': kfold,\n",
    "        'stratified_kfold': strat_kfold\n",
    "    }\n",
    "\n",
    "# æ¼”ç¤ºæ•°æ®åˆ†å‰²\n",
    "if 'X_bc_clean' in locals():\n",
    "    split_results = demonstrate_data_splitting(X_bc_clean, y_bc_clean, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ•°æ®åˆ†å‰²è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. åŸºç¡€åˆ†å‰²: ç®€å•ä½†å¯èƒ½å¯¼è‡´ç±»åˆ«ä¸å¹³è¡¡\")\n",
    "print(\"2. åˆ†å±‚åˆ†å‰²: ä¿æŒç±»åˆ«åˆ†å¸ƒï¼Œæ¨èç”¨äºåˆ†ç±»ä»»åŠ¡\")\n",
    "print(\"3. äº¤å‰éªŒè¯: æ›´å¯é çš„è¯„ä¼°ï¼Œä½†è®¡ç®—æˆæœ¬é«˜\")\n",
    "print(\"4. åˆ†å±‚äº¤å‰éªŒè¯: ç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼Œé€‚åˆç±»åˆ«ä¸å¹³è¡¡æ•°æ®\")\n",
    "print(\"5. æ•°æ®æ³„éœ²: å¿…é¡»æ£€æŸ¥é¿å…ï¼Œå¦åˆ™ä¼šé«˜ä¼°æ¨¡å‹æ€§èƒ½\")\n",
    "print(\"\\nğŸ”¥ é‡è¦åŸåˆ™: æ°¸è¿œä¸è¦åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä»»ä½•æ•°æ®é¢„å¤„ç†çš„å­¦ä¹ ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "\n",
    "### ğŸ¯ ç›®æ ‡\n",
    "å°†å‰é¢å­¦ä¹ çš„æ‰€æœ‰æ­¥éª¤ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"å®Œæ•´çš„æ•°æ®å¤„ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 handle_missing=True,\n",
    "                 handle_outliers=True,\n",
    "                 handle_duplicates=True,\n",
    "                 scaling_method='standard',\n",
    "                 feature_selection=False,\n",
    "                 n_features_to_select=10):\n",
    "        \n",
    "        self.handle_missing = handle_missing\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.handle_duplicates = handle_duplicates\n",
    "        self.scaling_method = scaling_method\n",
    "        self.feature_selection = feature_selection\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "        \n",
    "        # åˆå§‹åŒ–å¤„ç†å™¨\n",
    "        self.scaler = None\n",
    "        self.feature_selector = None\n",
    "        self.imputer = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"å­¦ä¹ æ•°æ®å¤„ç†å‚æ•°\"\"\"\n",
    "        print(\"ğŸ”§ å¼€å§‹å­¦ä¹ æ•°æ®å¤„ç†å‚æ•°...\")\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X_dense = X.toarray()\n",
    "        else:\n",
    "            X_dense = X.copy()\n",
    "        \n",
    "        # å­¦ä¹ æ ‡å‡†åŒ–å‚æ•°\n",
    "        if self.scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif self.scaling_method == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        \n",
    "        if self.scaler:\n",
    "            self.scaler.fit(X_dense)\n",
    "            print(f\"   âœ… å­¦ä¹ {self.scaling_method}ç¼©æ”¾å‚æ•°\")\n",
    "        \n",
    "        # å­¦ä¹ ç‰¹å¾é€‰æ‹©å‚æ•°\n",
    "        if self.feature_selection:\n",
    "            self.feature_selector = SelectKBest(\n",
    "                score_func=f_classif, \n",
    "                k=min(self.n_features_to_select, X_dense.shape[1])\n",
    "            )\n",
    "            # å…ˆæ ‡å‡†åŒ–å†é€‰æ‹©ç‰¹å¾\n",
    "            X_scaled = self.scaler.transform(X_dense)\n",
    "            self.feature_selector.fit(X_scaled, y)\n",
    "            print(f\"   âœ… å­¦ä¹ ç‰¹å¾é€‰æ‹©å‚æ•°\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(f\"   âœ… æ•°æ®å¤„ç†å‚æ•°å­¦ä¹ å®Œæˆ\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"åº”ç”¨æ•°æ®å¤„ç†\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨fitæ–¹æ³•å­¦ä¹ å‚æ•°\")\n",
    "        \n",
    "        print(\"ğŸ”„ å¼€å§‹æ•°æ®å¤„ç†...\")\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X_dense = X.toarray()\n",
    "        else:\n",
    "            X_dense = X.copy()\n",
    "        \n",
    "        # 1. å¤„ç†ç¼ºå¤±å€¼\n",
    "        if self.handle_missing:\n",
    "            missing_count = np.sum(np.isnan(X_dense))\n",
    "            if missing_count > 0:\n",
    "                if self.imputer is None:\n",
    "                    self.imputer = SimpleImputer(strategy='mean')\n",
    "                    X_dense = self.imputer.fit_transform(X_dense)\n",
    "                else:\n",
    "                    X_dense = self.imputer.transform(X_dense)\n",
    "                print(f\"   âœ… å¤„ç†äº† {missing_count} ä¸ªç¼ºå¤±å€¼\")\n",
    "        \n",
    "        # 2. ç‰¹å¾ç¼©æ”¾\n",
    "        if self.scaler:\n",
    "            X_dense = self.scaler.transform(X_dense)\n",
    "            print(f\"   âœ… åº”ç”¨{self.scaling_method}ç¼©æ”¾\")\n",
    "        \n",
    "        # 3. ç‰¹å¾é€‰æ‹©\n",
    "        if self.feature_selection and self.feature_selector:\n",
    "            original_features = X_dense.shape[1]\n",
    "            X_dense = self.feature_selector.transform(X_dense)\n",
    "            print(f\"   âœ… ç‰¹å¾é€‰æ‹©: {original_features} -> {X_dense.shape[1]}\")\n",
    "        \n",
    "        print(f\"   âœ… æ•°æ®å¤„ç†å®Œæˆ\")\n",
    "        return X_dense\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"å­¦ä¹ å‚æ•°å¹¶åº”ç”¨å¤„ç†\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "def create_ml_pipeline(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"åˆ›å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ æµæ°´çº¿\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ åˆ›å»ºå®Œæ•´æœºå™¨å­¦ä¹ æµæ°´çº¿\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. æ•°æ®åˆ†å‰²\n",
    "    print(\"\\nğŸ“Š æ­¥éª¤1: æ•°æ®åˆ†å‰²\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    print(f\"   è®­ç»ƒé›†: {X_train.shape}\")\n",
    "    print(f\"   æµ‹è¯•é›†: {X_test.shape}\")\n",
    "    \n",
    "    # 2. æ•°æ®å¤„ç†å™¨\n",
    "    print(\"\\nğŸ”§ æ­¥éª¤2: æ•°æ®å¤„ç†å™¨è®¾ç½®\")\n",
    "    processor = DataProcessor(\n",
    "        handle_missing=True,\n",
    "        handle_outliers=True,\n",
    "        scaling_method='standard',\n",
    "        feature_selection=False  # å¯¹äºé«˜ç»´æ•°æ®å¯ä»¥è®¾ä¸ºTrue\n",
    "    )\n",
    "    \n",
    "    # 3. è®­ç»ƒé›†å¤„ç†\n",
    "    print(\"\\nâš™ï¸ æ­¥éª¤3: è®­ç»ƒé›†å¤„ç†\")\n",
    "    X_train_processed = processor.fit_transform(X_train, y_train)\n",
    "    \n",
    "    # 4. æµ‹è¯•é›†å¤„ç†ï¼ˆä½¿ç”¨è®­ç»ƒé›†å­¦åˆ°çš„å‚æ•°ï¼‰\n",
    "    print(\"\\nâš™ï¸ æ­¥éª¤4: æµ‹è¯•é›†å¤„ç†\")\n",
    "    X_test_processed = processor.transform(X_test)\n",
    "    \n",
    "    # 5. å¤„ç†ç»“æœæ€»ç»“\n",
    "    print(\"\\nğŸ“‹ æµæ°´çº¿å¤„ç†æ€»ç»“:\")\n",
    "    print(f\"   åŸå§‹è®­ç»ƒé›†: {X_train.shape}\")\n",
    "    print(f\"   å¤„ç†åè®­ç»ƒé›†: {X_train_processed.shape}\")\n",
    "    print(f\"   åŸå§‹æµ‹è¯•é›†: {X_test.shape}\")\n",
    "    print(f\"   å¤„ç†åæµ‹è¯•é›†: {X_test_processed.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train_processed,\n",
    "        'X_test': X_test_processed,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'processor': processor\n",
    "    }\n",
    "\n",
    "# æ¼”ç¤ºå®Œæ•´æµæ°´çº¿\n",
    "if 'X_bc_clean' in locals():\n",
    "    pipeline_result = create_ml_pipeline(X_bc_clean, y_bc_clean)\n",
    "    \n",
    "    # éªŒè¯å¤„ç†æ•ˆæœ\n",
    "    X_train_p, X_test_p, y_train_p, y_test_p = (\n",
    "        pipeline_result['X_train'],\n",
    "        pipeline_result['X_test'],\n",
    "        pipeline_result['y_train'],\n",
    "        pipeline_result['y_test']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å¤„ç†æ•ˆæœéªŒè¯:\")\n",
    "    print(f\"   è®­ç»ƒé›†å‡å€¼èŒƒå›´: {np.min(np.mean(X_train_p, axis=0)):.4f} - {np.max(np.mean(X_train_p, axis=0)):.4f}\")\n",
    "    print(f\"   æµ‹è¯•é›†å‡å€¼èŒƒå›´: {np.min(np.mean(X_test_p, axis=0)):.4f} - {np.max(np.mean(X_test_p, axis=0)):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å®Œæ•´æµæ°´çº¿è¦ç‚¹æ€»ç»“ï¼š\")\n",
    "print(\"1. æ€»æ˜¯å…ˆåˆ†å‰²æ•°æ®å†è¿›è¡Œé¢„å¤„ç†\")\n",
    "print(\"2. åªåœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ é¢„å¤„ç†å‚æ•°\")\n",
    "print(\"3. ç”¨è®­ç»ƒé›†çš„å‚æ•°å¤„ç†æµ‹è¯•é›†\")\n",
    "print(\"4. ä¿æŒå¤„ç†çš„ä¸€è‡´æ€§ï¼Œé¿å…æ•°æ®æ³„éœ²\")\n",
    "print(\"5. æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ•°æ®å¤„ç†æµç¨‹æ€»ç»“\n",
    "\n",
    "### ğŸ“š å®Œæ•´æµç¨‹å›é¡¾ï¼š\n",
    "\n",
    "1. **æ•°æ®åŠ è½½** - é€‰æ‹©åˆé€‚çš„æ ¼å¼è¯»å–æ•°æ®\n",
    "2. **æ•°æ®æ¢ç´¢** - ç†è§£æ•°æ®ç‰¹å¾å’Œé—®é¢˜\n",
    "3. **æ•°æ®æ¸…æ´—** - å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼\n",
    "4. **æ•°æ®é¢„å¤„ç†** - ç‰¹å¾ç¼©æ”¾ã€ç‰¹å¾é€‰æ‹©ã€é™ç»´\n",
    "5. **æ•°æ®åˆ†å‰²** - è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ’åˆ†\n",
    "\n",
    "### ğŸ”¥ å…³é”®åŸåˆ™ï¼š\n",
    "\n",
    "#### 1. **æ•°æ®æ³„éœ²é˜²æŠ¤**\n",
    "```python\n",
    "# âŒ é”™è¯¯ï¼šå…ˆé¢„å¤„ç†å†åˆ†å‰²\n",
    "X_scaled = scaler.fit_transform(X)  # ç”¨äº†å…¨éƒ¨æ•°æ®ï¼\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# âœ… æ­£ç¡®ï¼šå…ˆåˆ†å‰²å†é¢„å¤„ç†\n",
    "X_train, X_test = train_test_split(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # åªç”¨è®­ç»ƒé›†ï¼\n",
    "X_test_scaled = scaler.transform(X_test)      # ç”¨è®­ç»ƒé›†å‚æ•°ï¼\n",
    "```\n",
    "\n",
    "#### 2. **ä¸€è‡´æ€§ä¿è¯**\n",
    "- è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†å‚æ•°\n",
    "- æ–°æ•°æ®ä½¿ç”¨è®­ç»ƒæ—¶å­¦åˆ°çš„å‚æ•°è¿›è¡Œå¤„ç†\n",
    "\n",
    "#### 3. **å¯é‡ç°æ€§**\n",
    "- è®¾ç½®éšæœºç§å­\n",
    "- è®°å½•æ‰€æœ‰å¤„ç†æ­¥éª¤\n",
    "- ä¿å­˜é¢„å¤„ç†å‚æ•°\n",
    "\n",
    "### ğŸ’¡ å®ç”¨å»ºè®®ï¼š\n",
    "\n",
    "#### 1. **é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ–¹æ³•**\n",
    "- **æ ‡å‡†åŒ–**: å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•\n",
    "- **å½’ä¸€åŒ–**: ç¥ç»ç½‘ç»œã€è·ç¦»æ•æ„Ÿç®—æ³•\n",
    "- **é²æ£’ç¼©æ”¾**: æœ‰å¼‚å¸¸å€¼çš„æ•°æ®\n",
    "\n",
    "#### 2. **å¤„ç†ç±»åˆ«ä¸å¹³è¡¡**\n",
    "- ä½¿ç”¨åˆ†å±‚åˆ†å‰²\n",
    "- è€ƒè™‘é‡é‡‡æ ·æˆ–æƒé‡è°ƒæ•´\n",
    "- ä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡\n",
    "\n",
    "#### 3. **è¯Šæ–­é—®é¢˜**\n",
    "- æ£€æŸ¥æ•°æ®åˆ†å¸ƒå˜åŒ–\n",
    "- éªŒè¯é¢„å¤„ç†æ•ˆæœ\n",
    "- ç›‘æ§è®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½å·®å¼‚\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š\n",
    "\n",
    "1. **å®è·µä¸åŒçš„æ•°æ®é›†** - å°è¯•å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®\n",
    "2. **è‡ªå®šä¹‰é¢„å¤„ç†** - æ ¹æ®ç‰¹å®šé—®é¢˜è®¾è®¡é¢„å¤„ç†æµç¨‹\n",
    "3. **è‡ªåŠ¨åŒ–æµæ°´çº¿** - ä½¿ç”¨Pipelineç±»æ„å»ºè‡ªåŠ¨åŒ–æµç¨‹\n",
    "4. **æ€§èƒ½ä¼˜åŒ–** - æ¯”è¾ƒä¸åŒé¢„å¤„ç†æ–¹æ³•çš„æ•ˆæœ\n",
    "\n",
    "### ğŸ‰ æ­å–œæ‚¨ï¼\n",
    "ç°åœ¨æ‚¨å·²ç»å®Œå…¨æŒæ¡äº†æ•°æ®å¤„ç†çš„å…¨æµç¨‹ï¼Œèƒ½å¤Ÿï¼š\n",
    "- ç†è§£æ¯ä¸€æ­¥çš„ç›®çš„å’Œæ–¹æ³•\n",
    "- è¯Šæ–­å’Œè§£å†³æ•°æ®é—®é¢˜\n",
    "- è®¾è®¡åˆé€‚çš„æ•°æ®å¤„ç†æµç¨‹\n",
    "- é¿å…å¸¸è§çš„æ•°æ®å¤„ç†é™·é˜±\n",
    "\n",
    "**è®°ä½**: å¥½çš„æ•°æ®å¤„ç†æ˜¯æˆåŠŸæœºå™¨å­¦ä¹ é¡¹ç›®çš„åŸºç¡€ï¼ ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
