{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ç®—æ³•æ·±åº¦æ¢ç´¢å®éªŒ\n",
    "\n",
    "## ğŸ“‹ å®éªŒç›®æ ‡\n",
    "è¿™ä¸ªnotebookä¸æ˜¯è®©æ‚¨è¿è¡Œç°æˆä»£ç ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ç³»åˆ—**æ¢ç´¢æ€§ä»»åŠ¡**å¸®åŠ©æ‚¨çœŸæ­£ç†è§£ç®—æ³•çš„æœ¬è´¨ã€‚\n",
    "\n",
    "### ğŸ¯ æ‚¨å°†å­¦åˆ°ï¼š\n",
    "1. **å‚æ•°çš„ä½œç”¨æœºåˆ¶** - ä¸ºä»€ä¹ˆè¦è¿™æ ·è°ƒå‚ï¼Ÿ\n",
    "2. **ç®—æ³•çš„é€‚ç”¨è¾¹ç•Œ** - ä»€ä¹ˆæ—¶å€™ç®—æ³•ä¼šå¤±æ•ˆï¼Ÿ\n",
    "3. **çœŸå®é—®é¢˜è§£å†³** - å¦‚ä½•åº”å¯¹å®é™…æŒ‘æˆ˜ï¼Ÿ\n",
    "4. **åˆ›æ–°æ€ç»´åŸ¹å…»** - å¦‚ä½•æ”¹è¿›å’Œä¼˜åŒ–ç®—æ³•ï¼Ÿ\n",
    "\n",
    "### âš ï¸ å­¦ä¹ åŸåˆ™ï¼š\n",
    "- **ä¸è¦åªè¿è¡Œä»£ç ** - å…ˆæ€è€ƒï¼Œå†éªŒè¯\n",
    "- **è®°å½•æ¯ä¸ªå‘ç°** - å»ºç«‹è‡ªå·±çš„çŸ¥è¯†ä½“ç³»\n",
    "- **æŒ‘æˆ˜ç°æœ‰ç»“è®º** - æå‡ºè‡ªå·±çš„å‡è®¾å¹¶éªŒè¯\n",
    "- **è”ç³»å®é™…åº”ç”¨** - æ€è€ƒè¿™äº›å‘ç°åœ¨å®é™…ä¸­çš„æ„ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ å®éªŒå‡†å¤‡\n",
    "\n",
    "è®©æˆ‘ä»¬å…ˆè®¾ç½®å¥½å®éªŒç¯å¢ƒï¼Œç„¶åå¼€å§‹çœŸæ­£çš„æ¢ç´¢ä¹‹æ—…ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "def load_data():\n",
    "    \"\"\"åŠ è½½ä¹³è…ºç™Œå’Œé¸¢å°¾èŠ±æ•°æ®é›†\"\"\"\n",
    "    # ä¹³è…ºç™Œæ•°æ®é›†\n",
    "    X_bc, y_bc = load_svmlight_file(\"../data/breast-cancer_scale\")\n",
    "    X_bc = X_bc.toarray()\n",
    "    y_bc = np.where(y_bc == 2, 0, 1)  # è½¬æ¢ä¸º0,1\n",
    "    \n",
    "    # é¸¢å°¾èŠ±æ•°æ®é›†\n",
    "    X_iris, y_iris = load_svmlight_file(\"../data/iris.scale\")\n",
    "    X_iris = X_iris.toarray()\n",
    "    y_iris = y_iris - 1  # è½¬æ¢ä¸º0,1,2\n",
    "    \n",
    "    return (X_bc, y_bc), (X_iris, y_iris)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "(X_bc, y_bc), (X_iris, y_iris) = load_data()\n",
    "\n",
    "# åˆ†å‰²æ•°æ®\n",
    "X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(\n",
    "    X_bc, y_bc, test_size=0.3, random_state=42, stratify=y_bc\n",
    ")\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "scaler_bc = StandardScaler()\n",
    "X_bc_train = scaler_bc.fit_transform(X_bc_train)\n",
    "X_bc_test = scaler_bc.transform(X_bc_test)\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_iris_train = scaler_iris.fit_transform(X_iris_train)\n",
    "X_iris_test = scaler_iris.transform(X_iris_test)\n",
    "\n",
    "print(\"âœ… å®éªŒç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n",
    "print(f\"ä¹³è…ºç™Œæ•°æ®é›†ï¼šè®­ç»ƒ{X_bc_train.shape}ï¼Œæµ‹è¯•{X_bc_test.shape}\")\n",
    "print(f\"é¸¢å°¾èŠ±æ•°æ®é›†ï¼šè®­ç»ƒ{X_iris_train.shape}ï¼Œæµ‹è¯•{X_iris_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ¢ç´¢ä»»åŠ¡1ï¼šå‚æ•°çš„æ·±åº¦ç†è§£\n",
    "\n",
    "### ğŸ¤” æ€è€ƒé—®é¢˜ï¼š\n",
    "1. **Cå‚æ•°çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ** å®ƒæ§åˆ¶æ¨¡å‹çš„ä»€ä¹ˆç‰¹æ€§ï¼Ÿ\n",
    "2. **gammaå‚æ•°å¦‚ä½•å½±å“å†³ç­–è¾¹ç•Œï¼Ÿ** ä¸ºä»€ä¹ˆå®ƒå¯¹RBFæ ¸å¦‚æ­¤é‡è¦ï¼Ÿ\n",
    "3. **å‚æ•°ä¹‹é—´æ˜¯å¦å­˜åœ¨äº¤äº’æ•ˆåº”ï¼Ÿ** æŸäº›å‚æ•°ç»„åˆæ˜¯å¦ä¼šç›¸äº’å¢å¼ºæˆ–æŠµæ¶ˆï¼Ÿ\n",
    "\n",
    "### ğŸ”¬ å®éªŒä»»åŠ¡ï¼š\n",
    "å®Œæˆä¸‹é¢çš„ä»£ç ï¼Œè§‚å¯Ÿå‚æ•°å˜åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œ**å¹¶è®°å½•æ‚¨çš„å‘ç°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡1.1: Cå‚æ•°çš„æ·±åº¦æ¢ç´¢\n",
    "# ğŸ¯ ç›®æ ‡ï¼šç†è§£Cå‚æ•°å¦‚ä½•æ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦å’Œè¿‡æ‹Ÿåˆ\n",
    "\n",
    "def explore_c_parameter(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"æ¢ç´¢Cå‚æ•°å¯¹SVMæ€§èƒ½çš„å½±å“\"\"\"\n",
    "    \n",
    "    # TODO: å®šä¹‰æ›´ç»†ç²’åº¦çš„Cå€¼èŒƒå›´\n",
    "    c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]  # å¯ä»¥è°ƒæ•´è¿™ä¸ªèŒƒå›´\n",
    "    \n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨æ¢ç´¢{dataset_name}ä¸ŠCå‚æ•°çš„å½±å“...\")\n",
    "    \n",
    "    for c in c_values:\n",
    "        # TODO: è®­ç»ƒRBF SVMï¼Œè®°å½•è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡\n",
    "        svm = SVC(C=c, kernel='rbf', gamma='scale', random_state=42)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, svm.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test, svm.predict(X_test))\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"C={c:.3f}: è®­ç»ƒå‡†ç¡®ç‡={train_acc:.4f}, æµ‹è¯•å‡†ç¡®ç‡={test_acc:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶ç»“æœ\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(c_values, train_accs, 'bo-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, markersize=8)\n",
    "    plt.semilogx(c_values, test_accs, 'ro-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Cå‚æ•°å€¼ (å¯¹æ•°å°ºåº¦)')\n",
    "    plt.ylabel('å‡†ç¡®ç‡')\n",
    "    plt.title(f'{dataset_name}: Cå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO: åˆ†æå¹¶è®°å½•æ‚¨çš„å‘ç°\n",
    "    print(\"\\nğŸ’¡ åˆ†æè¦ç‚¹ï¼š\")\n",
    "    print(\"1. ä»€ä¹ˆæ—¶å€™å‡ºç°æ¬ æ‹Ÿåˆï¼Ÿï¼ˆè®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡éƒ½ä½ï¼‰\")\n",
    "    print(\"2. ä»€ä¹ˆæ—¶å€™å‡ºç°æœ€ä½³æ€§èƒ½ï¼Ÿï¼ˆæµ‹è¯•å‡†ç¡®ç‡æœ€é«˜ï¼‰\")\n",
    "    print(\"3. ä»€ä¹ˆæ—¶å€™å‡ºç°è¿‡æ‹Ÿåˆï¼Ÿï¼ˆè®­ç»ƒå‡†ç¡®ç‡é«˜ï¼Œæµ‹è¯•å‡†ç¡®ç‡ä½ï¼‰\")\n",
    "    print(\"4. æœ€ä¼˜çš„Cå€¼èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "    \n",
    "    return c_values, train_accs, test_accs\n",
    "\n",
    "# åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šæ¢ç´¢Cå‚æ•°\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡1.1: Cå‚æ•°çš„æ·±åº¦ç†è§£\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä¹³è…ºç™Œæ•°æ®é›†\n",
    "c_vals_bc, train_accs_bc, test_accs_bc = explore_c_parameter(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\"\n",
    ")\n",
    "\n",
    "# é¸¢å°¾èŠ±æ•°æ®é›†\n",
    "c_vals_iris, train_accs_iris, test_accs_iris = explore_c_parameter(\n",
    "    X_iris_train, X_iris_test, y_iris_train, y_iris_test, \"é¸¢å°¾èŠ±æ•°æ®é›†\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡1.2: Gammaå‚æ•°çš„éçº¿æ€§æ•ˆåº”\n",
    "# ğŸ¯ ç›®æ ‡ï¼šç†è§£gammaå¦‚ä½•æ§åˆ¶RBFæ ¸çš„å½±å“èŒƒå›´\n",
    "\n",
    "def explore_gamma_parameter(X_train, X_test, y_train, y_test, dataset_name, best_c=1):\n",
    "    \"\"\"æ¢ç´¢gammaå‚æ•°å¯¹SVMæ€§èƒ½çš„å½±å“\"\"\"\n",
    "    \n",
    "    # TODO: å®šä¹‰gammaå€¼èŒƒå›´\n",
    "    gamma_values = ['auto', 'scale', 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨æ¢ç´¢{dataset_name}ä¸Šgammaå‚æ•°çš„å½±å“ (C={best_c})...\")\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        try:\n",
    "            # TODO: è®­ç»ƒSVMï¼Œå¤„ç†ä¸åŒgammaå€¼\n",
    "            svm = SVC(C=best_c, kernel='rbf', gamma=gamma, random_state=42)\n",
    "            svm.fit(X_train, y_train)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, svm.predict(X_train))\n",
    "            test_acc = accuracy_score(y_test, svm.predict(X_test))\n",
    "            \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print(f\"gamma={gamma}: è®­ç»ƒå‡†ç¡®ç‡={train_acc:.4f}, æµ‹è¯•å‡†ç¡®ç‡={test_acc:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"gamma={gamma}: å‡ºç°é”™è¯¯ - {e}\")\n",
    "            train_accs.append(0)\n",
    "            test_accs.append(0)\n",
    "    \n",
    "    # ç»˜åˆ¶ç»“æœï¼ˆè·³è¿‡'auto'å’Œ'scale'ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å­—ç¬¦ä¸²ï¼‰\n",
    "    numeric_gamma = [g for g in gamma_values if isinstance(g, (int, float))]\n",
    "    if numeric_gamma:\n",
    "        numeric_train_accs = [train_accs[i] for i, g in enumerate(gamma_values) if g in numeric_gamma]\n",
    "        numeric_test_accs = [test_accs[i] for i, g in enumerate(gamma_values) if g in numeric_gamma]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.semilogx(numeric_gamma, numeric_train_accs, 'bo-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, markersize=8)\n",
    "        plt.semilogx(numeric_gamma, numeric_test_accs, 'ro-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Gammaå‚æ•°å€¼ (å¯¹æ•°å°ºåº¦)')\n",
    "        plt.ylabel('å‡†ç¡®ç‡')\n",
    "        plt.title(f'{dataset_name}: Gammaå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ åˆ†æè¦ç‚¹ï¼š\")\n",
    "    print(\"1. å¤ªå°çš„gammaå€¼ä¼šå¯¼è‡´ä»€ä¹ˆï¼Ÿ\")\n",
    "    print(\"2. å¤ªå¤§çš„gammaå€¼ä¼šå¯¼è‡´ä»€ä¹ˆï¼Ÿ\")\n",
    "    print(\"3. 'scale'å’Œ'auto'çš„è¡¨ç°å¦‚ä½•ï¼Ÿ\")\n",
    "    print(\"4. æœ€ä¼˜çš„gammaèŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "    \n",
    "    return gamma_values, train_accs, test_accs\n",
    "\n",
    "# TODO: ä½¿ç”¨åˆšæ‰æ‰¾åˆ°çš„æœ€ä½³Cå€¼\n",
    "best_c_bc = 1  # æ ¹æ®ä¸Šä¸€æ­¥çš„ç»“æœè°ƒæ•´\n",
    "best_c_iris = 1  # æ ¹æ®ä¸Šä¸€æ­¥çš„ç»“æœè°ƒæ•´\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡1.2: Gammaå‚æ•°çš„éçº¿æ€§æ•ˆåº”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šæ¢ç´¢gamma\n",
    "gamma_vals_bc, train_accs_bc_gamma, test_accs_bc_gamma = explore_gamma_parameter(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\", best_c_bc\n",
    ")\n",
    "\n",
    "# åœ¨é¸¢å°¾èŠ±æ•°æ®é›†ä¸Šæ¢ç´¢gamma\n",
    "gamma_vals_iris, train_accs_iris_gamma, test_accs_iris_gamma = explore_gamma_parameter(\n",
    "    X_iris_train, X_iris_test, y_iris_train, y_iris_test, \"é¸¢å°¾èŠ±æ•°æ®é›†\", best_c_iris\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡1.3: å‚æ•°äº¤äº’æ•ˆåº”åˆ†æ\n",
    "# ğŸ¯ ç›®æ ‡ï¼šå‘ç°Cå’Œgammaå‚æ•°ä¹‹é—´çš„äº¤äº’ä½œç”¨\n",
    "\n",
    "def analyze_parameter_interaction(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"åˆ†æCå’Œgammaå‚æ•°çš„äº¤äº’æ•ˆåº”\"\"\"\n",
    "    \n",
    "    # TODO: å®šä¹‰å‚æ•°ç½‘æ ¼\n",
    "    c_values = [0.1, 1, 10, 100]\n",
    "    gamma_values = [0.01, 0.1, 1, 10]\n",
    "    \n",
    "    # åˆ›å»ºç»“æœçŸ©é˜µ\n",
    "    results_matrix = np.zeros((len(c_values), len(gamma_values)))\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨åˆ†æ{dataset_name}ä¸Šå‚æ•°çš„äº¤äº’æ•ˆåº”...\")\n",
    "    \n",
    "    # ç½‘æ ¼æœç´¢\n",
    "    for i, c in enumerate(c_values):\n",
    "        for j, gamma in enumerate(gamma_values):\n",
    "            svm = SVC(C=c, kernel='rbf', gamma=gamma, random_state=42)\n",
    "            svm.fit(X_train, y_train)\n",
    "            acc = accuracy_score(y_test, svm.predict(X_test))\n",
    "            results_matrix[i, j] = acc\n",
    "            print(f\"C={c}, gamma={gamma}: å‡†ç¡®ç‡={acc:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(results_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                xticklabels=gamma_values, yticklabels=c_values)\n",
    "    plt.xlabel('Gammaå€¼')\n",
    "    plt.ylabel('Cå€¼')\n",
    "    plt.title(f'{dataset_name}: Cå’ŒGammaå‚æ•°äº¤äº’æ•ˆåº”çƒ­åŠ›å›¾')\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ\n",
    "    best_i, best_j = np.unravel_index(np.argmax(results_matrix), results_matrix.shape)\n",
    "    best_c = c_values[best_i]\n",
    "    best_gamma = gamma_values[best_j]\n",
    "    best_acc = results_matrix[best_i, best_j]\n",
    "    \n",
    "    print(f\"\\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ: C={best_c}, gamma={best_gamma}\")\n",
    "    print(f\"ğŸ“ˆ æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}\")\n",
    "    \n",
    "    # TODO: åˆ†æäº¤äº’æ•ˆåº”\n",
    "    print(\"\\nğŸ’¡ äº¤äº’æ•ˆåº”åˆ†æï¼š\")\n",
    "    print(\"1. æ˜¯å¦å­˜åœ¨æŸäº›å‚æ•°ç»„åˆç‰¹åˆ«å·®ï¼Ÿï¼ˆå‚æ•°é™·é˜±ï¼‰\")\n",
    "    print(\"2. Cå’Œgammaæ˜¯å¦å­˜åœ¨ç›¸äº’è¡¥å¿æ•ˆåº”ï¼Ÿ\")\n",
    "    print(\"3. ä¸åŒCå€¼ä¸‹ï¼Œæœ€ä¼˜gammaå€¼å¦‚ä½•å˜åŒ–ï¼Ÿ\")\n",
    "    \n",
    "    return results_matrix, (best_c, best_gamma, best_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡1.3: å‚æ•°äº¤äº’æ•ˆåº”åˆ†æ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ†æä¹³è…ºç™Œæ•°æ®é›†\n",
    "matrix_bc, best_params_bc = analyze_parameter_interaction(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\"\n",
    ")\n",
    "\n",
    "# åˆ†æé¸¢å°¾èŠ±æ•°æ®é›†\n",
    "matrix_iris, best_params_iris = analyze_parameter_interaction(\n",
    "    X_iris_train, X_iris_test, y_iris_train, y_iris_test, \"é¸¢å°¾èŠ±æ•°æ®é›†\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ¢ç´¢ä»»åŠ¡2ï¼šç®—æ³•çš„é€‚ç”¨è¾¹ç•Œ\n",
    "\n",
    "### ğŸ¤” æ€è€ƒé—®é¢˜ï¼š\n",
    "1. **ç®—æ³•åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹ä¼šå¤±æ•ˆï¼Ÿ**\n",
    "2. **æ•°æ®è´¨é‡å¦‚ä½•å½±å“ç®—æ³•æ€§èƒ½ï¼Ÿ**\n",
    "3. **ä¸åŒç®—æ³•å¯¹æ•°æ®ç‰¹æ€§çš„æ•æ„Ÿåº¦å¦‚ä½•ï¼Ÿ**\n",
    "\n",
    "### ğŸ”¬ å®éªŒä»»åŠ¡ï¼š\n",
    "é€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„é—®é¢˜åœºæ™¯ï¼Œæµ‹è¯•ç®—æ³•çš„é²æ£’æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡2.1: æ•°æ®é‡å¯¹æ€§èƒ½çš„å½±å“\n",
    "# ğŸ¯ ç›®æ ‡ï¼šäº†è§£ç®—æ³•åœ¨æ•°æ®ç¨€å°‘æ—¶çš„è¡¨ç°\n",
    "\n",
    "def test_data_sufficiency(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„ç®—æ³•æ€§èƒ½\"\"\"\n",
    "    \n",
    "    # TODO: å®šä¹‰ä¸åŒçš„è®­ç»ƒæ•°æ®æ¯”ä¾‹\n",
    "    data_ratios = [1.0, 0.8, 0.6, 0.4, 0.2, 0.1]  # 100%åˆ°10%\n",
    "    \n",
    "    algorithms = {\n",
    "        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'çº¿æ€§SVM': SVC(C=1, kernel='linear', random_state=42),\n",
    "        'RBF_SVM': SVC(C=1, kernel='rbf', gamma='scale', random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {alg: [] for alg in algorithms.keys()}\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨æµ‹è¯•{dataset_name}ä¸Šæ•°æ®é‡çš„å½±å“...\")\n",
    "    \n",
    "    for ratio in data_ratios:\n",
    "        # éšæœºé‡‡æ ·æŒ‡å®šæ¯”ä¾‹çš„æ•°æ®\n",
    "        n_samples = int(len(X_train) * ratio)\n",
    "        indices = np.random.choice(len(X_train), n_samples, replace=False)\n",
    "        X_subset = X_train[indices]\n",
    "        y_subset = y_train[indices]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ä½¿ç”¨{ratio*100:.0f}%çš„æ•°æ® ({n_samples}ä¸ªæ ·æœ¬):\")\n",
    "        \n",
    "        for alg_name, alg in algorithms.items():\n",
    "            alg.fit(X_subset, y_subset)\n",
    "            acc = accuracy_score(y_test, alg.predict(X_test))\n",
    "            results[alg_name].append(acc)\n",
    "            print(f\"  {alg_name}: {acc:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶ç»“æœ\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for alg_name, accs in results.items():\n",
    "        plt.plot([r*100 for r in data_ratios], accs, 'o-', label=alg_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('è®­ç»ƒæ•°æ®æ¯”ä¾‹ (%)')\n",
    "    plt.ylabel('æµ‹è¯•å‡†ç¡®ç‡')\n",
    "    plt.title(f'{dataset_name}: æ•°æ®é‡å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO: åˆ†æç»“æœ\n",
    "    print(\"\\nğŸ’¡ æ•°æ®é‡å½±å“åˆ†æï¼š\")\n",
    "    print(\"1. å“ªä¸ªç®—æ³•åœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½ï¼Ÿ\")\n",
    "    print(\"2. å“ªä¸ªç®—æ³•å¯¹æ•°æ®é‡æœ€æ•æ„Ÿï¼Ÿ\")\n",
    "    print(\"3. åœ¨å¤šå°‘æ•°æ®é‡æ—¶æ€§èƒ½è¶‹äºç¨³å®šï¼Ÿ\")\n",
    "    print(\"4. è¿™å¯¹å®é™…åº”ç”¨æœ‰ä»€ä¹ˆå¯ç¤ºï¼Ÿ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡2.1: æ•°æ®é‡å¯¹æ€§èƒ½çš„å½±å“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šæµ‹è¯•\n",
    "data_sufficiency_bc = test_data_sufficiency(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\"\n",
    ")\n",
    "\n",
    "# åœ¨é¸¢å°¾èŠ±æ•°æ®é›†ä¸Šæµ‹è¯•\n",
    "data_sufficiency_iris = test_data_sufficiency(\n",
    "    X_iris_train, X_iris_test, y_iris_train, y_iris_test, \"é¸¢å°¾èŠ±æ•°æ®é›†\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡2.2: å™ªå£°æ•æ„Ÿæ€§æµ‹è¯•\n",
    "# ğŸ¯ ç›®æ ‡ï¼šäº†è§£ç®—æ³•å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§\n",
    "\n",
    "def add_noise(X, noise_level=0.1, noise_type='gaussian'):\n",
    "    \"\"\"å‘æ•°æ®æ·»åŠ å™ªå£°\"\"\"\n",
    "    if noise_type == 'gaussian':\n",
    "        noise = np.random.normal(0, noise_level, X.shape)\n",
    "    elif noise_type == 'uniform':\n",
    "        noise = np.random.uniform(-noise_level, noise_level, X.shape)\n",
    "    return X + noise\n",
    "\n",
    "def test_noise_robustness(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"æµ‹è¯•ç®—æ³•å¯¹å™ªå£°çš„é²æ£’æ€§\"\"\"\n",
    "    \n",
    "    # TODO: å®šä¹‰ä¸åŒçš„å™ªå£°æ°´å¹³\n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    \n",
    "    algorithms = {\n",
    "        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'çº¿æ€§SVM': SVC(C=1, kernel='linear', random_state=42),\n",
    "        'RBF_SVM': SVC(C=1, kernel='rbf', gamma='scale', random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {alg: [] for alg in algorithms.keys()}\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨æµ‹è¯•{dataset_name}ä¸Šå™ªå£°é²æ£’æ€§...\")\n",
    "    \n",
    "    for noise_level in noise_levels:\n",
    "        # å‘è®­ç»ƒæ•°æ®æ·»åŠ å™ªå£°\n",
    "        X_train_noisy = add_noise(X_train, noise_level)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š å™ªå£°æ°´å¹³ {noise_level:.1f}:\")\n",
    "        \n",
    "        for alg_name, alg in algorithms.items():\n",
    "            alg.fit(X_train_noisy, y_train)\n",
    "            acc = accuracy_score(y_test, alg.predict(X_test))\n",
    "            results[alg_name].append(acc)\n",
    "            print(f\"  {alg_name}: {acc:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶ç»“æœ\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for alg_name, accs in results.items():\n",
    "        plt.plot(noise_levels, accs, 'o-', label=alg_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('å™ªå£°æ°´å¹³')\n",
    "    plt.ylabel('æµ‹è¯•å‡†ç¡®ç‡')\n",
    "    plt.title(f'{dataset_name}: å™ªå£°å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # è®¡ç®—æ€§èƒ½ä¸‹é™ç‡\n",
    "    print(\"\\nğŸ’¡ å™ªå£°æ•æ„Ÿæ€§åˆ†æï¼š\")\n",
    "    for alg_name, accs in results.items():\n",
    "        original_acc = accs[0]\n",
    "        final_acc = accs[-1]\n",
    "        drop_rate = (original_acc - final_acc) / original_acc * 100\n",
    "        print(f\"{alg_name}: æ€§èƒ½ä¸‹é™ {drop_rate:.1f}%\")\n",
    "    \n",
    "    print(\"\\næ€è€ƒé—®é¢˜ï¼š\")\n",
    "    print(\"1. å“ªä¸ªç®—æ³•å¯¹å™ªå£°æœ€é²æ£’ï¼Ÿ\")\n",
    "    print(\"2. ä¸ºä»€ä¹ˆæŸäº›ç®—æ³•å¯¹å™ªå£°æ›´æ•æ„Ÿï¼Ÿ\")\n",
    "    print(\"3. åœ¨å®é™…åº”ç”¨ä¸­å¦‚ä½•åº”å¯¹å™ªå£°æ•°æ®ï¼Ÿ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡2.2: å™ªå£°æ•æ„Ÿæ€§æµ‹è¯•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šæµ‹è¯•\n",
    "noise_robustness_bc = test_noise_robustness(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\"\n",
    ")\n",
    "\n",
    "# åœ¨é¸¢å°¾èŠ±æ•°æ®é›†ä¸Šæµ‹è¯•\n",
    "noise_robustness_iris = test_noise_robustness(\n",
    "    X_iris_train, X_iris_test, y_iris_train, y_iris_test, \"é¸¢å°¾èŠ±æ•°æ®é›†\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ¢ç´¢ä»»åŠ¡3ï¼šçœŸå®é—®é¢˜æŒ‘æˆ˜\n",
    "\n",
    "### ğŸ¤” æ€è€ƒé—®é¢˜ï¼š\n",
    "1. **å½“ç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**\n",
    "2. **é«˜ç»´æ•°æ®ä¼šå¸¦æ¥ä»€ä¹ˆæŒ‘æˆ˜ï¼Ÿ**\n",
    "3. **å¦‚ä½•è¯„ä¼°ç®—æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ï¼Ÿ**\n",
    "\n",
    "### ğŸ”¬ å®éªŒä»»åŠ¡ï¼š\n",
    "æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„å¤æ‚æƒ…å†µï¼Œæµ‹è¯•ç®—æ³•çš„é€‚åº”æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ä»»åŠ¡3.1: ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜\n",
    "# ğŸ¯ ç›®æ ‡ï¼šäº†è§£ç®—æ³•åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šçš„è¡¨ç°å’Œè§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "def create_imbalanced_dataset(X, y, minority_ratio=0.1):\n",
    "    \"\"\"åˆ›å»ºä¸å¹³è¡¡æ•°æ®é›†\"\"\"\n",
    "    # å‡è®¾æˆ‘ä»¬è®©ç±»åˆ«0ä¸ºå¤šæ•°ç±»ï¼Œç±»åˆ«1ä¸ºå°‘æ•°ç±»\n",
    "    majority_mask = y == 0\n",
    "    minority_mask = y == 1\n",
    "    \n",
    "    # è®¡ç®—éœ€è¦çš„å°‘æ•°ç±»æ ·æœ¬æ•°\n",
    "    n_minority = int(len(y[majority_mask]) * minority_ratio / (1 - minority_ratio))\n",
    "    \n",
    "    # éšæœºé€‰æ‹©å°‘æ•°ç±»æ ·æœ¬\n",
    "    minority_indices = np.where(minority_mask)[0]\n",
    "    selected_minority = np.random.choice(minority_indices, \n",
    "                                        min(n_minority, len(minority_indices)), \n",
    "                                        replace=False)\n",
    "    \n",
    "    # ç»„åˆæ•°æ®\n",
    "    majority_indices = np.where(majority_mask)[0]\n",
    "    selected_indices = np.concatenate([majority_indices, selected_minority])\n",
    "    \n",
    "    X_imbalanced = X[selected_indices]\n",
    "    y_imbalanced = y[selected_indices]\n",
    "    \n",
    "    return X_imbalanced, y_imbalanced\n",
    "\n",
    "def test_imbalanced_learning(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"æµ‹è¯•ç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„å­¦ä¹ æ€§èƒ½\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºä¸åŒä¸å¹³è¡¡ç¨‹åº¦çš„æ•°æ®é›†\n",
    "    imbalance_ratios = [0.5, 0.3, 0.2, 0.1, 0.05]  # å°‘æ•°ç±»æ¯”ä¾‹\n",
    "    \n",
    "    algorithms = {\n",
    "        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'çº¿æ€§SVM': SVC(C=1, kernel='linear', random_state=42),\n",
    "        'RBF_SVM': SVC(C=1, kernel='rbf', gamma='scale', random_state=42)\n",
    "    }\n",
    "    \n",
    "    # åªåœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¿›è¡Œï¼ˆå› ä¸ºå®ƒæ˜¯äºŒåˆ†ç±»ï¼‰\n",
    "    if len(np.unique(y_train)) != 2:\n",
    "        print(f\"è·³è¿‡ {dataset_name}ï¼ˆéäºŒåˆ†ç±»æ•°æ®é›†ï¼‰\")\n",
    "        return None\n",
    "    \n",
    "    results = {ratio: {} for ratio in imbalance_ratios}\n",
    "    \n",
    "    print(f\"ğŸ” æ­£åœ¨æµ‹è¯•{dataset_name}ä¸Šç±»åˆ«ä¸å¹³è¡¡é—®é¢˜...\")\n",
    "    \n",
    "    original_acc = {}\n",
    "    \n",
    "    for ratio in imbalance_ratios:\n",
    "        # åˆ›å»ºä¸å¹³è¡¡æ•°æ®é›†\n",
    "        X_train_imb, y_train_imb = create_imbalanced_dataset(X_train, y_train, ratio)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š å°‘æ•°ç±»æ¯”ä¾‹ {ratio:.2f} (æ€»æ ·æœ¬: {len(X_train_imb)})\")\n",
    "        unique, counts = np.unique(y_train_imb, return_counts=True)\n",
    "        for cls, cnt in zip(unique, counts):\n",
    "            print(f\"  ç±»åˆ« {cls}: {cnt} æ ·æœ¬\")\n",
    "        \n",
    "        for alg_name, alg in algorithms.items():\n",
    "            alg.fit(X_train_imb, y_train_imb)\n",
    "            y_pred = alg.predict(X_test)\n",
    "            \n",
    "            # è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡\n",
    "            from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            results[ratio][alg_name] = {\n",
    "                'accuracy': acc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "            \n",
    "            print(f\"  {alg_name}: ACC={acc:.3f}, P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶ç»“æœ\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    metric_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']\n",
    "    \n",
    "    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "        ax = axes[idx//2, idx%2]\n",
    "        for alg_name in algorithms.keys():\n",
    "            values = [results[ratio][alg_name][metric] for ratio in imbalance_ratios]\n",
    "            ax.plot(imbalance_ratios, values, 'o-', label=alg_name, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax.set_xlabel('å°‘æ•°ç±»æ¯”ä¾‹')\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.set_title(f'{dataset_name}: ç±»åˆ«ä¸å¹³è¡¡å¯¹{metric_name}çš„å½±å“')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ç±»åˆ«ä¸å¹³è¡¡åˆ†æï¼š\")\n",
    "    print(\"1. å“ªä¸ªæŒ‡æ ‡å—ç±»åˆ«ä¸å¹³è¡¡å½±å“æœ€å¤§ï¼Ÿ\")\n",
    "    print(\"2. å“ªä¸ªç®—æ³•å¯¹ç±»åˆ«ä¸å¹³è¡¡æœ€é²æ£’ï¼Ÿ\")\n",
    "    print(\"3. ä¸ºä»€ä¹ˆå‡†ç¡®ç‡å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ï¼Ÿ\")\n",
    "    print(\"4. åœ¨å®é™…åº”ç”¨ä¸­å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Ÿ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ æ¢ç´¢ä»»åŠ¡3.1: ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åªåœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šæµ‹è¯•ï¼ˆäºŒåˆ†ç±»ï¼‰\n",
    "imbalance_results = test_imbalanced_learning(\n",
    "    X_bc_train, X_bc_test, y_bc_train, y_bc_test, \"ä¹³è…ºç™Œæ•°æ®é›†\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å®éªŒæ€»ç»“ä¸åæ€\n",
    "\n",
    "### ğŸ¯ æ‚¨çš„å‘ç°è®°å½•ï¼š\n",
    "\n",
    "é€šè¿‡ä»¥ä¸Šæ¢ç´¢å®éªŒï¼Œè¯·è®°å½•æ‚¨çš„å…³é”®å‘ç°ï¼š\n",
    "\n",
    "**1. å‚æ•°ç†è§£æ–¹é¢ï¼š**\n",
    "- Cå‚æ•°çš„æœ¬è´¨ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "- Gammaå‚æ•°å¦‚ä½•å½±å“æ¨¡å‹å¤æ‚åº¦ï¼Ÿ\n",
    "- å‚æ•°ä¹‹é—´å­˜åœ¨ä»€ä¹ˆæ ·çš„äº¤äº’æ•ˆåº”ï¼Ÿ\n",
    "\n",
    "**2. ç®—æ³•ç‰¹æ€§æ–¹é¢ï¼š**\n",
    "- å“ªäº›ç®—æ³•é€‚åˆå°æ ·æœ¬å­¦ä¹ ï¼Ÿ\n",
    "- å“ªäº›ç®—æ³•å¯¹å™ªå£°æ›´é²æ£’ï¼Ÿ\n",
    "- ç®—æ³•çš„é€‚ç”¨è¾¹ç•Œæ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**3. å®é™…åº”ç”¨æ–¹é¢ï¼š**\n",
    "- å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç®—æ³•ï¼Ÿ\n",
    "- å¦‚ä½•è¯„ä¼°ç®—æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ï¼Ÿ\n",
    "- é¢å¯¹æ•°æ®è´¨é‡é—®é¢˜è¯¥å¦‚ä½•åº”å¯¹ï¼Ÿ\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥æ¢ç´¢æ–¹å‘ï¼š\n",
    "1. **ç‰¹å¾å·¥ç¨‹å®éªŒ** - å°è¯•ä¸åŒçš„ç‰¹å¾å¤„ç†æ–¹æ³•\n",
    "2. **é›†æˆå­¦ä¹ å®éªŒ** - ç»“åˆå¤šä¸ªç®—æ³•çš„é¢„æµ‹ç»“æœ\n",
    "3. **è‡ªå®šä¹‰æŸå¤±å‡½æ•°** - é’ˆå¯¹ç‰¹å®šé—®é¢˜ä¼˜åŒ–ç›®æ ‡å‡½æ•°\n",
    "4. **å®é™…æ•°æ®é›†åº”ç”¨** - åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯æ‚¨çš„å‘ç°\n",
    "\n",
    "### ğŸ’¡ æ·±åº¦å­¦ä¹ å»ºè®®ï¼š\n",
    "- **ä¿æŒå¥½å¥‡å¿ƒ** - æ€»æ˜¯é—®\"ä¸ºä»€ä¹ˆ\"å’Œ\"æ€ä¹ˆæ ·\"\n",
    "- **è®°å½•å®éªŒè¿‡ç¨‹** - å»ºç«‹è‡ªå·±çš„çŸ¥è¯†åº“\n",
    "- **æŒ‘æˆ˜ç°æœ‰ç»“è®º** - æå‡ºå‡è®¾å¹¶éªŒè¯\n",
    "- **è”ç³»å®é™…åº”ç”¨** - æ€è€ƒç†è®ºåœ¨å®è·µä¸­çš„æ„ä¹‰\n",
    "\n",
    "**è®°ä½**ï¼šçœŸæ­£çš„å­¦ä¹ ä¸æ˜¯å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯é€šè¿‡å®éªŒè·å¾—å¯¹ç®—æ³•æœ¬è´¨çš„æ·±åˆ»ç†è§£ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}