{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“Š æ•°æ®å¤„ç†æµç¨‹ - äº¤äº’å¼å®è·µæ•™ç¨‹\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "è¿™ä¸ªæ•™ç¨‹éœ€è¦**æ‚¨ä¸»åŠ¨å‚ä¸**ï¼æ¯ä¸ªä»£ç å•å…ƒæ ¼éƒ½æœ‰ç©ºç™½éœ€è¦æ‚¨å¡«å†™ã€‚\n",
    "\n",
    "### ğŸ’¡ ä½¿ç”¨è¯´æ˜\n",
    "1. ğŸ” **è§‚å¯Ÿ** - æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æç¤º\n",
    "2. ğŸ’­ **æ€è€ƒ** - æƒ³æƒ³ä¸ºä»€ä¹ˆè¿™æ ·åš\n",
    "3. âœï¸ **åŠ¨æ‰‹** - å¡«å†™ä»£ç ç©ºç™½å¤„\n",
    "4. âœ… **éªŒè¯** - è¿è¡Œä»£ç æ£€æŸ¥ç»“æœ\n",
    "5. ğŸ”„ **åæ€** - æ€»ç»“å­¦åˆ°çš„ä¸œè¥¿\n",
    "\n",
    "### âš ï¸ é‡è¦æé†’\n",
    "**ä¸è¦ç›´æ¥å¤åˆ¶ç²˜è´´ç­”æ¡ˆï¼** å°è¯•è‡ªå·±æ€è€ƒï¼Œå³ä½¿é”™äº†ä¹Ÿæ²¡å…³ç³»ã€‚é”™è¯¯æ˜¯æœ€å¥½çš„å­¦ä¹ æœºä¼šï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ“¥ æ­¥éª¤1: æ•°æ®åŠ è½½\n",
    "\n",
    "### ğŸ¤” ä»»åŠ¡è¯´æ˜\n",
    "æ‚¨éœ€è¦åŠ è½½ä¸¤ä¸ªæ•°æ®é›†ï¼š\n",
    "- ä¹³è…ºç™Œæ•°æ®é›† (äºŒåˆ†ç±»)\n",
    "- é¸¢å°¾èŠ±æ•°æ®é›† (å¤šåˆ†ç±»)\n",
    "\n",
    "### ğŸ’¡ æç¤º\n",
    "- æ•°æ®ä½äº `../data/` ç›®å½•ä¸‹\n",
    "- ä¹³è…ºç™Œæ•°æ®ï¼š`breast-cancer_scale`\n",
    "- é¸¢å°¾èŠ±æ•°æ®ï¼š`iris.scale`\n",
    "- ä½¿ç”¨ `load_svmlight_file` å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ğŸ“š æ­¥éª¤1: æ•°æ®åŠ è½½\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# TODO: åœ¨è¿™é‡Œå¡«å†™æ‚¨çš„ä»£ç \n",
    "# æç¤ºï¼šä½¿ç”¨ load_svmlight_file åŠ è½½æ•°æ®\n",
    "\n",
    "# 1. åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†\n",
    "try:\n",
    "    # åœ¨ä¸‹é¢å¡«å†™åŠ è½½ä¹³è…ºç™Œæ•°æ®çš„ä»£ç \n",
    "    X_bc, y_bc =  # ğŸ‘ˆ åœ¨è¿™é‡Œå¡«å†™ä»£ç \n",
    "    print(f\"âœ… ä¹³è…ºç™Œæ•°æ®åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_bc.shape}\")\n",
    "    print(f\"   æ ‡ç­¾å‘é‡å½¢çŠ¶: {y_bc.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ä¹³è…ºç™Œæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„\")\n",
    "\n",
    "# 2. åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†\n",
    "try:\n",
    "    # åœ¨ä¸‹é¢å¡«å†™åŠ è½½é¸¢å°¾èŠ±æ•°æ®çš„ä»£ç \n",
    "    X_iris, y_iris =  # ğŸ‘ˆ åœ¨è¿™é‡Œå¡«å†™ä»£ç \n",
    "    print(f\"\\nâœ… é¸¢å°¾èŠ±æ•°æ®åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_iris.shape}\")\n",
    "    print(f\"   æ ‡ç­¾å‘é‡å½¢çŠ¶: {y_iris.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ é¸¢å°¾èŠ±æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„\")\n",
    "\n",
    "# 3. æ£€æŸ¥æ•°æ®ç±»å‹\n",
    "print(\"\\nğŸ“Š æ•°æ®ç±»å‹æ£€æŸ¥:\")\n",
    "# TODO: æ£€æŸ¥ä¸¤ä¸ªæ•°æ®é›†çš„æ•°æ®ç±»å‹\n",
    "print(f\"ä¹³è…ºç™Œæ•°æ®ç±»å‹: {type(X_bc)}\")  # ğŸ‘ˆ åœ¨è¿™é‡Œå¡«å†™ä»£ç \n",
    "print(f\"é¸¢å°¾èŠ±æ•°æ®ç±»å‹: {type(X_iris)}\")  # ğŸ‘ˆ åœ¨è¿™é‡Œå¡«å†™ä»£ç "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### âœ… è‡ªæˆ‘æ£€æŸ¥\n",
    "è¿è¡Œä¸‹é¢çš„ä»£ç æ¥éªŒè¯æ‚¨çš„åŠ è½½æ˜¯å¦æ­£ç¡®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®\n",
    "def check_data_loading():\n",
    "    \"\"\"æ£€æŸ¥æ•°æ®åŠ è½½çš„ç»“æœ\"\"\"\n",
    "    \n",
    "    # TODO: å¡«å†™æ‚¨æœŸæœ›çš„æ•°æ®å½¢çŠ¶\n",
    "    expected_bc_shape = (?, ?)  # ğŸ‘ˆ ä¹³è…ºç™Œæ•°æ®çš„é¢„æœŸå½¢çŠ¶\n",
    "    expected_iris_shape = (?, ?)  # ğŸ‘ˆ é¸¢å°¾èŠ±æ•°æ®çš„é¢„æœŸå½¢çŠ¶\n",
    "    \n",
    "    print(\"ğŸ” æ•°æ®åŠ è½½éªŒè¯:\")\n",
    "    \n",
    "    # æ£€æŸ¥ä¹³è…ºç™Œæ•°æ®\n",
    "    if 'X_bc' in locals() and 'y_bc' in locals():\n",
    "        if X_bc.shape == expected_bc_shape:\n",
    "            print(f\"   âœ… ä¹³è…ºç™Œæ•°æ®å½¢çŠ¶æ­£ç¡®: {X_bc.shape}\")\n",
    "        else:\n",
    "            print(f\"   âŒ ä¹³è…ºç™Œæ•°æ®å½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_bc_shape}, å®é™… {X_bc.shape}\")\n",
    "    else:\n",
    "        print(\"   âŒ ä¹³è…ºç™Œæ•°æ®æœªåŠ è½½\")\n",
    "    \n",
    "    # æ£€æŸ¥é¸¢å°¾èŠ±æ•°æ®\n",
    "    if 'X_iris' in locals() and 'y_iris' in locals():\n",
    "        if X_iris.shape == expected_iris_shape:\n",
    "            print(f\"   âœ… é¸¢å°¾èŠ±æ•°æ®å½¢çŠ¶æ­£ç¡®: {X_iris.shape}\")\n",
    "        else:\n",
    "            print(f\"   âŒ é¸¢å°¾èŠ±æ•°æ®å½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_iris_shape}, å®é™… {X_iris.shape}\")\n",
    "    else:\n",
    "        print(\"   âŒ é¸¢å°¾èŠ±æ•°æ®æœªåŠ è½½\")\n",
    "\n",
    "# è¿è¡Œæ£€æŸ¥\n",
    "check_data_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ğŸ” æ­¥éª¤2: æ•°æ®æ¢ç´¢\n",
    "\n",
    "### ğŸ¤” ä»»åŠ¡è¯´æ˜\n",
    "ç°åœ¨æ‚¨éœ€è¦æ¢ç´¢æ•°æ®çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- æ•°æ®è§„æ¨¡\n",
    "- æ ‡ç­¾åˆ†å¸ƒ\n",
    "- ç‰¹å¾ç»Ÿè®¡\n",
    "- æ•°æ®å¯†åº¦\n",
    "\n",
    "### ğŸ’¡ æç¤º\n",
    "- ä½¿ç”¨ `np.unique()` æŸ¥çœ‹æ ‡ç­¾\n",
    "- ä½¿ç”¨ `np.mean()` å’Œ `np.std()` è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "- ç¨€ç–çŸ©é˜µå¯ä»¥ä½¿ç”¨ `.nnz` å±æ€§æŸ¥çœ‹éé›¶å…ƒç´ æ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: åœ¨ä¸‹é¢å®Œæˆæ•°æ®æ¢ç´¢å‡½æ•°\n",
    "\n",
    "def explore_dataset(X, y, dataset_name):\n",
    "    \"\"\"æ¢ç´¢æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: ç‰¹å¾çŸ©é˜µ\n",
    "        y: æ ‡ç­¾å‘é‡  \n",
    "        dataset_name: æ•°æ®é›†åç§°\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š {dataset_name}æ•°æ®æ¢ç´¢\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. åŸºæœ¬ä¿¡æ¯\n",
    "    print(\"\\nğŸ”¹ åŸºæœ¬ä¿¡æ¯:\")\n",
    "    # TODO: å¡«å†™è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯çš„ä»£ç \n",
    "    n_samples =  # ğŸ‘ˆ è·å–æ ·æœ¬æ•°é‡\n",
    "    n_features =  # ğŸ‘ˆ è·å–ç‰¹å¾æ•°é‡\n",
    "    print(f\"   æ ·æœ¬æ•°é‡: {n_samples}\")\n",
    "    print(f\"   ç‰¹å¾æ•°é‡: {n_features}\")\n",
    "    \n",
    "    # 2. æ ‡ç­¾åˆ†æ\n",
    "    print(\"\\nğŸ”¹ æ ‡ç­¾åˆ†æ:\")\n",
    "    # TODO: åˆ†ææ ‡ç­¾åˆ†å¸ƒ\n",
    "    unique_labels =  # ğŸ‘ˆ è·å–å”¯ä¸€æ ‡ç­¾\n",
    "    label_counts =  # ğŸ‘ˆ è·å–æ¯ä¸ªæ ‡ç­¾çš„æ•°é‡\n",
    "    \n",
    "    print(f\"   ç±»åˆ«æ•°é‡: {len(unique_labels)}\")\n",
    "    print(f\"   ç±»åˆ«æ ‡ç­¾: {unique_labels}\")\n",
    "    \n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        percentage =  # ğŸ‘ˆ è®¡ç®—ç™¾åˆ†æ¯”\n",
    "        print(f\"   ç±»åˆ« {int(label)}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 3. æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥\n",
    "    # TODO: æ£€æŸ¥æ•°æ®æ˜¯å¦å¹³è¡¡\n",
    "    if len(unique_labels) == 2:  # äºŒåˆ†ç±»\n",
    "        balance_ratio =  # ğŸ‘ˆ è®¡ç®—å¹³è¡¡æ¯”ä¾‹\n",
    "        if balance_ratio > 0.8:\n",
    "            print(f\"   âœ… æ•°æ®å¹³è¡¡è‰¯å¥½ (æ¯”ä¾‹: {balance_ratio:.2f})\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ æ•°æ®ä¸å¹³è¡¡ (æ¯”ä¾‹: {balance_ratio:.2f})\")\n",
    "    \n",
    "    # 4. ç‰¹å¾ç»Ÿè®¡\n",
    "    print(\"\\nğŸ”¹ ç‰¹å¾ç»Ÿè®¡:\")\n",
    "    # TODO: è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯\n",
    "    if hasattr(X, 'toarray'):  # å¦‚æœæ˜¯ç¨€ç–çŸ©é˜µ\n",
    "        X_dense =  # ğŸ‘ˆ è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ\n",
    "    else:\n",
    "        X_dense = X\n",
    "    \n",
    "    mean_values =  # ğŸ‘ˆ è®¡ç®—å‡å€¼\n",
    "    std_values =  # ğŸ‘ˆ è®¡ç®—æ ‡å‡†å·®\n",
    "    \n",
    "    print(f\"   ç‰¹å¾å‡å€¼èŒƒå›´: {np.min(mean_values):.4f} - {np.max(mean_values):.4f}\")\n",
    "    print(f\"   ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: {np.min(std_values):.4f} - {np.max(std_values):.4f}\")\n",
    "    print(f\"   æ•°å€¼èŒƒå›´: {np.min(X_dense):.4f} - {np.max(X_dense):.4f}\")\n",
    "    \n",
    "    # 5. ç¨€ç–åº¦åˆ†æï¼ˆé’ˆå¯¹ç¨€ç–çŸ©é˜µï¼‰\n",
    "    if hasattr(X, 'toarray'):\n",
    "        print(\"\\nğŸ”¹ ç¨€ç–åº¦åˆ†æ:\")\n",
    "        # TODO: è®¡ç®—ç¨€ç–åº¦\n",
    "        total_elements =  # ğŸ‘ˆ è®¡ç®—æ€»å…ƒç´ æ•°\n",
    "        nonzero_elements =  # ğŸ‘ˆ è·å–éé›¶å…ƒç´ æ•°\n",
    "        sparsity =  # ğŸ‘ˆ è®¡ç®—ç¨€ç–åº¦\n",
    "        \n",
    "        print(f\"   éé›¶å…ƒç´ : {nonzero_elements} / {total_elements}\")\n",
    "        print(f\"   ç¨€ç–åº¦: {sparsity*100:.2f}%\")\n",
    "    \n",
    "    return X_dense\n",
    "\n",
    "# æ¢ç´¢ä¸¤ä¸ªæ•°æ®é›†\n",
    "if 'X_bc' in locals():\n",
    "    X_bc_dense = explore_dataset(X_bc, y_bc, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'X_iris' in locals():\n",
    "    X_iris_dense = explore_dataset(X_iris, y_iris, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒé¢˜\n",
    "æ ¹æ®æ‚¨åˆšæ‰çš„æ¢ç´¢ç»“æœï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è€ƒé¢˜ï¼šåŸºäºæ‚¨çš„æ¢ç´¢ç»“æœå›ç­”\n",
    "print(\"ğŸ¤” æ€è€ƒé¢˜:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# TODO: æ ¹æ®æ‚¨çš„åˆ†æç»“æœå›ç­”\n",
    "print(\"\\n1. ä¹³è…ºç™Œæ•°æ®é›†:\")\n",
    "print(\"   - è¿™æ˜¯äºŒåˆ†ç±»è¿˜æ˜¯å¤šåˆ†ç±»é—®é¢˜ï¼Ÿ\")\n",
    "print(\"   - æ•°æ®æ˜¯å¦å¹³è¡¡ï¼Ÿ\")\n",
    "print(\"   - ç‰¹å¾æ•°å€¼çš„å¤§è‡´èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ\")\n",
    "\n",
    "print(\"\\n2. é¸¢å°¾èŠ±æ•°æ®é›†:\")\n",
    "print(\"   - è¿™æ˜¯äºŒåˆ†ç±»è¿˜æ˜¯å¤šåˆ†ç±»é—®é¢˜ï¼Ÿ\")\n",
    "print(\"   - æœ‰å¤šå°‘ä¸ªç±»åˆ«ï¼Ÿ\")\n",
    "print(\"   - æ¯ä¸ªç±»åˆ«æœ‰å¤šå°‘æ ·æœ¬ï¼Ÿ\")\n",
    "\n",
    "print(\"\\n3. æ•°æ®ç‰¹å¾:\")\n",
    "print(\"   - ä¸¤ä¸ªæ•°æ®é›†çš„ç‰¹å¾æ•°é‡åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ\")\n",
    "print(\"   - ç‰¹å¾æ˜¯å¦å·²ç»æ ‡å‡†åŒ–ï¼Ÿ\")\n",
    "print(\"   - æ•°æ®æ˜¯å¦ç¨€ç–ï¼Ÿ\")\n",
    "\n",
    "# åœ¨ä¸‹é¢å†™ä¸‹æ‚¨çš„ç­”æ¡ˆ\n",
    "print(\"\\nğŸ’­ æˆ‘çš„è§‚å¯Ÿ:\")\n",
    "print(\"(åœ¨è¿™é‡Œå†™ä¸‹æ‚¨çš„å‘ç°å’Œæ€è€ƒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ğŸ§¹ æ­¥éª¤3: æ•°æ®æ¸…æ´—\n",
    "\n",
    "### ğŸ¤” ä»»åŠ¡è¯´æ˜\n",
    "ç°åœ¨æ‚¨éœ€è¦å®Œæˆæ•°æ®æ¸…æ´—çš„å„ä¸ªæ­¥éª¤ï¼š\n",
    "- æ£€æŸ¥å’Œå¤„ç†ç¼ºå¤±å€¼\n",
    "- è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸å€¼\n",
    "- å¤„ç†é‡å¤å€¼\n",
    "- æ ‡ç­¾æ ‡å‡†åŒ–\n",
    "\n",
    "### ğŸ’¡ æç¤º\n",
    "- ä½¿ç”¨ `np.isnan()` æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "- ä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼\n",
    "- ä½¿ç”¨ `np.unique()` æ£€æŸ¥é‡å¤å€¼\n",
    "- æ ‡ç­¾åº”è¯¥ä» 0 å¼€å§‹è¿ç»­ç¼–å·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å®Œæˆæ•°æ®æ¸…æ´—å‡½æ•°\n",
    "\n",
    "def clean_dataset(X, y, dataset_name):\n",
    "    \"\"\"æ¸…æ´—æ•°æ®é›†\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: ç‰¹å¾çŸ©é˜µ\n",
    "        y: æ ‡ç­¾å‘é‡\n",
    "        dataset_name: æ•°æ®é›†åç§°\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§¹ {dataset_name}æ•°æ®æ¸…æ´—\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # ç¡®ä¿ä½¿ç”¨å¯†é›†çŸ©é˜µè¿›è¡Œå¤„ç†\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X.copy()\n",
    "    \n",
    "    original_shape = X_dense.shape\n",
    "    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {original_shape}\")\n",
    "    \n",
    "    # 1. ç¼ºå¤±å€¼æ£€æŸ¥\n",
    "    print(\"\\nğŸ”¹ ç¼ºå¤±å€¼æ£€æŸ¥:\")\n",
    "    # TODO: æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "    missing_values =  # ğŸ‘ˆ æ£€æŸ¥ç¼ºå¤±å€¼çš„æ•°é‡\n",
    "    \n",
    "    if missing_values > 0:\n",
    "        print(f\"   å‘ç° {missing_values} ä¸ªç¼ºå¤±å€¼\")\n",
    "        \n",
    "        # TODO: å¤„ç†ç¼ºå¤±å€¼\n",
    "        # æç¤ºï¼šå¯ä»¥ç”¨å‡å€¼æˆ–ä¸­ä½æ•°å¡«å……\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_dense =  # ğŸ‘ˆ åº”ç”¨ç¼ºå¤±å€¼å¤„ç†\n",
    "        print(f\"   âœ… å·²ä½¿ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°ç¼ºå¤±å€¼\")\n",
    "    \n",
    "    # 2. å¼‚å¸¸å€¼æ£€æµ‹\n",
    "    print(\"\\nğŸ”¹ å¼‚å¸¸å€¼æ£€æµ‹:\")\n",
    "    \n",
    "    # TODO: ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼\n",
    "    def detect_outliers_iqr(data):\n",
    "        Q1 =  # ğŸ‘ˆ è®¡ç®—ç¬¬ä¸€å››åˆ†ä½æ•°\n",
    "        Q3 =  # ğŸ‘ˆ è®¡ç®—ç¬¬ä¸‰å››åˆ†ä½æ•°\n",
    "        IQR =  # ğŸ‘ˆ è®¡ç®—IQR\n",
    "        lower_bound =  # ğŸ‘ˆ è®¡ç®—ä¸‹ç•Œ\n",
    "        upper_bound =  # ğŸ‘ˆ è®¡ç®—ä¸Šç•Œ\n",
    "        return (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    # æ£€æµ‹æ¯ä¸ªç‰¹å¾çš„å¼‚å¸¸å€¼\n",
    "    total_outliers = 0\n",
    "    for feature_idx in range(X_dense.shape[1]):\n",
    "        feature_data = X_dense[:, feature_idx]\n",
    "        outliers =  # ğŸ‘ˆ æ£€æµ‹å¼‚å¸¸å€¼\n",
    "        outlier_count =  # ğŸ‘ˆ ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡\n",
    "        total_outliers += outlier_count\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        print(f\"   å‘ç° {total_outliers} ä¸ªå¼‚å¸¸å€¼\")\n",
    "        \n",
    "        # TODO: å¤„ç†å¼‚å¸¸å€¼\n",
    "        for feature_idx in range(X_dense.shape[1]):\n",
    "            feature_data = X_dense[:, feature_idx]\n",
    "            outliers = detect_outliers_iqr(feature_data)\n",
    "            if np.any(outliers):\n",
    "                # ä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼\n",
    "                median_val =  # ğŸ‘ˆ è®¡ç®—ä¸­ä½æ•°\n",
    "                X_dense[outliers, feature_idx] = median_val\n",
    "        print(f\"   âœ… å·²ä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸å€¼\")\n",
    "    \n",
    "    # 3. é‡å¤å€¼æ£€æŸ¥\n",
    "    print(\"\\nğŸ”¹ é‡å¤å€¼æ£€æŸ¥:\")\n",
    "    # TODO: æ£€æŸ¥é‡å¤å€¼\n",
    "    data_with_labels =  # ğŸ‘ˆ ç»„åˆç‰¹å¾å’Œæ ‡ç­¾\n",
    "    unique_data, unique_indices =  # ğŸ‘ˆ è·å–å”¯ä¸€æ•°æ®\n",
    "    \n",
    "    duplicate_count =  # ğŸ‘ˆ è®¡ç®—é‡å¤å€¼æ•°é‡\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   å‘ç° {duplicate_count} ä¸ªé‡å¤æ ·æœ¬\")\n",
    "        # TODO: åˆ é™¤é‡å¤å€¼\n",
    "        X_dense =  # ğŸ‘ˆ ä¿ç•™å”¯ä¸€æ ·æœ¬\n",
    "        y =  # ğŸ‘ˆ ä¿ç•™å¯¹åº”çš„æ ‡ç­¾\n",
    "        print(f\"   âœ… å·²åˆ é™¤é‡å¤æ ·æœ¬\")\n",
    "    else:\n",
    "        print(f\"   âœ… æœªå‘ç°é‡å¤æ ·æœ¬\")\n",
    "    \n",
    "    # 4. æ ‡ç­¾æ ‡å‡†åŒ–\n",
    "    print(\"\\nğŸ”¹ æ ‡ç­¾æ ‡å‡†åŒ–:\")\n",
    "    # TODO: æ£€æŸ¥å¹¶æ ‡å‡†åŒ–æ ‡ç­¾\n",
    "    unique_labels =  # ğŸ‘ˆ è·å–å”¯ä¸€æ ‡ç­¾\n",
    "    print(f\"   åŸå§‹æ ‡ç­¾: {unique_labels}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä»0å¼€å§‹è¿ç»­\n",
    "    if np.min(y) != 0 or len(unique_labels) != np.max(y) + 1:\n",
    "        print(f\"   æ ‡ç­¾éœ€è¦æ ‡å‡†åŒ–...\")\n",
    "        \n",
    "        # TODO: åˆ›å»ºæ ‡ç­¾æ˜ å°„\n",
    "        label_mapping =  # ğŸ‘ˆ åˆ›å»ºä»æ—§æ ‡ç­¾åˆ°æ–°æ ‡ç­¾çš„æ˜ å°„\n",
    "        print(f\"   æ ‡ç­¾æ˜ å°„: {label_mapping}\")\n",
    "        \n",
    "        # TODO: åº”ç”¨æ ‡ç­¾æ˜ å°„\n",
    "        y_standardized =  # ğŸ‘ˆ åº”ç”¨æ˜ å°„\n",
    "        print(f\"   æ ‡å‡†åŒ–åæ ‡ç­¾: {np.unique(y_standardized)}\")\n",
    "        y = y_standardized\n",
    "    else:\n",
    "        print(f\"   âœ… æ ‡ç­¾å·²ç»æ˜¯æ ‡å‡†æ ¼å¼\")\n",
    "    \n",
    "    # æ€»ç»“æ¸…æ´—ç»“æœ\n",
    "    print(f\"\\nğŸ“‹ æ¸…æ´—æ€»ç»“:\")\n",
    "    print(f\"   åŸå§‹æ•°æ®: {original_shape}\")\n",
    "    print(f\"   æ¸…æ´—åæ•°æ®: {X_dense.shape}\")\n",
    "    print(f\"   æ ‡ç­¾èŒƒå›´: {np.min(y)} - {np.max(y)}\")\n",
    "    \n",
    "    return X_dense, y\n",
    "\n",
    "# æ¸…æ´—ä¸¤ä¸ªæ•°æ®é›†\n",
    "if 'X_bc_dense' in locals():\n",
    "    X_bc_clean, y_bc_clean = clean_dataset(X_bc_dense, y_bc, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'X_iris_dense' in locals():\n",
    "    X_iris_clean, y_iris_clean = clean_dataset(X_iris_dense, y_iris, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### âœ… æ¸…æ´—ç»“æœéªŒè¯\n",
    "è¿è¡Œä¸‹é¢çš„ä»£ç æ¥éªŒè¯æ‚¨çš„æ¸…æ´—ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯æ•°æ®æ¸…æ´—ç»“æœ\n",
    "def validate_cleaning(X_original, y_original, X_cleaned, y_cleaned, dataset_name):\n",
    "    \"\"\"éªŒè¯æ•°æ®æ¸…æ´—çš„ç»“æœ\"\"\"\n",
    "    print(f\"\\nğŸ” {dataset_name}æ¸…æ´—éªŒè¯:\")\n",
    "    \n",
    "    # TODO: éªŒè¯æ¸…æ´—ç»“æœ\n",
    "    \n",
    "    # 1. æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "    missing_after =  # ğŸ‘ˆ æ£€æŸ¥æ¸…æ´—åæ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼\n",
    "    if missing_after == 0:\n",
    "        print(f\"   âœ… æ— ç¼ºå¤±å€¼\")\n",
    "    else:\n",
    "        print(f\"   âŒ ä»æœ‰ {missing_after} ä¸ªç¼ºå¤±å€¼\")\n",
    "    \n",
    "    # 2. æ£€æŸ¥æ ‡ç­¾æ ‡å‡†åŒ–\n",
    "    unique_labels_clean =  # ğŸ‘ˆ è·å–æ¸…æ´—åçš„æ ‡ç­¾\n",
    "    is_standard =  # ğŸ‘ˆ æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä»0å¼€å§‹è¿ç»­\n",
    "    \n",
    "    if is_standard:\n",
    "        print(f\"   âœ… æ ‡ç­¾æ ‡å‡†åŒ–æ­£ç¡®: {unique_labels_clean}\")\n",
    "    else:\n",
    "        print(f\"   âŒ æ ‡ç­¾æ ‡å‡†åŒ–æœ‰é—®é¢˜: {unique_labels_clean}\")\n",
    "    \n",
    "    # 3. æ£€æŸ¥æ•°æ®å½¢çŠ¶å˜åŒ–\n",
    "    shape_changed =  # ğŸ‘ˆ æ£€æŸ¥å½¢çŠ¶æ˜¯å¦æ”¹å˜\n",
    "    if shape_changed:\n",
    "        print(f\"   âš ï¸ æ•°æ®å½¢çŠ¶å‘ç”Ÿå˜åŒ–: å¯èƒ½åˆ é™¤äº†é‡å¤å€¼\")\n",
    "    else:\n",
    "        print(f\"   âœ… æ•°æ®å½¢çŠ¶ä¿æŒä¸€è‡´\")\n",
    "\n",
    "# éªŒè¯æ¸…æ´—ç»“æœ\n",
    "if 'X_bc_clean' in locals():\n",
    "    # è·å–åŸå§‹æ•°æ®çš„å¯†é›†ç‰ˆæœ¬ç”¨äºæ¯”è¾ƒ\n",
    "    X_bc_original_dense = X_bc.toarray() if hasattr(X_bc, 'toarray') else X_bc\n",
    "    validate_cleaning(X_bc_original_dense, y_bc, X_bc_clean, y_bc_clean, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'X_iris_clean' in locals():\n",
    "    X_iris_original_dense = X_iris.toarray() if hasattr(X_iris, 'toarray') else X_iris\n",
    "    validate_cleaning(X_iris_original_dense, y_iris, X_iris_clean, y_iris_clean, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## âš™ï¸ æ­¥éª¤4: æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "### ğŸ¤” ä»»åŠ¡è¯´æ˜\n",
    "ç°åœ¨æ‚¨éœ€è¦å®Œæˆæ•°æ®é¢„å¤„ç†çš„å„ä¸ªæ­¥éª¤ï¼š\n",
    "- ç‰¹å¾ç¼©æ”¾ï¼ˆæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ï¼‰\n",
    "- ç‰¹å¾é€‰æ‹©\n",
    "- æ•°æ®åˆ†å‰²\n",
    "\n",
    "### ğŸ’¡ é‡è¦æé†’\n",
    "**è®°ä½é¢„å¤„ç†çš„å…³é”®åŸåˆ™ï¼šå…ˆåˆ†å‰²ï¼Œå†é¢„å¤„ç†ï¼** é¿å…æ•°æ®æ³„éœ²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥é¢„å¤„ç†åº“\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# TODO: å®Œæˆé¢„å¤„ç†æµæ°´çº¿\n",
    "\n",
    "def preprocess_pipeline(X, y, dataset_name, test_size=0.3):\n",
    "    \"\"\"å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµæ°´çº¿\n",
    "    \n",
    "    é‡è¦ï¼šå…ˆåˆ†å‰²ï¼Œå†é¢„å¤„ç†ï¼\n",
    "    \"\"\"\n",
    "    print(f\"\\nâš™ï¸ {dataset_name}é¢„å¤„ç†æµæ°´çº¿\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. æ•°æ®åˆ†å‰²ï¼ˆç¬¬ä¸€æ­¥ï¼ï¼‰\n",
    "    print(\"\\nğŸ”¹ æ­¥éª¤1: æ•°æ®åˆ†å‰²\")\n",
    "    # TODO: åˆ†å‰²æ•°æ®\n",
    "    X_train, X_test, y_train, y_test =  # ğŸ‘ˆ åˆ†å‰²æ•°æ®\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†: {X_train.shape}\")\n",
    "    print(f\"   æµ‹è¯•é›†: {X_test.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥åˆ†å‰²åçš„ç±»åˆ«åˆ†å¸ƒ\n",
    "    print(\"\\n   ç±»åˆ«åˆ†å¸ƒæ£€æŸ¥:\")\n",
    "    # TODO: æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç±»åˆ«åˆ†å¸ƒ\n",
    "    train_unique, train_counts =  # ğŸ‘ˆ è·å–è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ\n",
    "    test_unique, test_counts =  # ğŸ‘ˆ è·å–æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ\n",
    "    \n",
    "    for label, count in zip(train_unique, train_counts):\n",
    "        train_pct =  # ğŸ‘ˆ è®¡ç®—ç™¾åˆ†æ¯”\n",
    "        print(f\"   è®­ç»ƒé›†ç±»åˆ«{int(label)}: {count} ({train_pct:.1f}%)\")\n",
    "    \n",
    "    for label, count in zip(test_unique, test_counts):\n",
    "        test_pct =  # ğŸ‘ˆ è®¡ç®—ç™¾åˆ†æ¯”\n",
    "        print(f\"   æµ‹è¯•é›†ç±»åˆ«{int(label)}: {count} ({test_pct:.1f}%)\")\n",
    "    \n",
    "    # 2. ç‰¹å¾ç¼©æ”¾ï¼ˆåœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å‚æ•°ï¼‰\n",
    "    print(\"\\nğŸ”¹ æ­¥éª¤2: ç‰¹å¾ç¼©æ”¾\")\n",
    "    \n",
    "    # æ–¹æ³•1: æ ‡å‡†åŒ–\n",
    "    print(\"   æ–¹æ³•1: æ ‡å‡†åŒ– (Z-score)\")\n",
    "    # TODO: åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ æ ‡å‡†åŒ–å‚æ•°\n",
    "    scaler_standard =  # ğŸ‘ˆ åˆ›å»ºæ ‡å‡†åŒ–å™¨\n",
    "    X_train_standard =  # ğŸ‘ˆ åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å¹¶åº”ç”¨\n",
    "    X_test_standard =  # ğŸ‘ˆ åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨ï¼ˆä½¿ç”¨è®­ç»ƒé›†å‚æ•°ï¼‰\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†æ ‡å‡†åŒ–åå‡å€¼èŒƒå›´: {np.min(np.mean(X_train_standard, axis=0)):.4f} - {np.max(np.mean(X_train_standard, axis=0)):.4f}\")\n",
    "    print(f\"   æµ‹è¯•é›†æ ‡å‡†åŒ–åå‡å€¼èŒƒå›´: {np.min(np.mean(X_test_standard, axis=0)):.4f} - {np.max(np.mean(X_test_standard, axis=0)):.4f}\")\n",
    "    \n",
    "    # æ–¹æ³•2: å½’ä¸€åŒ–\n",
    "    print(\"\\n   æ–¹æ³•2: å½’ä¸€åŒ– (Min-Max)\")\n",
    "    # TODO: åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å½’ä¸€åŒ–å‚æ•°\n",
    "    scaler_minmax =  # ğŸ‘ˆ åˆ›å»ºå½’ä¸€åŒ–å™¨\n",
    "    X_train_minmax =  # ğŸ‘ˆ åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å¹¶åº”ç”¨\n",
    "    X_test_minmax =  # ğŸ‘ˆ åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†å½’ä¸€åŒ–åèŒƒå›´: {np.min(X_train_minmax):.4f} - {np.max(X_train_minmax):.4f}\")\n",
    "    print(f\"   æµ‹è¯•é›†å½’ä¸€åŒ–åèŒƒå›´: {np.min(X_test_minmax):.4f} - {np.max(X_test_minmax):.4f}\")\n",
    "    \n",
    "    # 3. ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰\n",
    "    print(\"\\nğŸ”¹ æ­¥éª¤3: ç‰¹å¾é€‰æ‹©\")\n",
    "    if X_train_standard.shape[1] > 10:  # åªæœ‰å½“ç‰¹å¾è¾ƒå¤šæ—¶æ‰è¿›è¡Œé€‰æ‹©\n",
    "        # TODO: é€‰æ‹©å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
    "        selector =  # ğŸ‘ˆ åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨\n",
    "        X_train_selected =  # ğŸ‘ˆ åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å¹¶åº”ç”¨\n",
    "        X_test_selected =  # ğŸ‘ˆ åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨\n",
    "        \n",
    "        print(f\"   ç‰¹å¾é€‰æ‹©: {X_train_standard.shape[1]} -> {X_train_selected.shape[1]}\")\n",
    "        use_selection = True\n",
    "    else:\n",
    "        print(f\"   ç‰¹å¾æ•°é‡è¾ƒå°‘({X_train_standard.shape[1]})ï¼Œè·³è¿‡ç‰¹å¾é€‰æ‹©\")\n",
    "        use_selection = False\n",
    "        X_train_selected = X_train_standard\n",
    "        X_test_selected = X_test_standard\n",
    "    \n",
    "    # 4. æ•°æ®æ³„éœ²æ£€æŸ¥\n",
    "    print(\"\\nğŸ”¹ æ­¥éª¤4: æ•°æ®æ³„éœ²æ£€æŸ¥\")\n",
    "    # TODO: æ£€æŸ¥æ•°æ®æ³„éœ²\n",
    "    def check_leakage(X_train, X_test):\n",
    "        # ç®€å•æ£€æŸ¥ï¼šæ¯”è¾ƒè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç»Ÿè®¡ç‰¹æ€§\n",
    "        train_mean =  # ğŸ‘ˆ è®¡ç®—è®­ç»ƒé›†å‡å€¼\n",
    "        test_mean =  # ğŸ‘ˆ è®¡ç®—æµ‹è¯•é›†å‡å€¼\n",
    "        \n",
    "        mean_diff =  # ğŸ‘ˆ è®¡ç®—å‡å€¼å·®å¼‚\n",
    "        return mean_diff\n",
    "    \n",
    "    leakage_score = check_leakage(X_train_standard, X_test_standard)\n",
    "    print(f\"   æ•°æ®æ³„éœ²è¯„åˆ†: {leakage_score:.4f}\")\n",
    "    if leakage_score < 0.1:\n",
    "        print(f\"   âœ… æ•°æ®æ³„éœ²é£é™©ä½\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼Œè¯·æ£€æŸ¥é¢„å¤„ç†æµç¨‹\")\n",
    "    \n",
    "    # è¿”å›å¤„ç†ç»“æœ\n",
    "    return {\n",
    "        'X_train': X_train_selected,\n",
    "        'X_test': X_test_selected,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler_standard': scaler_standard,\n",
    "        'scaler_minmax': scaler_minmax,\n",
    "        'selector': selector if use_selection else None,\n",
    "        'used_selection': use_selection\n",
    "    }\n",
    "\n",
    "# å¯¹ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†\n",
    "if 'X_bc_clean' in locals():\n",
    "    bc_processed = preprocess_pipeline(X_bc_clean, y_bc_clean, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'X_iris_clean' in locals():\n",
    "    iris_processed = preprocess_pipeline(X_iris_clean, y_iris_clean, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### ğŸ” é¢„å¤„ç†æ•ˆæœéªŒè¯\n",
    "è®©æˆ‘ä»¬éªŒè¯æ‚¨çš„é¢„å¤„ç†æ˜¯å¦æ­£ç¡®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯é¢„å¤„ç†ç»“æœ\n",
    "def validate_preprocessing(processed_data, dataset_name):\n",
    "    \"\"\"éªŒè¯é¢„å¤„ç†çš„ç»“æœ\"\"\"\n",
    "    print(f\"\\nğŸ” {dataset_name}é¢„å¤„ç†éªŒè¯:\")\n",
    "    \n",
    "    X_train = processed_data['X_train']\n",
    "    X_test = processed_data['X_test']\n",
    "    y_train = processed_data['y_train']\n",
    "    y_test = processed_data['y_test']\n",
    "    \n",
    "    # TODO: éªŒè¯é¢„å¤„ç†ç»“æœ\n",
    "    \n",
    "    # 1. æ£€æŸ¥æ ‡å‡†åŒ–æ•ˆæœ\n",
    "    train_mean =  # ğŸ‘ˆ è®¡ç®—è®­ç»ƒé›†å‡å€¼\n",
    "    train_std =  # ğŸ‘ˆ è®¡ç®—è®­ç»ƒé›†æ ‡å‡†å·®\n",
    "    \n",
    "    print(f\"   è®­ç»ƒé›†æ ‡å‡†åŒ–æ•ˆæœ:\")\n",
    "    print(f\"     å‡å€¼èŒƒå›´: {np.min(train_mean):.6f} - {np.max(train_mean):.6f}\")\n",
    "    print(f\"     æ ‡å‡†å·®èŒƒå›´: {np.min(train_std):.6f} - {np.max(train_std):.6f}\")\n",
    "    \n",
    "    # 2. æ£€æŸ¥æ•°æ®åˆ†å‰²æ¯”ä¾‹\n",
    "    total_samples =  # ğŸ‘ˆ è®¡ç®—æ€»æ ·æœ¬æ•°\n",
    "    train_ratio =  # ğŸ‘ˆ è®¡ç®—è®­ç»ƒé›†æ¯”ä¾‹\n",
    "    test_ratio =  # ğŸ‘ˆ è®¡ç®—æµ‹è¯•é›†æ¯”ä¾‹\n",
    "    \n",
    "    print(f\"\\n   æ•°æ®åˆ†å‰²æ¯”ä¾‹:\")\n",
    "    print(f\"     è®­ç»ƒé›†: {train_ratio:.1%} ({len(X_train)} æ ·æœ¬)\")\n",
    "    print(f\"     æµ‹è¯•é›†: {test_ratio:.1%} ({len(X_test)} æ ·æœ¬)\")\n",
    "    \n",
    "    # 3. æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒä¿æŒ\n",
    "    train_classes =  # ğŸ‘ˆ è·å–è®­ç»ƒé›†ç±»åˆ«\n",
    "    test_classes =  # ğŸ‘ˆ è·å–æµ‹è¯•é›†ç±»åˆ«\n",
    "    \n",
    "    if np.array_equal(train_classes, test_classes):\n",
    "        print(f\"\\n   âœ… ç±»åˆ«åˆ†å¸ƒä¿æŒä¸€è‡´: {train_classes}\")\n",
    "    else:\n",
    "        print(f\"\\n   âŒ ç±»åˆ«åˆ†å¸ƒä¸ä¸€è‡´:\")\n",
    "        print(f\"     è®­ç»ƒé›†: {train_classes}\")\n",
    "        print(f\"     æµ‹è¯•é›†: {test_classes}\")\n",
    "    \n",
    "    # 4. æ£€æŸ¥ç‰¹å¾é€‰æ‹©æ•ˆæœ\n",
    "    if processed_data['used_selection']:\n",
    "        original_features = X_train.shape[1]\n",
    "        print(f\"\\n   ç‰¹å¾é€‰æ‹©: ä¿ç•™äº† {original_features} ä¸ªç‰¹å¾\")\n",
    "    else:\n",
    "        print(f\"\\n   âœ… æœªä½¿ç”¨ç‰¹å¾é€‰æ‹©\")\n",
    "\n",
    "# éªŒè¯é¢„å¤„ç†ç»“æœ\n",
    "if 'bc_processed' in locals():\n",
    "    validate_preprocessing(bc_processed, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'iris_processed' in locals():\n",
    "    validate_preprocessing(iris_processed, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ğŸ¯ æŒ‘æˆ˜ä»»åŠ¡ï¼šå®Œæ•´æµæ°´çº¿\n",
    "\n",
    "### ğŸ¤” ä»»åŠ¡è¯´æ˜\n",
    "ç°åœ¨æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œæ•´åˆæ‰€æœ‰æ­¥éª¤ã€‚è¿™ä¸ªå‡½æ•°åº”è¯¥èƒ½å¤Ÿå¤„ç†ä»»ä½•ç¬¦åˆæ ¼å¼çš„æ•°æ®é›†ã€‚\n",
    "\n",
    "### ğŸ’¡ è¦æ±‚\n",
    "- åŒ…å«æ‰€æœ‰5ä¸ªæ­¥éª¤\n",
    "- å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ\n",
    "- æä¾›è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯\n",
    "- è¿”å›å¤„ç†ç»“æœçš„å­—å…¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: åˆ›å»ºå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "\n",
    "def complete_data_pipeline(X, y, dataset_name=\"æ•°æ®é›†\", test_size=0.3, random_state=42):\n",
    "    \"\"\"å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "    \n",
    "    è¿™ä¸ªå‡½æ•°æ•´åˆäº†æ‰€æœ‰çš„æ•°æ®å¤„ç†æ­¥éª¤ï¼š\n",
    "    1. æ•°æ®æ¢ç´¢\n",
    "    2. æ•°æ®æ¸…æ´—\n",
    "    3. æ•°æ®åˆ†å‰²\n",
    "    4. æ•°æ®é¢„å¤„ç†\n",
    "    5. ç»“æœéªŒè¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: ç‰¹å¾çŸ©é˜µ\n",
    "        y: æ ‡ç­¾å‘é‡\n",
    "        dataset_name: æ•°æ®é›†åç§°\n",
    "        test_size: æµ‹è¯•é›†æ¯”ä¾‹\n",
    "        random_state: éšæœºç§å­\n",
    "    \n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ‰€æœ‰å¤„ç†ç»“æœçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹{dataset_name}å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # å­˜å‚¨æ‰€æœ‰å¤„ç†ç»“æœ\n",
    "    pipeline_results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'original_shape': None,\n",
    "        'cleaned_shape': None,\n",
    "        'final_shape': None,\n",
    "        'processing_steps': [],\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # æ­¥éª¤1: æ•°æ®æ¢ç´¢\n",
    "        print(\"\\nğŸ“Š æ­¥éª¤1: æ•°æ®æ¢ç´¢\")\n",
    "        # TODO: å®Œæˆæ•°æ®æ¢ç´¢\n",
    "        pipeline_results['original_shape'] =  # ğŸ‘ˆ è®°å½•åŸå§‹å½¢çŠ¶\n",
    "        \n",
    "        # åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„æ¢ç´¢ä»£ç ...\n",
    "        # æç¤ºï¼šè°ƒç”¨ä¹‹å‰å†™çš„ explore_dataset å‡½æ•°\n",
    "        \n",
    "        pipeline_results['processing_steps'].append(\"æ•°æ®æ¢ç´¢å®Œæˆ\")\n",
    "        \n",
    "        # æ­¥éª¤2: æ•°æ®æ¸…æ´—\n",
    "        print(\"\\nğŸ§¹ æ­¥éª¤2: æ•°æ®æ¸…æ´—\")\n",
    "        # TODO: å®Œæˆæ•°æ®æ¸…æ´—\n",
    "        X_cleaned, y_cleaned =  # ğŸ‘ˆ è°ƒç”¨æ¸…æ´—å‡½æ•°\n",
    "        pipeline_results['cleaned_shape'] =  # ğŸ‘ˆ è®°å½•æ¸…æ´—åå½¢çŠ¶\n",
    "        \n",
    "        pipeline_results['processing_steps'].append(\"æ•°æ®æ¸…æ´—å®Œæˆ\")\n",
    "        \n",
    "        # æ­¥éª¤3: æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²\n",
    "        print(\"\\nâš™ï¸ æ­¥éª¤3: æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²\")\n",
    "        # TODO: å®Œæˆé¢„å¤„ç†å’Œåˆ†å‰²\n",
    "        processed_data =  # ğŸ‘ˆ è°ƒç”¨é¢„å¤„ç†æµæ°´çº¿\n",
    "        \n",
    "        pipeline_results['final_shape'] = {\n",
    "            'train': processed_data['X_train'].shape,\n",
    "            'test': processed_data['X_test'].shape\n",
    "        }\n",
    "        \n",
    "        pipeline_results['processing_steps'].append(\"é¢„å¤„ç†å’Œåˆ†å‰²å®Œæˆ\")\n",
    "        \n",
    "        # æ­¥éª¤4: æœ€ç»ˆéªŒè¯\n",
    "        print(\"\\nâœ… æ­¥éª¤4: æœ€ç»ˆéªŒè¯\")\n",
    "        # TODO: æ·»åŠ æœ€ç»ˆéªŒè¯\n",
    "        \n",
    "        # éªŒè¯æ•°æ®å½¢çŠ¶å˜åŒ–\n",
    "        shape_change =  # ğŸ‘ˆ è®¡ç®—å½¢çŠ¶å˜åŒ–\n",
    "        if shape_change > 0:\n",
    "            pipeline_results['warnings'].append(f\"æ•°æ®å½¢çŠ¶å‘ç”Ÿå˜åŒ–ï¼Œå¯èƒ½åˆ é™¤äº†é‡å¤æ ·æœ¬\")\n",
    "        \n",
    "        # éªŒè¯å¤„ç†è´¨é‡\n",
    "        # åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„éªŒè¯ä»£ç ...\n",
    "        \n",
    "        pipeline_results['processing_steps'].append(\"æœ€ç»ˆéªŒè¯å®Œæˆ\")\n",
    "        \n",
    "        # ä¿å­˜æ‰€æœ‰å¤„ç†ç»“æœ\n",
    "        pipeline_results.update({\n",
    "            'X_train': processed_data['X_train'],\n",
    "            'X_test': processed_data['X_test'],\n",
    "            'y_train': processed_data['y_train'],\n",
    "            'y_test': processed_data['y_test'],\n",
    "            'scaler': processed_data['scaler_standard'],\n",
    "            'selector': processed_data['selector']\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nğŸ‰ {dataset_name}æ•°æ®å¤„ç†æµæ°´çº¿å®Œæˆï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_results['errors'].append(f\"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "        print(f\"âŒ é”™è¯¯: {str(e)}\")\n",
    "        \n",
    "    return pipeline_results\n",
    "\n",
    "# æµ‹è¯•å®Œæ•´æµæ°´çº¿\n",
    "if 'X_bc_clean' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    bc_pipeline_result = complete_data_pipeline(X_bc_clean, y_bc_clean, \"ä¹³è…ºç™Œ\")\n",
    "\n",
    "if 'X_iris_clean' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    iris_pipeline_result = complete_data_pipeline(X_iris_clean, y_iris_clean, \"é¸¢å°¾èŠ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ€»ç»“ä¸åæ€\n",
    "\n",
    "### ğŸ¯ æ­å–œæ‚¨ï¼\n",
    "å¦‚æœæ‚¨å®Œæˆäº†æ‰€æœ‰çš„ä»£ç å¡«ç©ºï¼Œé‚£ä¹ˆæ‚¨å·²ç»ï¼š\n",
    "\n",
    "1. âœ… **æŒæ¡äº†æ•°æ®åŠ è½½** - èƒ½å¤Ÿå¤„ç†ä¸åŒæ ¼å¼çš„æ•°æ®\n",
    "2. âœ… **å­¦ä¼šäº†æ•°æ®æ¢ç´¢** - èƒ½å¤Ÿåˆ†ææ•°æ®çš„åŸºæœ¬ç‰¹å¾\n",
    "3. âœ… **æŒæ¡äº†æ•°æ®æ¸…æ´—** - èƒ½å¤Ÿå¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼\n",
    "4. âœ… **å­¦ä¼šäº†æ•°æ®é¢„å¤„ç†** - èƒ½å¤Ÿæ­£ç¡®åœ°è¿›è¡Œç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åˆ†å‰²\n",
    "5. âœ… **ç†è§£äº†æ•°æ®æ³„éœ²** - çŸ¥é“å¦‚ä½•é¿å…æ•°æ®å¤„ç†ä¸­çš„å¸¸è§é”™è¯¯\n",
    "\n",
    "### ğŸ¤” åæ€é—®é¢˜\n",
    "è¯·åœ¨ä¸‹é¢å†™ä¸‹æ‚¨çš„å­¦ä¹ åæ€ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ åæ€\n",
    "print(\"ğŸ¤” å­¦ä¹ åæ€\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nğŸ’¡ æˆ‘å­¦åˆ°äº†ä»€ä¹ˆ:\")\n",
    "print(\"(åœ¨è¿™é‡Œå†™ä¸‹æ‚¨å­¦åˆ°çš„ä¸»è¦å†…å®¹)\")\n",
    "\n",
    "print(\"\\nâŒ æˆ‘é‡åˆ°äº†ä»€ä¹ˆå›°éš¾:\")\n",
    "print(\"(åœ¨è¿™é‡Œå†™ä¸‹æ‚¨é‡åˆ°çš„é—®é¢˜å’Œå¦‚ä½•è§£å†³çš„)\")\n",
    "\n",
    "print(\"\\nğŸ” æˆ‘è¿˜æœ‰ä»€ä¹ˆç–‘é—®:\")\n",
    "print(\"(åœ¨è¿™é‡Œå†™ä¸‹æ‚¨è¿˜ä¸ç†è§£çš„åœ°æ–¹)\")\n",
    "\n",
    "print(\"\\nğŸš€ æˆ‘æƒ³è¿›ä¸€æ­¥å­¦ä¹ :\")\n",
    "print(\"(åœ¨è¿™é‡Œå†™ä¸‹æ‚¨æ„Ÿå…´è¶£çš„ç›¸å…³ä¸»é¢˜)\")\n",
    "\n",
    "# è¯„ä¼°æ‚¨çš„æŒæ¡ç¨‹åº¦\n",
    "print(\"\\nğŸ“Š è‡ªæˆ‘è¯„ä¼° (1-10åˆ†):\")\n",
    "print(\"1. æ•°æ®åŠ è½½: \", \"_\" * 20)\n",
    "print(\"2. æ•°æ®æ¢ç´¢: \", \"_\" * 20)\n",
    "print(\"3. æ•°æ®æ¸…æ´—: \", \"_\" * 20)\n",
    "print(\"4. æ•°æ®é¢„å¤„ç†: \", \"_\" * 20)\n",
    "print(\"5. æ•°æ®åˆ†å‰²: \", \"_\" * 20)\n",
    "print(\"6. é¿å…æ•°æ®æ³„éœ²: \", \"_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### ğŸ”¥ é‡è¦çŸ¥è¯†ç‚¹å›é¡¾\n",
    "\n",
    "#### 1. æ•°æ®å¤„ç†çš„å…³é”®åŸåˆ™\n",
    "```python\n",
    "# âŒ é”™è¯¯åšæ³•ï¼šæ•°æ®æ³„éœ²\n",
    "X_scaled = scaler.fit_transform(X)  # ç”¨äº†å…¨éƒ¨æ•°æ®ï¼\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# âœ… æ­£ç¡®åšæ³•ï¼šé¿å…æ•°æ®æ³„éœ²\n",
    "X_train, X_test = train_test_split(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # åªç”¨è®­ç»ƒé›†\n",
    "X_test_scaled = scaler.transform(X_test)       # ç”¨è®­ç»ƒé›†å‚æ•°\n",
    "```\n",
    "\n",
    "#### 2. é¢„å¤„ç†æ–¹æ³•é€‰æ‹©\n",
    "- **æ ‡å‡†åŒ–**: é€‚ç”¨äºå¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•\n",
    "- **å½’ä¸€åŒ–**: é€‚ç”¨äºç¥ç»ç½‘ç»œã€è·ç¦»æ•æ„Ÿç®—æ³•\n",
    "- **é²æ£’ç¼©æ”¾**: é€‚ç”¨äºæœ‰å¼‚å¸¸å€¼çš„æ•°æ®\n",
    "\n",
    "#### 3. æ•°æ®åˆ†å‰²ç­–ç•¥\n",
    "- **åŸºç¡€åˆ†å‰²**: ç®€å•ä½†å¯èƒ½å¯¼è‡´ç±»åˆ«ä¸å¹³è¡¡\n",
    "- **åˆ†å±‚åˆ†å‰²**: ä¿æŒç±»åˆ«åˆ†å¸ƒï¼Œæ¨èç”¨äºåˆ†ç±»ä»»åŠ¡\n",
    "- **äº¤å‰éªŒè¯**: æ›´å¯é çš„è¯„ä¼°ï¼Œä½†è®¡ç®—æˆæœ¬é«˜\n",
    "\n",
    "### ğŸ‰ ä¸‹ä¸€æ­¥å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ ä¸åŒæ•°æ®é›†** - å°è¯•å¤„ç†æ›´å¤æ‚çš„æ•°æ®\n",
    "2. **è‡ªå®šä¹‰é¢„å¤„ç†** - æ ¹æ®ç‰¹å®šéœ€æ±‚è®¾è®¡å¤„ç†æµç¨‹\n",
    "3. **æ€§èƒ½æ¯”è¾ƒ** - æ¯”è¾ƒä¸åŒé¢„å¤„ç†æ–¹æ³•çš„æ•ˆæœ\n",
    "4. **è‡ªåŠ¨åŒ–æµæ°´çº¿** - å­¦ä¹ ä½¿ç”¨ sklearn çš„ Pipeline ç±»\n",
    "\n",
    "### ğŸ“š æ¨èé˜…è¯»\n",
    "- scikit-learn æ•°æ®é¢„å¤„ç†æ–‡æ¡£\n",
    "- ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹æ•°æ®å¤„ç†ç« èŠ‚\n",
    "- æ•°æ®ç§‘å­¦ç«èµ›ä¸­çš„æ•°æ®å¤„ç†æœ€ä½³å®è·µ\n",
    "\n",
    "**è®°ä½**: å¥½çš„æ•°æ®å¤„ç†æ˜¯æˆåŠŸæœºå™¨å­¦ä¹ é¡¹ç›®çš„åŸºç¡€ï¼ç»§ç»­ç»ƒä¹ ï¼Œä¸æ–­æé«˜ï¼ ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}