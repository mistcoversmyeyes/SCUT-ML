# ğŸ“Š æ•°æ®å¤„ç†é€ŸæŸ¥è¡¨

## ğŸš€ å¿«é€Ÿå¯åŠ¨æŒ‡å—

```python
# æ ‡å‡†æ•°æ®å¤„ç†æµç¨‹
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# 1. åŠ è½½æ•°æ®
X, y = load_your_data()

# 2. åˆ†å‰²æ•°æ®ï¼ˆé‡è¦ï¼šå…ˆåˆ†å‰²å†å¤„ç†ï¼ï¼‰
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 3. å¤„ç†è®­ç»ƒé›†
imputer = SimpleImputer(strategy='mean')
X_train_clean = imputer.fit_transform(X_train)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_clean)

# 4. å¤„ç†æµ‹è¯•é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°ï¼‰
X_test_clean = imputer.transform(X_test)
X_test_scaled = scaler.transform(X_test_clean)
```

---

## ğŸ“¥ æ•°æ®åŠ è½½æ–¹æ³•å¯¹æ¯”

| æ ¼å¼ | é€‚ç”¨åœºæ™¯ | ä»£ç ç¤ºä¾‹ | ä¼˜ç¼ºç‚¹ |
|------|----------|----------|--------|
| **LIBSVM** | ç¨€ç–æ•°æ®ã€æ–‡æœ¬åˆ†ç±» | `X, y = load_svmlight_file('data.scale')` | âœ…èŠ‚çœå†…å­˜ âŒå¯è¯»æ€§å·® |
| **CSV** | è¡¨æ ¼æ•°æ®ã€ç»“æ„åŒ–æ•°æ® | `df = pd.read_csv('data.csv')` | âœ…æ˜“è¯»æ˜“ç¼–è¾‘ âŒå†…å­˜å ç”¨å¤§ |
| **NumPy** | æ•°å€¼è®¡ç®—ã€å›¾åƒæ•°æ® | `X = np.load('data.npy')` | âœ…é€Ÿåº¦å¿« âŒæ— å…ƒæ•°æ® |
| **JSON** | åŠç»“æ„åŒ–æ•°æ®ã€APIæ•°æ® | `data = json.load(open('data.json'))` | âœ…çµæ´»æ€§å¥½ âŒè§£æå¤æ‚ |

---

## ğŸ” æ•°æ®æ¢ç´¢æ£€æŸ¥æ¸…å•

### å¿…é¡»æ£€æŸ¥çš„é¡¹ç›®ï¼š
- [ ] **æ•°æ®è§„æ¨¡**: `X.shape`, `y.shape`
- [ ] **æ ‡ç­¾åˆ†å¸ƒ**: `np.unique(y, return_counts=True)`
- [ ] **ç¼ºå¤±å€¼**: `np.sum(np.isnan(X))`
- [ ] **æ•°æ®ç±»å‹**: `X.dtype`, `y.dtype`
- [ ] **æ•°å€¼èŒƒå›´**: `np.min(X)`, `np.max(X)`

### é«˜çº§æ£€æŸ¥é¡¹ç›®ï¼š
- [ ] **ç±»åˆ«å¹³è¡¡**: `min(counts)/max(counts)`
- [ ] **ç‰¹å¾ç›¸å…³æ€§**: `np.corrcoef(X.T)`
- [ ] **å¼‚å¸¸å€¼**: ç®±çº¿å›¾åˆ†æ
- [ ] **ç¨€ç–åº¦**: `1 - X.nnz/(X.shape[0]*X.shape[1])`

---

## ğŸ§¹ æ•°æ®æ¸…æ´—å¸¸è§é—®é¢˜

### ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
```python
# ç­–ç•¥1: åˆ é™¤
X_clean = X[~np.isnan(X).any(axis=1)]

# ç­–ç•¥2: å‡å€¼å¡«å……ï¼ˆæ¨èï¼‰
imputer = SimpleImputer(strategy='mean')
X_clean = imputer.fit_transform(X)

# ç­–ç•¥3: ä¸­ä½æ•°å¡«å……ï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
imputer = SimpleImputer(strategy='median')
X_clean = imputer.fit_transform(X)

# ç­–ç•¥4: ä¼—æ•°å¡«å……ï¼ˆåˆ†ç±»å‹æ•°æ®ï¼‰
imputer = SimpleImputer(strategy='most_frequent')
X_clean = imputer.fit_transform(X)
```

### å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
```python
# æ–¹æ³•1: IQRæ–¹æ³•
Q1 = np.percentile(X, 25)
Q3 = np.percentile(X, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = (X < lower_bound) | (X > upper_bound)

# æ–¹æ³•2: Z-scoreæ–¹æ³•
z_scores = (X - np.mean(X)) / np.std(X)
outliers = np.abs(z_scores) > 3

# æ–¹æ³•3: å¯è§†åŒ–æ£€æŸ¥
plt.boxplot(X)
plt.show()
```

---

## âš™ï¸ é¢„å¤„ç†æ–¹æ³•é€‰æ‹©æŒ‡å—

### ç‰¹å¾ç¼©æ”¾å¯¹æ¯”
| æ–¹æ³• | å…¬å¼ | é€‚ç”¨åœºæ™¯ | æ³¨æ„äº‹é¡¹ |
|------|------|----------|----------|
| **æ ‡å‡†åŒ–** | `(x-Î¼)/Ïƒ` | å¤§å¤šæ•°MLç®—æ³• | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ |
| **å½’ä¸€åŒ–** | `(x-min)/(max-min)` | ç¥ç»ç½‘ç»œã€è·ç¦»ç®—æ³• | å—å¼‚å¸¸å€¼å½±å“å¤§ |
| **é²æ£’ç¼©æ”¾** | `(x-median)/IQR` | æœ‰å¼‚å¸¸å€¼çš„æ•°æ® | æŠ—å¼‚å¸¸å€¼èƒ½åŠ›å¼º |

```python
# æ ‡å‡†åŒ–ï¼ˆæœ€å¸¸ç”¨ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å½’ä¸€åŒ–ï¼ˆç¥ç»ç½‘ç»œï¼‰
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# é²æ£’ç¼©æ”¾ï¼ˆæœ‰å¼‚å¸¸å€¼ï¼‰
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
```

### ç‰¹å¾é€‰æ‹©æ–¹æ³•
```python
# ç»Ÿè®¡æ–¹æ³•ï¼ˆå¿«é€Ÿï¼‰
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# æ¨¡å‹æ–¹æ³•ï¼ˆæ›´å‡†ç¡®ï¼‰
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
selector = SelectFromModel(RandomForestClassifier(), threshold='median')
X_selected = selector.fit_transform(X, y)

# é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆè®¡ç®—é‡å¤§ï¼‰
from sklearn.feature_selection import RFE
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=10)
X_selected = rfe.fit_transform(X, y)
```

---

## âœ‚ï¸ æ•°æ®åˆ†å‰²ç­–ç•¥

### åˆ†å‰²æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ä»£ç  | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **åŸºç¡€åˆ†å‰²** | `train_test_split(X, y)` | ç®€å•å¿«é€Ÿ | å¯èƒ½ä¸å¹³è¡¡ | å¤§æ•°æ®é›† |
| **åˆ†å±‚åˆ†å‰²** | `train_test_split(X, y, stratify=y)` | ä¿æŒç±»åˆ«åˆ†å¸ƒ | è®¡ç®—ç¨æ…¢ | åˆ†ç±»ä»»åŠ¡ |
| **äº¤å‰éªŒè¯** | `KFold(n_splits=5)` | è¯„ä¼°å¯é  | è®¡ç®—æˆæœ¬é«˜ | å°æ•°æ®é›† |
| **åˆ†å±‚äº¤å‰éªŒè¯** | `StratifiedKFold(n_splits=5)` | å…¼é¡¾ä¸¤è€… | è®¡ç®—æˆæœ¬é«˜ | ç±»åˆ«ä¸å¹³è¡¡ |

### æœ€ä½³å®è·µ
```python
# æ¨èï¼šåˆ†å±‚åˆ†å‰² + äº¤å‰éªŒè¯
from sklearn.model_selection import StratifiedKFold, cross_val_score

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# æ¨¡å‹è¯„ä¼°
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train, y_train, cv=skf)
```

---

## ğŸš¨ å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1: æ•°æ®æ³„éœ²
```python
# âŒ é”™è¯¯ï¼šå…ˆé¢„å¤„ç†å†åˆ†å‰²
X_scaled = scaler.fit_transform(X)  # ç”¨äº†å…¨éƒ¨æ•°æ®ï¼
X_train, X_test = train_test_split(X_scaled)

# âœ… æ­£ç¡®ï¼šå…ˆåˆ†å‰²å†é¢„å¤„ç†
X_train, X_test = train_test_split(X, stratify=y)
X_train_scaled = scaler.fit_transform(X_train)  # åªç”¨è®­ç»ƒé›†ï¼
X_test_scaled = scaler.transform(X_test)       # ç”¨è®­ç»ƒé›†å‚æ•°ï¼
```

### é”™è¯¯2: æ ‡ç­¾ä¸æ ‡å‡†
```python
# âŒ é”™è¯¯ï¼šæ ‡ç­¾ä¸ä»0å¼€å§‹
y = [1, 2, 1, 2]  # åº”è¯¥æ˜¯ [0, 1, 0, 1]

# âœ… æ­£ç¡®ï¼šæ ‡å‡†åŒ–æ ‡ç­¾
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_standardized = le.fit_transform(y)
```

### é”™è¯¯3: å¿˜è®°å¤„ç†æµ‹è¯•é›†
```python
# âŒ é”™è¯¯ï¼šåªå¤„ç†è®­ç»ƒé›†
X_train_processed = preprocess(X_train)
# å¿˜è®°å¤„ç†X_testï¼

# âœ… æ­£ç¡®ï¼šå¤„ç†ä¸¤ä¸ªæ•°æ®é›†
X_train_processed = preprocess(X_train)
X_test_processed = preprocess(X_test)  # ä½¿ç”¨è®­ç»ƒå‚æ•°
```

---

## ğŸ“Š æ€§èƒ½æ£€æŸ¥æ¸…å•

### æ•°æ®è´¨é‡æ£€æŸ¥
```python
def check_data_quality(X, y, dataset_name="æ•°æ®é›†"):
    print(f"\nğŸ” {dataset_name}è´¨é‡æ£€æŸ¥:")

    # åŸºæœ¬ç»Ÿè®¡
    print(f"æ ·æœ¬æ•°: {X.shape[0]}, ç‰¹å¾æ•°: {X.shape[1]}")

    # ç¼ºå¤±å€¼
    missing = np.sum(np.isnan(X))
    print(f"ç¼ºå¤±å€¼: {missing}")

    # æ ‡ç­¾åˆ†å¸ƒ
    unique, counts = np.unique(y, return_counts=True)
    balance = min(counts) / max(counts)
    print(f"ç±»åˆ«å¹³è¡¡: {balance:.2f}")

    # æ•°å€¼èŒƒå›´
    print(f"æ•°å€¼èŒƒå›´: [{np.min(X):.3f}, {np.max(X):.3f}]")

    return missing == 0 and balance > 0.1  # è¿”å›æ˜¯å¦é€šè¿‡æ£€æŸ¥
```

### é¢„å¤„ç†æ•ˆæœæ£€æŸ¥
```python
def check_preprocessing_effect(X_before, X_after):
    print(f"\nğŸ“Š é¢„å¤„ç†æ•ˆæœæ£€æŸ¥:")

    # å‡å€¼æ£€æŸ¥ï¼ˆæ ‡å‡†åŒ–ååº”æ¥è¿‘0ï¼‰
    mean_after = np.mean(X_after, axis=0)
    print(f"å‡å€¼èŒƒå›´: [{np.min(mean_after):.6f}, {np.max(mean_after):.6f}]")

    # æ ‡å‡†å·®æ£€æŸ¥ï¼ˆæ ‡å‡†åŒ–ååº”æ¥è¿‘1ï¼‰
    std_after = np.std(X_after, axis=0)
    print(f"æ ‡å‡†å·®èŒƒå›´: [{np.min(std_after):.6f}, {np.max(std_after):.6f}]")

    # æ•°å€¼èŒƒå›´æ£€æŸ¥
    print(f"å¤„ç†åèŒƒå›´: [{np.min(X_after):.3f}, {np.max(X_after):.3f}]")
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### DO âœ…
1. **å…ˆåˆ†å‰²å†é¢„å¤„ç†** - é¿å…æ•°æ®æ³„éœ²
2. **ä½¿ç”¨åˆ†å±‚åˆ†å‰²** - ä¿æŒç±»åˆ«åˆ†å¸ƒ
3. **ä¿å­˜é¢„å¤„ç†å‚æ•°** - ç”¨äºæ–°æ•°æ®å¤„ç†
4. **æ£€æŸ¥æ•°æ®è´¨é‡** - åŠæ—¶å‘ç°é—®é¢˜
5. **å¯è§†åŒ–æ•°æ®** - ç†è§£æ•°æ®ç‰¹å¾

### DON'T âŒ
1. **ä¸è¦åœ¨é¢„å¤„ç†å‰åˆ†å‰²** - ä¼šå¯¼è‡´æ•°æ®æ³„éœ²
2. **ä¸è¦å¿˜è®°å¤„ç†æµ‹è¯•é›†** - ä¿æŒä¸€è‡´æ€§
3. **ä¸è¦å¿½ç•¥æ•°æ®è´¨é‡** - å½±å“æ¨¡å‹æ€§èƒ½
4. **ä¸è¦è¿‡åº¦å¤„ç†** - å¯èƒ½æŸå¤±ä¿¡æ¯
5. **ä¸è¦å¿½ç•¥ç±»åˆ«ä¸å¹³è¡¡** - å½±å“è¯„ä¼°ç»“æœ

---

## ğŸ”§ å¸¸ç”¨ä»£ç æ¨¡æ¿

### å®Œæ•´é¢„å¤„ç†æµæ°´çº¿
```python
def create_preprocessing_pipeline(handle_outliers=True, scaling='standard'):
    """åˆ›å»ºæ ‡å‡†é¢„å¤„ç†æµæ°´çº¿"""

    steps = []

    # ç¼ºå¤±å€¼å¤„ç†
    steps.append(('imputer', SimpleImputer(strategy='mean')))

    # å¼‚å¸¸å€¼å¤„ç†ï¼ˆå¯é€‰ï¼‰
    if handle_outliers:
        # è‡ªå®šä¹‰å¼‚å¸¸å€¼å¤„ç†å™¨
        pass

    # ç‰¹å¾ç¼©æ”¾
    if scaling == 'standard':
        steps.append(('scaler', StandardScaler()))
    elif scaling == 'minmax':
        steps.append(('scaler', MinMaxScaler()))
    elif scaling == 'robust':
        steps.append(('scaler', RobustScaler()))

    return Pipeline(steps)
```

### æ•°æ®éªŒè¯å‡½æ•°
```python
def validate_processed_data(X_train, X_test, y_train, y_test):
    """éªŒè¯å¤„ç†åçš„æ•°æ®"""

    checks = []

    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    checks.append(np.all(np.isfinite(X_train)))
    checks.append(np.all(np.isfinite(X_test)))

    # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
    checks.append(len(X_train) == len(y_train))
    checks.append(len(X_test) == len(y_test))

    # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
    train_labels = set(np.unique(y_train))
    test_labels = set(np.unique(y_test))
    checks.append(test_labels.issubset(train_labels))

    if all(checks):
        print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
    else:
        print("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤„ç†æµç¨‹")

    return all(checks)
```

---

**ğŸ’¡ è®°ä½**: å¥½çš„æ•°æ®å¤„ç†æ˜¯æˆåŠŸæœºå™¨å­¦ä¹ é¡¹ç›®çš„80%ï¼ğŸ¯